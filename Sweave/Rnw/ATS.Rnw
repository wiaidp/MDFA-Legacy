\def\pf{{\bf Proof. }}
\def\logimplies{\Rightarrow}
\def\convinlaw{\stackrel{{\cal L}}{\Longrightarrow }}
\def\convinp{\stackrel{P}{\longrightarrow }}
\def\convas{\stackrel{a.s.}{\longrightarrow }}
\def\convv{\stackrel{v}{\longrightarrow}}
\def\asymp{\stackrel{{\mathbb P}}{\sim}}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\def\QQ{\mathbb Q}
\def\NN{\mathbb N}
\def\MM{\mathbb M}
\def\LL{\mathbb L}
\def\EE{\mathbb E}
\def\PP{\mathbb P}
\def\DD{\mathbb D}
\def\WW{\mathbb W}
\def\FF{\mathbb F}
\def\II{\mathbb I}
\def\FF{\mathbb F}
\def\ttheta{\widetilde{\theta}}
\def\tTheta{\widetilde{\Theta}}
\def\tsig{\widetilde{\sigma}^2}
\def\tc{\widetilde{c}}
\def\etheta{\widehat{\theta}}
\def\eTheta{\widehat{\Theta}}
\def\esig{\widehat{\sigma}^2}
\def\ptheta{\underline{\theta}}
\def\pTheta{\underline{\Theta}}
\def\psig{\underline{\sigma}^2}

\def\eqinlaw{\stackrel{{\cal L}}{=}}
\def\tends{\rightarrow}
\def\tendsinf{\rightarrow\infty}
\def\isodynamo{\Leftrightarrow}



\chapter{The Accuracy-Timeliness-Smoothness Trilemma}\label{ats_sec}

We propose an extension of the classic MSE-paradigm which addresses nowcast and forecast applications\footnote{Backcasts were discussed in chapter \ref{fil_sec}.}. Specifically, we split the original MSE-norm into three components, identified as Accuracy, Timeliness and Smoothness. The resulting two-dimensional trade-off, controlled by the parameter-pair $\lambda,\eta$ in the head of the main function call, is called Accuracy-Timeliness-Smoothness Trilemma, or ATS-Trilemma for short, see Wildi (2005), McElroy and Wildi (2015). We derive  a generic optimization principle, called Customization, which nests the classic MSE-approach.  We show that the ATS-trilemma collapses to a one-dimensional trade off, the so-called AT-dilemma, in the case of forecasting. We infer  that classic (pseudo-maximum likelihood)  one-step ahead forecast approaches are incapable of addressing Timeliness and Smoothness, simultaneously. Efficiency gains of customized designs are quantified in a series of empirical examples. \\

The ATS-trilemma and the generic customization principle are introduced in section \ref{seatatst}; section \ref{fatatd} highlights the classic dichotomic forecast paradigm; quadratic optimization criteria and closed-form solutions are presented in section \ref{idfas}; an application of customization is proposed in section \ref{ats_work_o}; performance measures are presented in section \ref{peco_cu}; finally, sections \ref{double_score_ats} and \ref{ucdvbmseli} assess performances of customized designs when benchmarked against classic MSE-approaches. 











\section{Signal Extraction and the ATS-Trilemma}\label{seatatst}

We address the univariate dfa-case and generalize criterion \ref{dfa_ms}. Our treatment follows \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, section 4.3, and McElroy and Wildi (2015). 


\subsection{Decomposition of the MSE-Norm}

The DFA emphasizes the so-called transferfunction-error $\Gamma(\cdot)-\hat{\Gamma}(\cdot)$. The following geometric identity holds in general (law of cosine in the complex plane):
\begin{eqnarray}
|\Gamma(\omega)-\hat{\Gamma}(\omega)|^2&=&A(\omega)^2+\hat{A}(\omega)^2-2A(\omega)
\hat{A}(\omega)\cos\left(\hat{\Phi}(\omega)-\Phi(\omega)\right)\nonumber\\
&=&
(A(\omega)-\hat{A}(\omega))^2\nonumber\\
&&+2A(\omega)\hat{A}(\omega)\left[1-\cos\left(\hat{\Phi}(\omega)-\Phi(\omega)\right)\right]\label{etrigid}
\end{eqnarray}
If $\Gamma$ is symmetric and positive, then \(\Phi(\omega)\equiv
0\) so that we can omit $\Phi(\omega)$ from our notation\footnote{The ideal trend, classical filters (HP, CF, henderson) or model-based filters are typically positive (symmetric) filters. It should be clear that $\Phi(\omega)$ cannot be omitted if the condition is unfulfilled.}. By inserting \ref{etrigid} into \ref{dfa_ms} and using
$1-\cos(\hat{\Phi}(\omega))=2\sin(\hat{\Phi}(\omega)/2)^2$ we obtain
\begin{eqnarray} &&\frac{2\pi}{T}\sum_{k=-[T/2]}^{[T/2]}\left|\Gamma(\omega_k)-\hat{\Gamma}(\omega_k) \right|^2 I_{TX}(\omega_k)\nonumber\\
&=&\frac{2\pi}{ T} \sum_{k=-T/2}^{T/2}
(A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\label{unboptidioe}\\
&&+\frac{2\pi}{ T} \sum_{k=-T/2}^{T/2}
4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)\label{unboptidio}
\end{eqnarray}
The first summand \ref{unboptidioe} is the distinctive part of the total mean-square filter error
which is attributable to the amplitude function of the real-time filter (the MS-amplitude error).
The second summand \ref{unboptidio} measures the distinctive contribution of the phase or time-shift to
the total mean-square error (the MS-time-shift error). Note that the product
$A(\omega_k)\hat{A}(\omega_k)I_{TX}(\omega_k)$ in \ref{unboptidio} maps the scales of the filter outputs $y_t,\hat{y}_t$ to the frequency-domain (the phase function alone would be dimensionless).



\subsection{Accuracy, Timeliness and Smoothness (ATS-) Error-Components}  \label{ats_section}



The previous decomposition can be refined by splitting
each term into contributions from \emph{pass-} and \emph{stopbands}. For ease of exposition we assume an ideal lowpass target 
\[\Gamma(\omega)=\left\{\begin{array}{cc}1~,~|\omega|\leq\textrm{cutoff}\\0~,~\textrm{otherwise}\end{array}\right.\]
The set $\{\omega_k||\omega_k|\leq\textrm{cutoff}\}$ corresponds to the passband and its complement $\{\omega_k||\omega_k|>\textrm{cutoff}\}$ is the stop- or rejection-band. We now define
\begin{eqnarray}
\left.\begin{array}{ccc}
\textrm{A(ccuracy)}&:=&\frac{2\pi}{ T} \sum_{\textrm{Passband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\\
\textrm{S(moothness)}&:=&\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\\
\textrm{T(imeliness)}&:=&\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)                                     \\
\textrm{R(esidual)}&:=&\frac{2\pi}{ T}  \sum_{\textrm{Stopband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)
\end{array}\right\}\label{mse_dec_ats}
\end{eqnarray}
\begin{itemize}
\item \emph{Residual} is that part of the MSE which is contributed by the time-shift in the stopband: it \emph{vanishes} exactly in the case of the ideal trend.  Even if it does not vanish, it is generally negligible because the product $A(\omega_k)\hat{A}(\omega_k)$ is small in the stopband. We now ignore this error-term (it is dropped from subsequent notation).
\item \emph{Accuracy} measures the contribution to the MSE-norm which would be obtained by ignoring time-shift (passband) and noise suppression (stop-band) issues\footnote{It is the performance of a symmetric filter (no time-shift) with perfect noise suppression ($\hat{A}(\cdot)=\Gamma(0)=0$ in
the stopband) and with the same amplitude as $\hat{\Gamma}(\cdot)$ in the passband. Such a design could be easily constructed by
the techniques presented in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, section 1.}.
\item \emph{Smoothness} measures the contribution to total MSE of the undesirable high-frequency noise, leaking
from the imperfect amplitude fit in the stopband (convolution theorem \ref{convolution_dft}). This measure is tightly linked to classical time-domain \emph{curvature} (smoothness) measures.
\item \emph{Timeliness} measures the MSE-contribution generated by the time-shift. This measure is closely linked
to the well-known (time-domain) \emph{peak-correlation}, as referenced against the target signal.
\end{itemize}
We are now able to address each single or each \emph{pairwise} combination of error-terms by splitting the MSE-criterion accordingly. Typically, speed (Timeliness) and noise suppression (Smoothness) are important properties of a real-time filter in prospective estimation problems (nowcast, forecast)  



\subsection{Generic Customized Criterion}\label{gen_cust_crit}


We are in a position to generalize the mean-square optimization criterion \ref{dfa_ms}
by assigning weights to the ATS-components (recall that the Residual either vanishes exactly or is negligible):
\begin{equation}\label{ats_cust}
\textrm{MSE-Cust}(\lambda_1,\lambda_2)=\textrm{Accuracy}+(1+\lambda_1) \textrm{Timeliness}+(1+\lambda_2) \textrm{Smoothness}\to \min_{\mathbf{b}}
\end{equation}
Selecting $\lambda_1>0$ results, ceteris paribus, in a smaller Timeliness component; as we shall see the resulting filter will be faster: turning-points can be detected earlier (smaller peak-correlation against the target signal). Selecting
$\lambda_2>0$ magnifies, ceteris paribus, the Smoothness term: the output of the resulting filter is smoother or, stated otherwise, undesirable high-frequency noise is damped more effectively. Finally, selecting $\lambda_1>0,\lambda_2>0$ emphasizes both attributes: the resulting filter-output is able to gain both in terms of `speed' as well as in terms of `noise suppression'. The ATS-trilemma confers the user the possibility to adjust trade-offs according to research priorities.\\


The schematic criterion \ref{ats_cust} can be generalized by substituting weighting functions $W_1(\omega_k)\geq 0$, $W_2(\omega_k)\geq 0$  for the fixed weights $\lambda_1,\lambda_2$:
\begin{eqnarray*}
&&\frac{2\pi}{ T} \sum_{\textrm{Passband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (A(\omega_k)-\hat{A}(\omega_k))^2 W_2(\omega_k)I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
W_1(\omega_k)I_{TX}(\omega_k)\to\min_{\mathbf{b}}
\end{eqnarray*}
Criterion \ref{ats_cust} can be replicated by selecting
\begin{eqnarray*}
W_1(\omega_k,\lambda_1)&=&1+\lambda_1\\
W_2(\omega_k,\lambda_2)&=&1+\lambda_2
\end{eqnarray*}
Note, however, that $W_2$ would introduce an undesirable discontinuity in the cutoff-frequency, if $\lambda_2\neq 0$. Therefore, we propose the following weighting-scheme
\begin{eqnarray}
\left.\begin{array}{ccc}W_1(\omega_k,\lambda)&=&1+\lambda\\
W_2(\omega_k,\eta)&=&(1+|\omega_k|-\textrm{cutoff})^{\eta}
\end{array}\right\}\label{w_func_cust}
\end{eqnarray}
The new function $W_2$ allows for a continuous transition from pass- to stopband and its monotonic shape overemphasizes undesirable high-frequency components\footnote{The `nuisance' of an undesirable high-frequency component, as measured by its derivative (or its first-order difference) is proportional to $\omega$ (or to $|1-\exp(-i\omega)|$).}. We re-labelled
$\lambda_1,\lambda_2$ by $\lambda,\eta$ in order to distinguish symbolically both effects: $\lambda>0$ emphasizes Timeliness and $\eta>0$ addresses Smoothness. The criterion can be re-written more compactly as
\begin{eqnarray}
&&\frac{2\pi}{ T} \sum_{\textrm{All~Frequencies}} (A(\omega_k)-\hat{A}(\omega_k))^2 W(\omega_k,\eta) I_{TX}(\omega_k)\nonumber\\
&&+(1+\lambda)\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)\to\min_{\mathbf{b}}\label{dfatp}
\end{eqnarray}
where
\begin{equation}\label{w}
W(\omega_k,\eta,\textrm{cutoff})=\left\{\begin{array}{cc}
1~,~\textrm{if~} |\omega_k|<\textrm{cutoff}\\
(1+|\omega_k|-\textrm{cutoff})^{\eta}~,~\textrm{otherwise}
\end{array}\right.
\end{equation}
The optimization principle \ref{dfatp} is called a \emph{customized} criterion, see Wildi (2005) and McElroy and Wildi (2015). Obviously, the customized criterion nests the MSE-criterion \ref{dfa_ms} (select $\lambda=\eta=0$).  


\section{Forecasting and the AT-Dilemma}\label{fatatd}

In the previous section we assumed a particular signal as specified by a lowpass target. Here we analyze (anticipative) \emph{allpass} targets which appear in classical forecast applications, recall section \ref{one_step}.    

\subsection{ATS-Trilemma Collapses to AT-Dilemma}

An application of the MDFA-MSE criterion \ref{dfanv} to ordinary one- (and multi-) step head forecasting was proposed in section \ref{one_step}. Specifically, in the case of the classic one-step ahead criterion, \ref{dfa_ms} becomes 
\begin{equation}\label{dfanv_1s_ats}
\frac{2\pi}{T} \sum_{k=-T/2}^{T/2}
\left|\exp(i\omega_k)-\hat{\Gamma}_X(\omega_k)\right|^2I_{T
X}(\omega_k) \to \min_{\mathbf{b}}
\end{equation}
Note that the target filter $\Gamma(\omega_k):=\exp(i\omega_k)$ is an allpass since $|\Gamma(\omega_k)|=1$ for all frequencies. As a consequence, both the Residual- as well as the Smoothness-term vanish -- they are missing -- and (the schematic) criterion  \ref{ats_cust} simplifies to
\begin{eqnarray*}
\textrm{MSE-Cust}(\lambda_1)=\textrm{Accuracy}+(1+\lambda_1) \textrm{Timeliness}\to \min_{\mathbf{b}}
\end{eqnarray*}
The signal-extraction ATS-trilemma collapses to a forecast {AT-dilemma}. We may infer that the classic time series paradigm (pseudo-maximum-likelihood), which relies on (mean-square one-step ahead) forecast performances is immanently unable to break-up the tie between Timeliness and Smoothness in real-time signal extraction.  




\subsection{Illustration: White Noise and Random-Walk Processes}\label{i_w_rw}

\subsubsection{LDP-Filter and Arithmetic Mean}

Let $\Gamma(\omega_k)=\exp(i\omega_k)$ be the one-step ahead target and consider the following two polar filter designs: 
\begin{enumerate}
\item Let $b_0=1,b_j=0,j=1,...,L-1$: the filter assigns full weight $b_0=1$ to $x_T$ in the data-sample $x_1,...,x_T$. Let's call it `Last-Data-Point' (LDP-) filter. Its transfer function is
\[\hat{\Gamma}(\omega_k)=1\]
\item Let $b_j=1/L,j=0,...,L-1$: the filter assigns equal weight $1/L$ to $x_T,...,x_{T-(L-1)}$. We now set $L:=T$ so that the equal-weight filter corresponds to the arithmetic mean. The transfer function of the arithmetic mean is
\[\hat{\Gamma}(\omega_k)=\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)\]
\end{enumerate}
The LDP-filter is an allpass without time-shift; the arithmetic mean is a lowpass with a narrow passband and a large time-shift. 



\subsubsection{Random-Walk Process}

Let $x_1,...,x_T$ be a realization of a random-walk process. In this case, the LDP-filter is optimal, in a mean-square one-step ahead perspective: the best forecast of $x_{T+1}$ is $x_T$. Since Smoothness is missing (no stop-band), the (schematic) customized criterion \ref{ats_cust} simplifies -- degenerates -- to
\begin{eqnarray}
\textrm{MSE-Cust}(\lambda_1)&=&\textrm{Accuracy}+(1+\lambda_1) \textrm{Timeliness}~\left(\to\min_{\mathbf{b}}\right)\label{mdfa_cust_forec}\\
&=&\frac{2\pi}{ T} \sum_{\textrm{All ~Frequencies}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\nonumber\\
&&+(1+\lambda_1)\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4A(\omega_k)\hat{A}(\omega_k)\sin\left(\frac{\Phi(\omega_k)-\hat{\Phi}(\omega_k)}{2}\right)^2I_{TX}(\omega_k)\nonumber\\
&=&(1+\lambda_1)\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4\sin\left(\omega_k/2\right)^2I_{TX}(\omega_k)~\left(\to\min_{\mathbf{b}}\right)\label{ast_cust_for}
\end{eqnarray}
The last equality follows from $A(\omega_k)=\hat{A}(\omega_k)=1$ (both the target as well as the LDP-filter are allpass designs), $\Phi(\omega_k)=\textrm{Arg}(\exp(i\omega_k))=\omega_k$ (the target filter is anticipative) and $\hat{\Phi}(\omega_k)=0$. We infer that MSE is entirely attributable to Timeliness (Accuracy vanishes); indeed, the output $x_T$ of $\hat{\Gamma}(\cdot)$ is shifted by one time-unit relative to the target $x_{T+1}$: no other action or effect is supported by the LDP-filter. In contrast, the arithmetic mean would strongly smooth the filter output: Accuracy would inflate considerably in \ref{mdfa_cust_forec}.\\

The (true) pseudo-spectrum of the process is given by 
\begin{eqnarray}\label{pseudo_spectrum}
h_X(\omega)=\frac{\sigma^2}{2\pi|1-\exp(-i\omega)|^2}
\end{eqnarray} 
where $\sigma^2$ is the variance of the (stationary) white noise $\tilde{x}_t=x_t-x_{t-1}$ and $\displaystyle{\frac{\sigma^2}{2\pi}}$ is its (flat) spectral-density;  $\displaystyle{\frac{1}{1-\exp(-i\omega)}}$ is the transfer function of the (non-stationary) AR(1)-filter linking the noise $\tilde{x}_t$ to the random-walk $x_t$, see also section \ref{pseudo_dft}. We could now plug the true pseudo-spectrum into \ref{ast_cust_for} and set $\lambda_1=0$ to obtain a so-called `true' MSE%\footnote{The rationale for this substitution is given by \ref{add_res_dfa} i.e. one would minimize the true (unknown) mean-square filter error.} 
\begin{eqnarray}
\textrm{true~MSE}&=&\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4\sin\left(\omega_k/2\right)^2\frac{\sigma^2}{2\pi|1-\exp(-i\omega_k)|^2}\nonumber\\
&=&\frac{2\sigma^2}{ T}  \sum_{\textrm{All~Frequencies}} \frac{1-\cos(\Phi(\omega_k))}{|1-\exp(-i\omega_k)|^2}\nonumber\\
&=&\frac{2\sigma^2}{ T}  \sum_{\textrm{All~Frequencies}} \frac{1}{2}\label{rwmse}\\
&=&\sigma^2\nonumber
\end{eqnarray}
where we used $|1-\exp(-i\omega_k)|^2=(1-\cos(\omega_k))^2+\sin(\omega_k)^2=2-2\cos(\omega_k)$\footnote{For notational simplicity we did not discriminate the case of odd and even $T$: in the latter case additional weights $w_k$ would be necessary, see section \ref{dft_and_per}.}. The `true MSE' is the mean-square one-step ahead forecast error, as expected.



\subsubsection{White Noise}


Assume $x_t$ is a white noise process and consider, once again, the LDP-filter. From the above we have 
\begin{eqnarray}
\textrm{MSE-Cust}(\lambda_1)&=&(1+\lambda_1)\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4\sin\left(\omega_k/2\right)^2I_{TX}(\omega_k)~\left(\to\min_{\mathbf{b}}\right)\label{ast_cust_for_wn}
\end{eqnarray}
Setting $\lambda_1=0$ and plugging the (flat) spectral density $\displaystyle{\frac{\sigma^2}{2\pi}}$ of the process into this expression %\footnote{The rationale for this substitution is given by \ref{add_res_dfa} i.e. one would minimize the true (unknown) mean-square filter error.} 
we obtain 
\begin{eqnarray}
\textrm{true~MSE}&=&\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4\sin(\omega_k/2)^2\frac{\sigma^2}{2\pi}\nonumber\\
&=&\frac{\sigma^2}{ T}  \sum_{\textrm{All~Frequencies}} 4\sin(\omega_k/2)^2\nonumber\\
&=&\frac{2\sigma^2}{ T}  \sum_{\textrm{All~Frequencies}} (1-\cos(\omega_k))\label{wnmse}\\
&=&2\sigma^2\nonumber
\end{eqnarray}
where we made use of $\sum_{\textrm{All~Frequencies}} \cos(\omega_k)=0$, see corollary \ref{discret_sums_cor} in the \ref{dstt}. Indeed, the (true) MSE of the LDP-filter is $Var(x_{T+1}-x_{T})=2\sigma^2$. Comparing \ref{wnmse} and \ref{rwmse} reveals that high-frequency components contribute disproportionate to Timeliness since $1-\cos(\omega_k)$ is monotonically increasing. We now substitute the arithmetic mean for the LDP-filter and obtain:
\begin{eqnarray}
\textrm{true~MSE}&=&\textrm{Accuracy+Timeliness}\nonumber\\
&=&\frac{2\pi}{ T} \sum_{\textrm{All ~Frequencies}} (A(\omega_k)-\hat{A}(\omega_k))^2 \frac{\sigma^2}{2\pi}\nonumber\\
&&+\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4A(\omega_k)\hat{A}(\omega_k)\sin\left(\frac{\Phi(\omega_k)-\hat{\Phi}(\omega_k)}{2}\right)^2\frac{\sigma^2}{2\pi}\nonumber\\
&=&\frac{2\pi}{ T} \sum_{\textrm{All ~Frequencies}} \left(1-\left|\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)\right|\right)^2\frac{\sigma^2}{2\pi} \nonumber\\
&&+\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4\left|\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)\right| \sin\left(\frac{-\omega_k-\frac{T}{2}\omega_k}{2}\right)^2\frac{\sigma^2}{2\pi}\nonumber
\end{eqnarray}
where we inserted $\Phi(\omega_k)=-Arg(\exp(i\omega_k))=-\omega_k$ (anticipative allpass target) and $\hat{\Phi}(\omega_k)=\hat{\phi}(\omega_k)\omega_k=\displaystyle{\frac{T}{2}}\omega_k$\footnote{The equally-weighted (arithmetic mean) filter shifts all components by half its filter-length $L/2$ and we selected $L=T$. Therefore $\hat{\phi}(\omega_k)=T/2$.}. 
Using $\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)=0, \omega_k\not= 0$, see proposition \ref{discret_sums} in the appendix \ref{dstt}, and the fact that the sine function vanishes at $\omega_0=0$, the expression for the (true) MSE simplifies to
\begin{eqnarray}
\textrm{MSE}&=&\frac{2\pi}{ T} \sum_{\textrm{All ~Frequencies}} \left(1-\left|\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)\right|\right)^2\frac{\sigma^2}{2\pi} \nonumber\\
&=&\sigma^2\frac{T-1}{T}\label{mse_ins}
\end{eqnarray}
Timeliness vanishes completely because $\hat{A}(\omega_k)=\left|\frac{1}{T}\sum_{j=0}^{T-1}\exp(-ij\omega_k)\right|=0$ if $\omega_k\neq 0$ and because $\sin\left(\frac{\omega_0-\frac{T}{2}\omega_0}{2}\right)=0$ for $\omega_0=0$. Also, the summand of the Accuracy term vanishes in $\omega_0$; therefore Accuracy sums to $\displaystyle{\frac{T-1}{T}}\sigma^2$. The asymptotically negligible correction $\displaystyle{\frac{T-1}{T}}$ accounts for the fact that the arithmetic mean is an in-sample estimate i.e. we observe a tiny bit of overfitting here (the fit in frequency zero is `too good'). We conclude that the (optimal) arithmetic mean trades the entire Timeliness term against a full-blown Accuracy term\footnote{Figuratively, the arithmetic mean squeezes the data to a flat line which is free of time-shifts.}. The net result is that MSE decreases from $2\sigma^2$, in the case of the LDP filter, to $\approx\sigma^2$ (ignoring the finite sample correction).\\

The fact that the arithmetic mean, a strong smoothing filter with a large time-shift, reduces Timeliness is not without a touch of irony\footnote{This is because Timeliness is not a scale-invariant measure i.e. the contribution of the time-shift to MSE depends on the magnitude of the signals. See section \ref{peco_cu} for a corresponding scale-invariant metric.}; even more so when considering that the decrease in Timeliness must overcompensate the inflated Accuracy; and exceedingly so when considering that Smoothness does not even appear in criterion \ref{mdfa_cust_forec} since the forecast-target is an \emph{allpass}. Hopefully, the proposed counter-intuitive examples shed some light on the rich(er) structure of the ATS-trilemma or, for that matter, of its degenerate sibling, the forecast AT-dilemma. 



\section{Quadratic Criterion$^*$}\label{idfas}

\subsection{I-DFA}


The mean-square error criterion \ref{dfa_ms} is a quadratic function of the filter coefficients $\mathbf{b}$ and can be solved  in closed-form, see \ref{bregms}: the solution is unique and numerical computations are fast. Unfortunately, \ref{dfatp} is no longer quadratic in $\mathbf{b}$, if $\lambda>0$. We here propose an alternative expression which is quadratic irrespective of $\lambda$. Interestingly, the new quadratic criterion matches closely \ref{dfatp}, even for `large' $\lambda$ (virtuous feedback loop\footnote{Larger $\lambda$ tend to linearize the problem because $\hat{\Phi}(\omega)-\Phi(\omega)$ will be small, see section \ref{ti_o_app_idfa}.}). Our treatment is general i.e. the target $\Gamma(\cdot)$ does not need to be positive and real. Moreover, the proposed formalism lends itself for a straightforward generalization to the multivariate case, to be developed in chapter \ref{atsm_sec}. Consider
\begin{eqnarray}\label{idfa}
&&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\big|\Gamma(\omega_k)\Xi_{TX}(\omega_k)\big|-\left\{\Re\left[\hat{\Gamma}(\omega_k)\Xi_{TX}(\omega_k)\exp\big(-i\arg\big\{\Gamma(\omega_k)\Xi_{TX}(\omega_k)\big\}\big)\right]\right.\right.\nonumber\\
 &&\left.\left.+i\sqrt{1+\lambda|\Gamma(\omega_k)|}
 \Im\left[\hat{\Gamma}(\omega_k)\Xi_{TX}(\omega_k)\exp\big(-i\arg\left\{\Gamma(\omega_k)\Xi_{TX}(\omega_k)\right\}\big)\right]\right\}\right|^2 W(\omega_k,\eta)\to\min_{\mathbf{b}}\nonumber\\
\end{eqnarray}
where $\Re(\cdot)$ and $\Im(\cdot)$ denote real and imaginary parts and $i^2=-1$ is the imaginary unit.
We call the resulting approach I-DFA\footnote{The capital I in the acronym stands for the imaginary part which
is emphasized by $\lambda$.}. The above expression is quadratic in the filter coefficients because real and imaginary parts are linear in the coefficients. In analogy to \ref{dfatp}, the weighting function $W(\omega_k,\eta)$ emphasizes the fit  in the
stop band. The term $\lambda|\Gamma(\omega_k)|$, under the square-root, emphasizes the imaginary part of the real-time filter
in the pass band: for $\lambda>0$ the imaginary part is artificially inflated and therefore we expect the phase 
to shrink, see below for details. If $\Gamma(\cdot)$ is a real and positive function (ideal trend, for example), then the quadratic criterion \ref{idfa} simplifies to
\begin{equation}\label{idfa_s}
\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\Gamma(\omega_k)-\left\{\Re\left(\hat{\Gamma}(\omega_k)\right)+i\sqrt{1+\lambda\Gamma(\omega_k)}
 \Im\left(\hat{\Gamma}(\omega_k)\right)\right\}\right|^2 W(\omega_k,\eta)I_{TX}(\omega_k)\to\min_{\mathbf{b}}
\end{equation}
Expression \ref{idfa} is intentionally more complex, than strictly necessary, because it will allow for a straightforward derivation of the closed-form solution, in the next chapter. The following development allows for a direct comparison of \ref{dfatp} and \ref{idfa}:
\begin{eqnarray}
&&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\big|\Gamma(\omega_k)\Xi_{TX}(\omega_k)\big|-\left\{\Re\left[\hat{\Gamma}(\omega_k)\Xi_{TX}(\omega_k)\exp\big(-i\arg\big\{\Gamma(\omega_k)\Xi_{TX}(\omega_k)\big\}\big)\right]\right.\right.\nonumber\\
 &&\left.\left.+i\sqrt{1+\lambda|\Gamma(\omega_k)|}
 \Im\left[\hat{\Gamma}(\omega_k)\Xi_{TX}(\omega_k)\exp\big(-i\arg\big\{\Gamma(\omega_k))\Xi_{TX}(\omega_k)\big\}\big)\right]\right\}\right|^2 W(\omega_k,\eta)\label{idfa_t}\\
&=&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  \left\{\left(\big|\Gamma(\omega_k)\big|-\Re\left[\hat{\Gamma}(\omega_k)\exp\big(-i\arg(\Gamma(\omega_k))\big)\right]\right)^2\right.\nonumber\\
&&\left.+\Im\left[\hat{\Gamma}(\omega_k)\exp\big(-i\arg(\Gamma(\omega_k))\big)\right]^2\right\}W(\omega_k,\eta)I_{TX}(\omega_k)\nonumber\\
&&+\lambda\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  A(\omega_k)\Im\left[\hat{\Gamma}(\omega_k)\exp\big(-i\arg(\Gamma(\omega_k))\big)\right]^2W(\omega_k,\eta)I_{TX}(\omega_k) \nonumber\\
&=&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\Gamma(\omega_k)-\hat{\Gamma}(\omega_k)\right|^2 W(\omega_k,\eta)I_{TX}(\omega_k)\nonumber\\
&&+\lambda\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  A(\omega_k)\hat{A}(\omega_k)^2\sin\left(\hat{\Phi}(\omega_k)-\Phi(\omega_k)\right)^2W(\omega_k,\eta)I_{TX}(\omega_k)\nonumber\\
&=&\left\{\begin{array}{c}\displaystyle{\frac{2\pi}{T} \sum_{\textrm{All~Frequencies}} (A(\omega_k)-\hat{A}(\omega_k))^2 W(\omega_k,\eta) I_{TX}(\omega_k)}\\
\displaystyle{+\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2I_{TX}(\omega_k)}\\
+\displaystyle{\lambda\frac{2\pi}{T} \sum_{\textrm{Passband}}
  A(\omega_k)\hat{A}(\omega_k)^2\sin\left(\hat{\Phi}(\omega_k)-\Phi(\omega_k)\right)^2I_{TX}(\omega_k)}\end{array}\right.\label{idfatp}  
\end{eqnarray}
where we assumed that $W(\omega_k,\eta)=1$ for $\omega_k$ in the passband. If $\Phi(\omega_k)=0$, then a direct comparison of \ref{dfatp} and \ref{idfatp} reveals that $\hat{\Phi}(\omega_k)/2$, in the former, is replaced
by $\hat{\Phi}(\omega_k)$ in the term weighted by $\lambda$ in \ref{idfatp}; also, a supernumerary weighting-term $\hat{A}(\omega_k)$ appears in this expression and the constant scaling-term 4 is missing. Before discussing the quality of the approximation of \ref{dfatp} by \ref{idfatp} we note that the latter expression is quadratic in the filter coefficients because both sums are quadratic: while this statement is obvious for the first sum, it applies to the second too because $\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)-\Phi(\omega_k))$ is the
imaginary part of $\hat{\Gamma}(\omega_k)\exp(-i\Phi(\omega_k))$ which is linear in the filter coefficients. Therefore, minimization of \ref{idfatp} can be solved in closed form. 
For $\lambda=\eta=0$ the original (DFA) mean-square
criterion \ref{dfa_ms} is obtained. Overemphasizing the imaginary part of the real-time filter
in the pass-band, by $\lambda>0$, achieves a smaller time-shift and increasing $\eta$ magnifies stopband rejection, as desired. If $\Phi(\omega_k)\not=0$\footnote{A classic (allpass) one-step ahead target implies $\Phi(\omega_k)=\omega_k$, see section \ref{one_step}.} then a larger $\lambda$ generates a smaller phase-error i.e. $|\hat{\Phi}(\omega_k)-\Phi(\omega_k)|$ shrinks in the passband, as desired. \\

\textbf{Remark}: 
\begin{itemize}
\item Although \ref{idfatp} is a simpler, more elegant and intuitively more appealing form of the equivalent criterion \ref{idfa}, we prefer the latter because the solution can be derived more easily, in closed-form, see chapter \ref{atsm_sec}. Also, \ref{idfa} matches the implementation in our R-code: the interested reader can track theory in code. 
\end{itemize}


\subsection{Tightness of Approximation} \label{ti_o_app_idfa}

If $\lambda>0$ then \ref{dfatp} is no more quadratic in the filter coefficients. Therefore, we might be tempted to deduce that   \ref{idfatp} and \ref{dfatp} differ in proportion to $\lambda$: larger $\lambda$ should magnify discrepancies between both criteria. Interestingly, a larger $\lambda$ implies a smaller phase-error which, in turn, `linearizes' the approximation problem as we now show --  virtuous feedback-loop -- (for notational convenience we assume that $\Phi(\omega_k)=0$):
\begin{enumerate}
\item For small $\hat{\Phi}(\omega_k)$ the following approximations apply (first order Taylor)
\begin{equation}\label{approx_prob_linearized}
4\sin(\hat{\Phi}(\omega_k)/2)^2\approx 4\hat{\Phi}(\omega_k)^2/4 \approx \sin(\hat{\Phi}(\omega_k))^2
\end{equation}
We deduce that the phase terms in \ref{dfatp} and \ref{idfatp} are interchangeable; also, the additional scaling constant 4, in \ref{dfatp}, is replicated exactly by \ref{idfatp}. 
\item In general, $\hat{A}(\omega_k)$ is nearly constant in the passband because Accuracy ensures that $\hat{A}(\omega_k)\approx|\Gamma(\omega_k)|=1$. Obviously, in such a case, the supernumerary amplitude term in \ref{idfatp} can be ignored. 
\item Sometimes, imposing a strong customization weakens Accuracy, see for example section \ref{l_e_geq_0}. If $\hat{A}(\omega_k)\not\approx 1$, then the effect of the supernumerary amplitude term in \ref{idfatp} cannot be ignored, anymore. However, in such a case the distortion induced by the additional amplitude term  in \ref{idfatp} could be compensated by a simple re-adjusting or re-scaling of $\lambda$, as discussed in section \ref{l_e_geq_0} below.
\end{enumerate}
We conclude that the quadratic customized criterion \ref{idfatp} is identical to \ref{dfatp}, when $\lambda=0$; if $\lambda>0$ is small, then both criteria are nearly identical; if $\lambda>0$ is large, then $\hat{\Phi}(\omega_k)-\Phi(\omega_k)$ tends to be small (because $\lambda$ emphasizes the time-shift) and therefore the approximation problem is linearized, see \ref{approx_prob_linearized}.





\subsection{R-code}


The R-function $dfa\textunderscore analytic()$ proposed in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, section 4.3.5,  implements a closed-form solution of criterion \ref{idfa}. 

<<echo=True>>=
head(dfa_analytic)
@
The entries $L$, $weight\textunderscore func$, $Lag$, $Gamma$, $i1$ and $i2$ retain their original meanings, as discussed in previous chapters. The new customization parameters $\lambda,\eta$ and $cutoff$ refer to the weighting functions $W_1$ and $W_2$ defined in \ref{w_func_cust} (in general cutoff coincides with the specification of the target $Gamma$). The function returns coefficients as well as transfer function of the filter:
<<echo=True>>=
tail(dfa_analytic)
@
\textbf{Remark}
\begin{itemize}
\item For historical reasons the level-constraint (i1=T) was not implemented in closed (exact) form\ref{con_sec}\footnote{In order to improve the fit of the transfer functions in frequency zero ($\hat{\Gamma}(0)\equiv\Gamma(0)$) a large artificial value is assigned to $I_{TX}(0)$.}. We therefore recommend usage of the generic MDFA-code $mdfa\textunderscore analytic$ instead. The latter relies on exact formulas, as derived in chapter \ref{con_sec}.  
\end{itemize}





\section{ATS-Components: a Worked-Out Example}\label{ats_work_o}


%We just saw  that -- under some particular circumstances (allpass target) -- Timeliness can be reduced by a filter with a considerable time-shift. This seemingly counterintuitive result should not distract from the fact that customization \emph{effectively} tackles `speed' and `noise suppression' in typical prospective signal extraction applications. In order to illustrate the relevant issues 
We here rely on the simulation framework of section \ref{ex_dfa_1}, taken over from McElroy and Wildi (2015). Specifically, we consider the three AR(1)-processes 
\begin{eqnarray}
\left.\begin{array}{ccc}x_t&=&0.9x_{t-1}+\epsilon_t\\
x_t&=&0.1x_{t-1}+\epsilon_t\\
x_t&=&-0.9x_{t-1}+\epsilon_t
\end{array}\right\}\label{ar1_processes}
\end{eqnarray}
and we generate a single realization of length $T=120$ (10 years of monthly data) for each process. Our target is an ideal trend with cutoff $\pi/12$ and we rely on real-time DFA-filters of length $L=24$\footnote{The target signal eliminates components whose duration is shorter than $2\pi/(\pi/12)=24$ (two years of monthly data). This design is inspired from real-time `business-cycle' applications. The chosen filter-length $L=24$ reflects the maximal duration of components in the stopband of the target filter (two years).}. We then assess customization effects obtained by $\lambda,\eta$ in (the quadratic) criterion \ref{idfa} or, equivalently, \ref{idfatp}. For this purpose we compute and report ATS-components, total MSE, amplitude and time-shift functions, filter coefficients as well as filter outputs. In order to save space, we report results for the third process, $a_1=0.9$, only (the other results are reported in the appendix). The estimation routine is based on $dfa\textunderscore analytic$ as introduced in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA} and briefly presented in chapter \ref{intro_sec}. In-sample and out-of-sample distributions of performances of competing designs, based on multiple realizations of the above processes, will be analyzed in sections \ref{double_score_ats} and \ref{ucdvbmseli}, further below.


\subsection{Timeliness Only: $\lambda\geq0$, $\eta=0$ Fixed}

\begin{enumerate}
\item We source the relevant R-file 
<<echo=TRUE>>=
source(file=paste(path.pgm,"functions_trilemma.r",sep=""))
@
The functions in this file generate the data, estimate filter coefficients, compute  ATS-components as well as alternative performance measures (Peak Correlation and Curvature, see section \ref{peco_cu} below) and amplitude and time-shift-functions. 
\item We specify the empirical design and run the code. Specifically, we select $\lambda=0$ (MSE) and $\lambda=2^k, k=0,...,7$ (emphasize Timeliness) and we fix $\eta=0$ (no emphasis of Smoothness).
<<echo=True>>=
#rm(list=ls())
# Specify the processes: ar(1) with coefficients -0.9,0.1 and 0.9
a_vec<-c(0.9,0.1,-0.9)
# Specify the lambdas
lambda_vec<-c(0,2^(0:7))
# Specify the fixed eta
eta_vec<-rep(0,length(lambda_vec))
# Specify filter length
L<-24
# Length of estimation sample
len<-120
# cutoff
cutoff<-pi/12
# Nowcast
Lag<-0
# No filter constraints
i1<-i2<-F
@
<<echo=False>>=
# Unscaled ATS-components: see below for an activation of this option
scaled_ATS<-F
# Generate a single realization of the processes
anzsim<-1
# Use periodogram
mba<-F
estim_MBA<-T
M<-len/2
# Length of symmetric filter (will be used later)
L_sym<-1000
# Length of long data (for computing the target)
len1<-3000
# difference data
dif<-F
@
<<echo=True>>=
# Proceed to estimation
for_sim_obj<-for_sim_out(a_vec,len1,len,cutoff,L,mba,estim_MBA,L_sym,
              Lag,i1,i2,scaled_ATS,lambda_vec,eta_vec,anzsim,M,dif)
@
<<echo=False>>=
# Extract sample performances
# 1 ATS
ats_sym_T<-for_sim_obj$ats_sym
# 2 Curvature, Peak Correlation, ...
amp_shift_mat_sim<-for_sim_obj$amp_shift_mat_sim
# 3. Amplitude and time-shifts
amp_sim_per<-for_sim_obj$amp_sim_per
shift_sim_per<-for_sim_obj$shift_sim_per
# 4. Output series
xff_sim<-for_sim_obj$xff_sim
# 5. Peak correlation and Curvature
amp_shift_mat_sim_T<-for_sim_obj$amp_shift_mat_sim
dim_names<-for_sim_obj$dim_names
i_process<-1
@
\item ATS-components and MSE for the first process ($a_1=0.9$) are summarized in table \ref{ats_comp_dfa_T}.\\
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(ats_sym_T[-1,,i_process,1], dec = 1,digits=6, caption = paste("ATS-Components as a function of lambda (eta=0 fixed)",sep=""),label=paste("ats_comp_dfa_T",sep=""),
center = "centering", file = "", floating = FALSE)
@
Let us briefly explain how the numbers in the above table were obtained (the same proceeding applies to all subsequent examples): 
\begin{itemize}
\item For each combination of $\lambda,\eta$ we obtain a corresponding filter from criterion \ref{idfa}.
\item ATS-components are then obtained by plugging the resulting amplitude and time-shift functions into \ref{mse_dec_ats}.
\item MSE is obtained as the sum of these ATS-components. It is an estimate of the true (unknown) mean-square filter error, recall section \ref{ex_dfa_1}.
\item This MSE-number does not correspond to the criterion value \ref{idfa} because the latter is `distorted' by the customization weights $\lambda,\eta\neq 0$.
\end{itemize}
Table \ref{ats_comp_dfa_T} as well as the following graphs will be analyzed all at once, at the end of the exercise.
\item Amplitude and time-shift functions for the first process are plotted in fig.\ref{z_box_plot_amp_and_shift_cust_T_1}.
<<echo=False>>=
for (DGP in 1:length(a_vec))#DGP<-3
{
  file = paste("z_box_plot_amp_and_shift_cust_T_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
  par(mfrow=c(1,2))
  mplot<-amp_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Amplitude (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-"Amplitude functions"
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  mplot<-shift_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Time-shifts (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-"Time-shifts"
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  invisible(dev.off())
}
@
<<label=z_box_plot_amp_and_shift_cust_T_1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_amp_and_shift_cust_T_1", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (left) and time-shift functions (right) as a function of lambda for fixed eta=0", sep = "")
  cat("\\label{z_box_plot_amp_and_shift_cust_T_1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Filter-outputs are plotted in fig.\ref{z_dfa_cust_ats_out_1_T}: series are standardized in order to facilitate visual inspection.
<<echo=FALSE>>=
# Plots
#colo<-c("red","blue")
# we select DFA-MSE (second series) and a customized 
series_vec<-2:(length(eta_vec)+1)#c(2,8)
for (ki in 1:length(a_vec)) #ki<-1  
{
file = paste("z_dfa_cust_ats_out_",ki,"_T.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
# extract all series from first realization (there is only one realization here)
  xf_per<-xff_sim[940:(940+len),,ki,1]#dim(xff_sim)
  dimnames(xf_per)[[2]]<-dim_names[[1]]
  anf<-1
  enf<-len
  sel<-1:dim(amp)[2]
  mplot<-scale(xf_per[,series_vec][anf:enf,])  #head(xf_per)
  plot(as.ts(mplot[,1]),type="l",axes=F,col="red",ylim=c(min(na.exclude(mplot)),
  max(na.exclude(mplot))),ylab="",xlab="",
  main=paste("MSE (red) vs. Customized",sep=""),lwd=1)
  mtext("MSE", side = 3, line = -1,at=(enf-anf)/2,col=colo[1])
  for (i in 2:length(series_vec))
  {
    lines(as.ts(mplot[,i]),col=colo[i],lwd=1)
    mtext(paste("Customized: ",dimnames(xf_per)[[2]][series_vec[i]],sep=""), side = 3, line = -i,at=(enf-anf)/2,col=colo[i])
  }
  axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
  labels=as.integer(anf+(0:6)*(enf-anf)/6))
  axis(2)
  box()

invisible(dev.off())
}
@
<<label=z_dfa_cust_ats_out_1_T.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats_out_1_T", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs: MSE (red) vs. customized , a1=0.9", sep = "")
  cat("\\label{z_dfa_cust_ats_out_1_T}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@

\end{enumerate}



\subsubsection{Analysis}
\begin{itemize}
\item Accuracy, Timeliness and Smoothness sum-up to total MSE in table \ref{ats_comp_dfa_T}. Residual vanishes because the target filter vanishes in the stopband.
\item Timeliness decreases with increasing $\lambda$, as desired. Accuracy and Smoothness as well as total MSE increase, as a consequence.
\item Amplitude and time-shift functions in fig.\ref{z_box_plot_amp_and_shift_cust_T_1} provide more insights. Timeliness decreases because the time-shift is uniformly decreasing in the passband, as a function of $\lambda$. Accuracy increases with increasing $\lambda$ because the amplitude function departs from the target ($\Gamma(\omega)$ in the stopband: leakage is increasing with $\lambda$. As a consequence, Smoothness deteriorates. 
\item Outputs of the customized filter in fig.\ref{z_dfa_cust_ats_out_1_T} tend to lie to the left (they are `faster') and the series are becoming increasingly noisy, as $\lambda$ increases.
\end{itemize}


\subsection{Smoothness Only: $\eta\geq 0$, $\lambda=0$ Fixed}\label{smoo_on}



\begin{enumerate}
\item We fix $\lambda=0$ (no emphasis of Timeliness) and let $\eta=k*0.3, k=0,...,6$ (Smoothness is emphasized).
<<echo=True>>=
# Specify the etas
eta_vec<-0.3*0:6
# Specify the fixed lambda
lambda_vec<-rep(0,length(eta_vec))
@
<<echo=False>>=
#rm(list=ls())
# Specify the processes: ar(1) with coefficients -0.9,0.1 and 0.9
a_vec<-c(0.9,0.1,-0.9)
# Ordinary ATS-components
scaled_ATS<-F
# Generate a single realization of the processes
anzsim<-1
# Specify filter length
L<-24
# Use periodogram
mba<-F
estim_MBA<-T
M<-len/2
L_sym<-1000
# Length of long data (for computing the target)
len1<-3000
# Length of estimation sample
len<-120
# cutoff
cutoff<-pi/12
# Real-time design
Lag<-0
# No constraints
i1<-i2<-F
# difference data
dif<-F
@
<<echo=True>>=
# Proceed to estimation
for_sim_obj<-for_sim_out(a_vec,len1,len,cutoff,L,mba,estim_MBA,L_sym,
             Lag,i1,i2,scaled_ATS,lambda_vec,eta_vec,anzsim,M,dif)
@
<<echo=False>>=
# Extract sample performances
# 1 ATS
ats_sym_S<-for_sim_obj$ats_sym
# 2 Curvature, Peak Correlation, ...
amp_shift_mat_sim<-for_sim_obj$amp_shift_mat_sim
# 3. Amplitude and time-shifts
amp_sim_per<-for_sim_obj$amp_sim_per
shift_sim_per<-for_sim_obj$shift_sim_per
# 4. Output series
xff_sim<-for_sim_obj$xff_sim
# 5. Peak correlation and Curvature
amp_shift_mat_sim_S<-for_sim_obj$amp_shift_mat_sim
# 6. Filter coefficients
b_sim<-for_sim_obj$b_sim
dim_names<-for_sim_obj$dim_names
i_process<-1
@

\item ATS-components and MSE for the first process ($a_1=0.9$) are summarized in table \ref{ats_comp_dfa_S}.
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(ats_sym_S[-1,,i_process,1], dec = 1,digits=6, caption = paste("ATS-Components as a function of eta (lambda=0 fixed)",sep=""),label=paste("ats_comp_dfa_S",sep=""),
center = "centering", file = "", floating = FALSE)
@
\item Amplitude and time-shift functions for the first process are plotted in fig.\ref{z_box_plot_amp_and_shift_cust_S_1}.
<<echo=False>>=
for (DGP in 1:length(a_vec))#DGP<-1
{
  file = paste("z_box_plot_amp_and_shift_cust_S_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
  par(mfrow=c(1,2))
  mplot<-amp_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Amplitude (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-"Amplitude functions"
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  mplot<-shift_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Time-shifts (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-"Time-shifts"
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  invisible(dev.off())
}
@
<<label=z_box_plot_amp_and_shift_cust_S_1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_amp_and_shift_cust_S_1", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (left) and time-shift functions (right) as a function of eta (lambda=0 fixed)", sep = "")
  cat("\\label{z_box_plot_amp_and_shift_cust_S_1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Filter coefficients can be seen in fig.\ref{z_box_plot_coef_S_1}.
<<echo=False>>=
for (DGP in 1:length(a_vec))#DGP<-1
{
  file = paste("z_box_plot_coef_S_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
  mplot<-b_sim[,-1,DGP,1]
  ax<-dimnames(mplot)[[1]]
  plot_title<-"Filter coefficients"
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  invisible(dev.off())
}
@
<<label=z_box_plot_coef_S_1.pdf,echo=FALSE,results=tex>>=   
  file = paste("z_box_plot_coef_S_1", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Filter coefficients as a function of eta (lambda=0 fixed)", sep = "")
  cat("\\label{z_box_plot_coef_S_1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Filter-outputs  are plotted in fig.\ref{z_dfa_cust_ats_out_1_S}: the series are standardized for ease of visual inspection.
<<echo=FALSE>>=
# Plots
#colo<-c("red","blue")
# we select DFA-MSE (second series) and a customized 
series_vec<-2:length(eta_vec)
for (ki in 1:length(a_vec)) #ki<-1  
{
file = paste("z_dfa_cust_ats_out_",ki,"_S.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
# extract all series from first realization (there is only one realization here)
  xf_per<-xff_sim[940:(940+len),,ki,1]#dim(xff_sim)
  dimnames(xf_per)[[2]]<-dim_names[[1]]
  anf<-1
  enf<-len
  sel<-1:dim(amp)[2]
  mplot<-scale(xf_per[,series_vec][anf:enf,])  #head(xf_per)
  plot(as.ts(mplot[,1]),type="l",axes=F,col="red",ylim=c(min(na.exclude(mplot)),
  max(na.exclude(mplot))),ylab="",xlab="",
  main=paste("MSE (red) vs. Customized",sep=""),lwd=1)
  mtext("MSE", side = 3, line = -1,at=(enf-anf)/2,col=colo[1])
  for (i in 2:length(series_vec))
  {
    lines(as.ts(mplot[,i]),col=colo[i],lwd=1)
    mtext(paste("Customized: ",dimnames(xf_per)[[2]][series_vec[i]],sep=""), side = 3, line = -i,at=(enf-anf)/2,col=colo[i])
  }
  axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
  labels=as.integer(anf+(0:6)*(enf-anf)/6))
  axis(2)
  box()
invisible(dev.off())
}
@
<<label=z_dfa_cust_ats_out_1_S.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats_out_1_S", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs MSE (red) vs. customized, a1=0.9", sep = "")
  cat("\\label{z_dfa_cust_ats_out_1_S}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@


\end{enumerate}


\subsubsection{Analysis}
\begin{itemize}
\item Smoothness in table \ref{ats_comp_dfa_S} decreases with increasing $\eta$, as required. Accuracy, Timeliness as well as total MSE increase, as a consequence.
\item Amplitude and time-shift functions in fig.\ref{z_box_plot_amp_and_shift_cust_S_1} offer a more detailed picture: for increasing $\eta$ the amplitude functions are approaching zero in the stopband at costs of the time-shifts which grow substantially in the passband. 
\item Fig.\ref{z_box_plot_coef_S_1} reveals that filter coefficients become smoother for increasing $\eta$. This desirable side-effect is obtained by imposing stronger shrinkage of the amplitude function in the (wide) stopband: degrees of freedom are implicitly freezed; in particular, we expect overfitting to be contained. 
\item As $\eta$ increases, outputs of the customized filter in fig.\ref{z_dfa_cust_ats_out_1_S} tend to lie to the right (delay) and the series appear to be increasingly smooth.
\end{itemize}



\subsection{Emphasizing Timeliness and Smoothness: $\lambda,\eta\geq0$}\label{l_e_geq_0}




\begin{enumerate}
\item We compare the MSE-design $\lambda=\eta=0$ with the strongly customized filter $\lambda=128,\eta=1.8$ which emphasizes Timeliness and Smoothness simultaneously.
<<echo=True>>=
# Specify the etas
eta_vec<-c(0,1.8)
# Specify the fixed lambda
lambda_vec<-c(0,128)
@
<<echo=False>>=
#rm(list=ls())
# Specify the processes: ar(1) with coefficients -0.9,0.1 and 0.9
a_vec<-c(0.9,0.1,-0.9)
# Ordinary ATS-components
scaled_ATS<-F
# Generate a single realization of the processes
anzsim<-1
# Specify filter length
L<-24
# Use periodogram
mba<-F
estim_MBA<-T
M<-len/2
L_sym<-1000
# Length of long data (for computing the target)
len1<-3000
# Length of estimation sample
len<-120
# cutoff
cutoff<-pi/12
# Real-time design
Lag<-0
# no constraints
i1<-i2<-F
# difference data
dif<-F
@
<<echo=True>>=
# Proceed to estimation
for_sim_obj<-for_sim_out(a_vec,len1,len,cutoff,L,mba,estim_MBA,L_sym,
              Lag,i1,i2,scaled_ATS,lambda_vec,eta_vec,anzsim,M,dif)
@
<<echo=False>>=
# Extract sample performances
# 1 ATS
ats_sym_ST<-for_sim_obj$ats_sym
# 2 Curvature, Peak Correlation, ...
amp_shift_mat_sim<-for_sim_obj$amp_shift_mat_sim
# 3. Amplitude and time-shifts
amp_sim_per<-for_sim_obj$amp_sim_per
shift_sim_per<-for_sim_obj$shift_sim_per
# 4. Output series
xff_sim<-for_sim_obj$xff_sim
# 5. Peak correlation and Curvature
amp_shift_mat_sim_ST<-for_sim_obj$amp_shift_mat_sim
dim_names<-for_sim_obj$dim_names
i_process<-1
@

\item ATS-components and MSE for the first process ($a_1=0.9$) are summarized in table \ref{ats_comp_dfa_ST_1}.
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(ats_sym_ST[-1,,i_process,1], dec = 1,digits=6, caption = paste("ATS-Components as a function of lambda and eta, a1=0.9",sep=""),label=paste("ats_comp_dfa_ST_1",sep=""),
center = "centering", file = "", floating = FALSE)
@


\item Amplitude and time-shift functions for the first process are plotted in fig.\ref{z_box_plot_amp_and_shift_cust_ST_1}.
<<echo=False>>=

for (DGP in 1:length(a_vec))#DGP<-3
{
  file = paste("z_box_plot_amp_and_shift_cust_ST_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", 
      width = 6, height = 3)
  par(mfrow=c(1,2))
  mplot<-amp_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Amplitude (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-paste("Amplitude functions: a1=",a_vec[DGP],sep="")
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  mplot<-shift_sim_per[,-1,DGP,1]
  dimnames(mplot)[[2]]<-paste("Time-shifts (",lambda_vec,",",eta_vec,")",sep="")
  ax<-rep(NA,ncol(mplot))
  ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
  plot_title<-paste("Time-shifts: a1=",a_vec[DGP],sep="")
  insamp<-1.e+90
  title_more<-dimnames(mplot)[[2]]
  colo<-rainbow(ncol(mplot))
  mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
  invisible(dev.off())
}

@
<<label=z_box_plot_amp_and_shift_cust_ST_1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_amp_and_shift_cust_ST_1", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (left) and time-shift functions (right) as a function of lambda and eta", sep = "")
  cat("\\label{z_box_plot_amp_and_shift_cust_ST_1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Filter-outputs of MSE and customized  filters are plotted in fig.\ref{z_dfa_cust_ats_out_1_ST} (series are standardized for ease of visual inspection).
<<echo=FALSE>>=
# Plots
#colo<-c("red","blue")
# we select DFA-MSE (second series) and a customized 
series_vec<-c(2,3)
for (ki in 1:length(a_vec)) #ki<-3  
{
# extract all series from first realization (there is only one realization here)
  file = paste("z_dfa_cust_ats_out_",ki,"_ST.pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 3)
  xf_per<-xff_sim[940:(940+len),,ki,1]#dim(xff_sim)
  dimnames(xf_per)[[2]]<-dim_names[[1]]
  anf<-1
  enf<-len
  sel<-1:dim(amp)[2]
  mplot<-scale(cbind(xf_per[,series_vec[1]],xf_per[,series_vec[2]])[anf:enf,])  #head(xf_per)
  plot(as.ts(mplot[,1]),type="l",axes=F,col="red",ylim=c(min(na.exclude(mplot)),
  max(na.exclude(mplot))),ylab="",xlab="",
  main=paste("MSE (red) vs. Customized (cyan): a1=",a_vec[ki],sep=""),lwd=1)
  mtext("MSE", side = 3, line = -1,at=(enf-anf)/2,col=colo[1])
  i<-2
  lines(as.ts(mplot[,i]),col=colo[2],lwd=2)
  mtext(paste("Customized: ",dimnames(xf_per)[[2]][series_vec[2]],sep=""), side = 3, line = -i,at=(enf-anf)/2,col=colo[2])
  axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
  labels=as.integer(anf+(0:6)*(enf-anf)/6))
  axis(2)
  box()
  invisible(dev.off())

}
@
<<label=z_dfa_cust_ats_out_1_ST.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats_out_1_ST", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs MSE (red) vs. customized (cyan), a1=0.9", sep = "")
  cat("\\label{z_dfa_cust_ats_out_1_ST}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@


\end{enumerate}


\subsubsection{Analysis}
\begin{itemize}
\item As confirmed in table \ref{ats_comp_dfa_ST_1}, the customized design improves in terms of Timeliness as well as of Smoothness, {simultaneously} at costs of a remarkable deterioration of Accuracy and, as a a consequence, of total MSE.
\item The time-shift of the customized design in fig.\ref{z_box_plot_amp_and_shift_cust_ST_1} is generally smaller in the passband (except at frequency zero). Its amplitude function is close to zero in the passband, as desired;  but the amplitude of the customized filter is also pulled towards zero in the passband (zero-shrinkage of the coefficients) which explains the excess Accuracy- and MSE-losses.   
\item The output of the customized filter in fig.\ref{z_dfa_cust_ats_out_1_ST} tends to lie to the left (faster) and it is smoother, as desired (recall that the series are standardized).
\end{itemize}
In order to grasp the observed zero-shrinkage of the customized filter we observe that  the scale-dependent Timeliness and Smoothness measures vanish if the amplitude function shrinks to zero. Accuracy alone lifts the amplitude away from zero (at least in the passband). Therefore, emphasizing heavily Smoothness and Timeliness, as we did in our example, exhausts to some extent Accuracy's action\footnote{The supernumerary amplitude-term in criterion \ref{idfatp} is counterproductive too, see section \ref{idfas}.}. Fortunately, this problem could be addressed by a simple re-scaling of the (customized) filter output\footnote{Recall that the series in fig.\ref{z_dfa_cust_ats_out_1_ST} were standardized.}, as we briefly explore below. 
<<echo=True>>=
# We allow for a re-calibration (by the inverse amplitude function in the passband)
scaled_ATS<-T
for_sim_obj<-for_sim_out(a_vec,len1,len,cutoff,L,mba,estim_MBA,L_sym,Lag,i1,
                        i2,scaled_ATS,lambda_vec,eta_vec,anzsim,M,dif)
@
<<echo=False>>=
# Extract sample performances
# 1 ATS
ats_sym_ST_re_scaled<-for_sim_obj$ats_sym
# 2. Amplitude and time-shifts
amp_sim_per_re_scaled<-for_sim_obj$amp_sim_per
# 3. Target
Gamma<-for_sim_obj$Gamma
i_process<-1
ats_scaled_unscaled<-rbind(ats_sym_ST[3,,i_process,1],ats_sym_ST_re_scaled[3,,i_process,1])
dimnames(ats_scaled_unscaled)[[1]]<-c("Customized unscaled","Customized re-scaled")
@
Specifically, we scale the filter coefficients by a constant which is inversely proportional to the  amplitude function in the passband\footnote{Alternatively one could fit an optimal MSE-normalization.}
\[
\textrm{Calibration~term}:=\frac{\textrm{Length~of~passband}}{\sum_{\textrm{Passband}}\hat{A}(\omega_k)}
\]
Fig. \ref{z_box_plot_amp_and_shift_cust_ST_1_scaled_unscaled} and table \ref{ats_comp_dfa_ST_1_scaled_unscaled} confirm that the previous zero-shrinkage is withdrawn.
<<echo=False>>=
DGP<-1
colo<-c("blue","cyan","orange")
passband<-1:(1+(length(amp_sim_per[,1+length(eta_vec),DGP,1])-1)*cutoff/pi)
file = paste("z_box_plot_amp_and_shift_cust_ST_",DGP,"_scaled_unscaled.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 3)
mplot<-cbind(Gamma,amp_sim_per[,1+length(eta_vec),DGP,1],amp_sim_per[,1+length(eta_vec),DGP,1]/mean(amp_sim_per[passband,1+length(eta_vec),DGP,1]))
dimnames(mplot)[[2]]<-c("Target","Amplitude original","Amplitude re-calibrated")
ax<-rep(NA,ncol(mplot))
ax[1+(0:6)*((nrow(mplot)-1)/6)]<-c(0,"pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi")
plot_title<-"Amplitude customized: original (cyan) vs. re-scaled (orange)"
insamp<-1.e+90
title_more<-dimnames(mplot)[[2]]
mplot_func(mplot, ax, plot_title, title_more, insamp, colo)
invisible(dev.off())
@
<<label=z_box_plot_amp_and_shift_cust_ST_1_scaled_unscaled.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_amp_and_shift_cust_ST_1_scaled_unscaled", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude functions of original (cyan) and scaled customized filter (orange)", sep = "")
  cat("\\label{z_box_plot_amp_and_shift_cust_ST_1_scaled_unscaled}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
In particular, Accuracy and MSE are relaxed, this time at costs of Smoothness and Timeliness.  
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(ats_scaled_unscaled, dec = 1,digits=6, caption = paste("ATS-Components of customized design: unscaled vs. scaled filter, a1=0.9",sep=""),label=paste("ats_comp_dfa_ST_1_scaled_unscaled",sep=""),
center = "centering", file = "", floating = FALSE)
@
We retain from the above examples that both the time-shift (delay) as well as the noise-suppression of a (real-time) filter can be addressed by a decomposition of the MSE-norm into ATS-components. However, The fact that Timeliness and Smoothness are scale-dependent measures hampers interpretability. Therefore we now propose an alternative set of scale-invariant measures which will allow for a straightforward evaluation of customized designs. 









\section{Curvature and Peak-Correlation}\label{peco_cu}

\subsection{Definition}

Define
\begin{eqnarray}\label{mse_2diff}
\textrm{Curvature}&:=&\frac{E\left[\Big((1-B)^2 \widehat{y}_t\Big)^2\right]}{\textrm{var}(\widehat{y_t})}\\
\label{peak_corr}
\textrm{Peak-Correlation}&:=&\textrm{Arg}\left(\max_j(cor(y_t,\widehat{y}_{t+j}))\right)
\end{eqnarray}
where $(1-B)^2$ is the second-order difference operator (curvature) and where $\textrm{Arg}\left(\max_j(cor(y_t,\widehat{y}_{t+j}))\right)$ means the lead or lag $j_0$ at which the correlation between the target $y_t$ and the estimate $\hat{y}_{t+j_0}$ is maximized\footnote{In applications, $y_t$ is generally unobserved and must be approximated by symmetric filters of finite order. Fortunately, the Peak Correlation concept is fairly robust against such approximations because the effective value of the correlation is irrelevant since we are solely interested in the lead/lag at which the correlation peaks.}. Note also that the covariance could be substituted to the correlation without altering the corresponding outcome.  Curvature and Peak Correlation are scale-invariant measures. As we shall show, they are linked to Smoothness and Timeliness. Empirical measures can be obtained by substituting  sample-estimates for unknown expectations in the above expressions. 


\subsection{Examples}  

We apply the above statistics to our previous example and complete the former tables \ref{ats_comp_dfa_T} (emphasizing Timeliness only), \ref{ats_comp_dfa_S} (emphasizing Smoothness only) and \ref{ats_comp_dfa_ST_1} (emphasizing Timeliness and Smoothness). 
<<label=print_tab_cor,echo=FALSE,results=tex>>=
i_process<-1
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(cbind(ats_sym_T[-1,,i_process,1],amp_shift_mat_sim_T[-1,3:4,i_process,1]), dec = 1,digits=6, caption = paste("ATS-Components, Peak-Correlation and Curvature: emphasizing Timeliness only, a1=0.9",sep=""),label=paste("ats_comp_dfa_T_1_pc",sep=""),
center = "centering", file = "", floating = FALSE)
@
\begin{itemize}
\item In table \ref{ats_comp_dfa_T_1_pc} Timeliness as well as Peak-Correlation decrease with increasing $\lambda$, as desired. The Peak Correlation is readily interpretable: in particular target series and real-time estimate appear to be  synchronized for $\lambda\geq 8$ (Peak Correlation vanishes).
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(cbind(ats_sym_S[-1,,i_process,1],amp_shift_mat_sim_S[-1,3:4,i_process,1]), dec = 1,digits=6, caption = paste("ATS-Components, Peak-Correlation and Curvature: emphasizing Smoothness only, a1=0.9",sep=""),label=paste("ats_comp_dfa_S_1_pc",sep=""),
center = "centering", file = "", floating = FALSE)
@
\item In table \ref{ats_comp_dfa_S_1_pc} Smoothness and Curvature decline with increasing $\eta$, as desired. The latter is readily interpretable in terms of normalized (inverse) signal-to-noise ratio. As a trade off, the Peak Correlation increases substantially. 
<<echo=False>>=
ats_scaled_unscaled_pc<-rbind(cbind(ats_sym_ST[2:3,,i_process,1],amp_shift_mat_sim_ST[-1,3:4,i_process,1]),c(ats_sym_ST_re_scaled[3,,i_process,1],amp_shift_mat_sim_ST[3,3:4,i_process,1]))
dimnames(ats_scaled_unscaled_pc)[[1]][3]<-"Scaled customized"
@
<<label=print_tab_cor,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(ats_scaled_unscaled_pc, dec = 1,digits=6, caption = paste("ATS-Components, Peak-Correlation and Curvature: emphasizing Timeliness and Smoothness, a1=0.9",sep=""),label=paste("ats_comp_dfa_ST_1_pc",sep=""),
center = "centering", file = "", floating = FALSE)
@
\item In table \ref{ats_comp_dfa_ST_1} the double-score of the customized design is evidenced by Peak Correlation and Curvature which remain unaffected by the normalization (last row), in contrast to Timeliness and Smoothness.      
\end{itemize}
We deduce from the above examples that the new Peak Correlation and Curvature statistics are simple to implement, simple to formalize and simple to interpret.


\subsection{Linking ATS-Components, Peak Correlation and Curvature}


Consider the second-order differences in the numerator of the Curvature statistic \ref{mse_2diff}

\begin{eqnarray*}
E\left[\Big((1-B)^2 \widehat{y}_t\Big)^2\right]&\approx&\frac{1}{T}\sum_{t=1}^T \left\{(1-B)^2 \widehat{y}_t\right\}^2\\
&=&\frac{2\pi}{T}\sum_{\textrm{All~frequencies}}I_{T\Delta^2\hat{Y}}(\omega_k)\\
&\approx&\frac{2\pi}{T}\sum_{\textrm{All~frequencies}}|1-\exp(-i\omega_k)|^4I_{T\hat{Y}}\\
&\approx&\frac{2\pi}{T}\sum_{\textrm{All~frequencies}}|1-\exp(-i\omega_k)|^4\hat{A}^2(\omega_k)I_{TX}
\end{eqnarray*}
where we assumed that $\hat{y}_{-1},\hat{y}_0$ were available and where $I_{T\Delta^2\hat{Y}}(\omega_k)$ is the periodogram of $(1-B)^2 \widehat{y}_t$. The first equality follows from \ref{spec_dec_per} and the subsequent approximations follow from \ref{conv_per}.\\
We can compare this expression to  Smoothness
\[S=\frac{2\pi}{ T} \sum_{\textrm{Stopband}} \hat{A}(\omega_k)^2 W(\omega_k,\eta) I_{TX}(\omega_k)\]
which is defined in the stopband, only. Assimilating $|1-\exp(-i\omega_k)|^4$ with $W(\omega_k,\eta)$ and ignoring the passband establishes a link between Curvature and Smoothness\footnote{Note also that we ignored the denominator $\textrm{var}(\widehat{y_t})$ in the Curvature expression: the latter ensures scale-invariance of the measure.}.\\

In order to relate Peak Correlation to Timeliness we consider the following development\footnote{Recall that the covariance could be substituted to the correlation without affecting numbers i.e. Peak Correlation and Peak Covariance are identical statistics.}
\begin{eqnarray*}
cov(y_t,\widehat{y}_{t+j})&\approx&\frac{1}{T}\sum_{t=1}^Ty_t\hat{y}_{t+j}\\
&\approx&\frac{2\pi}{T}\sum_{\textrm{All~frequencies}} \Re\left(\Xi_{TY}(\omega_k)\overline{\exp(ij\omega_k)\Xi_{T\hat{Y}}(\omega_k)}\right)\\
&\approx&\frac{2\pi}{T}\sum_{\textrm{All~frequencies}}\Re\left(\Gamma(\omega_k)\Xi_{TX}(\omega_k)\overline{\exp(ij\omega_k)\hat{\Gamma}(\omega_k)\Xi_{TX}(\omega_k)}\right)\\
&=&\frac{2\pi}{T}\sum_{\textrm{Passband}}A(\omega_k)\hat{A}(\omega_k) \Re\left(\overline{\exp(ij\omega_k-i\hat{\Phi}(\omega_k))}\right)I_{TX}(\omega_k)\\
&=&\frac{2\pi}{T}\sum_{\textrm{Passband}}A(\omega_k)\hat{A}(\omega_k) \cos(j\omega_k-\hat{\Phi}(\omega_k))I_{TX}(\omega_k)
\end{eqnarray*}
The second approximation is a consequence of proposition \ref{convolution theorem} in \ref{dis_con_app}. Let's assume, for a while, that $j$ is not a fixed integer but a general real-valued function of $\omega_k$. Then the last term in the above development would be maximized, as a function of $j(\omega_k)$, if $j(\omega_k)=\hat{\Phi}(\omega_k)/\omega_k=\hat{\phi}(\omega_k)$ (up to multiples of $2\pi$).  Therefore, reducing the time-shift $\hat{\phi}(\omega_k)$ in the passband, by emphasizing Timeliness, will decrease the lag at which the covariance or the correlation are peaking, which formalizes the link between Peak Correlation and Timeliness. 





\section{Double Score Against the MSE Paradigm}\label{double_score_ats}



\subsection{Introduction}

We here illustrate the flexibility of the ATS-trilemma by benchmarking empirical real-time designs (nowcasts), optimized in view of particular research priorities (speed/reliability), against the best theoretical \emph{MSE}-filter, assuming knowledge of the \emph{true DGP}. The empirical framework leans on the previous sections, see McElroy and Wildi (2015), and the whole R-code is carried over. In contrast to the previous sections, where we emphasized \emph{single} realization \emph{in-sample} results, we now consider \emph{multiple} realizations and compute in-sample as well as \emph{out-of-sample} {distributions} of performance measures of competing designs. Specifically, we analyze performances based on the classic (total) MSE-norm as well as on the scale-invariant Peak Correlation and Curvature measures introduced in the previous section. \\

Our contenders in this study will be the best theoretical MSE-estimate based on knowledge of the true DGP (benchmark), as well as the following empirical designs
\begin{itemize}
\item DFA-MSE: $\lambda=\eta=0$
\item Strong noise suppression: $\lambda=0,\eta=1.5$
\item Balanced (fast and smooth): $\lambda=30,\eta=1$
\item Very fast: $\lambda=500,\eta=0.3$
\end{itemize}



\subsection{Simulation Run}





Coefficients of the benchmark MSE-filters can be obtained by substituting the true spectral density
\[\left|\frac{\sigma^2}{1-a_1\exp(-i\omega)}\right|^2~,~a_1=-0.9,0.1,0.9\]
for the periodogram $I_{TX}(\omega)$ in \ref{dfatp}, setting $\lambda=\eta=0$, see chapter \ref{rep_sec} for further background\footnote{We implement an alternative time-domain solution in our R-code, see chapter \ref{rep_sec}.}. Note that benchmark filters do not depend on data, at all. Our target filter is the ideal trend with cutoff $\pi/12$.  
For the empirical DFA-filters we generate 100 realizations for each process and compute real-time (nowcast) filters of length $L=24$. The code leans on McElroy and Wildi (2015)\footnote{It is a faster-running `debugged' version based on reconciliation with a multivariate extension proposed in the next section.}:
\begin{enumerate}
\item Source the R-file
<<echo=TRUE>>=
source(file=paste(path.pgm,"functions_trilemma.r",sep=""))
@
The functions in this file generate the data, estimate filter coefficients, compute performance measures (in- and out-of-sample), ATS-components as well as amplitude and time-shift-functions. 
\item Specify the empirical design (same as in the previous section except for the number of realizations) and run the code. 
<<echo=True>>=
# Number of realizations
anzsim<-100
@
<<echo=False>>=
# Specify the processes: ar(1) with coefficients -0.9,0.1 and 0.9
a_vec<-c(0.9,0.1,-0.9)
# Ordinary ATS-components
scaled_ATS<-F
# Specify the lambdas
lambda_vec<-c(0,0,30,500)
# Specify the etas
eta_vec<-c(0,1.5,1,0.3)
# Specify filter length
L<-24
# Use periodogram
mba<-F
estim_MBA<-T
M<-len/2
# Length of symmetric target filter (for computing MSEs)
L_sym<-2*939
# Length of long data
len1<-2000
# Length of estimation sample
len<-120
# cutoff
cutoff<-pi/12
# Real-time design
Lag<-0
# no constraints
i1<-i2<-F
# difference data
dif<-F

@
<<echo=True>>=
# Proceed to simulation run
for_sim_obj<-for_sim_out(a_vec,len1,len,cutoff,L,mba,estim_MBA,L_sym,Lag,
                      i1,i2,scaled_ATS,lambda_vec,eta_vec,anzsim,M,dif)
@
<<echo=False>>=
# Extract sample performances
amp_shift_mat_sim<-for_sim_obj$amp_shift_mat_sim
amp_sim_per<-for_sim_obj$amp_sim_per
shift_sim_per<-for_sim_obj$shift_sim_per
xff_sim<-for_sim_obj$xff_sim
xff_sim_sym<-for_sim_obj$xff_sim_sym
ats_sym<-for_sim_obj$ats_sym
dim_names<-for_sim_obj$dim_names
@
\end{enumerate}

\subsection{Analysis}


\subsubsection{Performance Measures}

In order to save space we emphasize the second process $a_1=0.1$\footnote{Recall that this model fits log-returns of INDPRO quite well over a longer historical time span.}. Fig.\ref{z_box_plot_emp_per_perf_inout_2}  shows box-plots of in-sample (left) and out-of-sample (right) Curvature and Peak Correlation performances and fig.\ref{z_box_plot_emp_per_perf_mse_inout_2}  shows the corresponding sample MSEs: the latter are \emph{effective} time-domain measures
\[\frac{1}{120}\sum_{t=1}^{120}(y_t-\hat{y}_t)^2\]
where the target signal $y_t$ is obtained by applying a high-order (finite) symmetric trend filter to (very) long time series. Plots of the other two processes are shown in  section \ref{app_double_score_ats} in the Appendix \ref{dstt}.

<<echo=False>>=
# Boxplots performance measures: curvatures and peak-cor in and out-of-sample

colo<-c("red","orange","yellow","green","blue")#rainbow(length(lambda_vec)+1)

Perf_meas_sel<-c(3,7,4,8,5,9,6,10)
for (DGP in 1:length(a_vec))#DGP<-2
{
  file = paste("z_box_plot_emp_per_perf_inout_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
  par(mfrow=c(2,2))
  for (Perf_meas in Perf_meas_sel[1:4])
  {
    boxplot(list(amp_shift_mat_sim[1,Perf_meas,DGP,],amp_shift_mat_sim[2,Perf_meas,DGP,], amp_shift_mat_sim[3,Perf_meas,DGP,],amp_shift_mat_sim[4,Perf_meas,DGP,],amp_shift_mat_sim[5,Perf_meas,DGP,]),outline=T,names=c("Best MSE",paste("(",lambda_vec,",",eta_vec,")",sep="")),main=paste(dim_names[[2]][Perf_meas],", a1=",a_vec[DGP],sep=""),cex.axis=0.8,col=colo)
  }
  invisible(dev.off())
  file = paste("z_box_plot_emp_per_perf_mse_inout_",DGP,".pdf", sep = "")
  pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
  par(mfrow=c(1,2))
  for (Perf_meas in Perf_meas_sel[5:6])
  {
    boxplot(list(amp_shift_mat_sim[1,Perf_meas,DGP,],amp_shift_mat_sim[2,Perf_meas,DGP,],
    amp_shift_mat_sim[3,Perf_meas,DGP,],amp_shift_mat_sim[4,Perf_meas,DGP,],amp_shift_mat_sim[5,Perf_meas,DGP,]),outline=T,
    names=c("Best MSE",paste("(",lambda_vec,",",eta_vec,")",sep="")),
    main=paste(dim_names[[2]][Perf_meas],", a1=",a_vec[DGP],sep=""),cex.axis=0.8,col=colo,notch=F)
  }
  invisible(dev.off())
}
@

<<label=z_box_plot_emp_per_perf_inout_2.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_emp_per_perf_inout_2", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=5in, width=5in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distributions
  of Curvature and Peak-Correlation of best theoretical MSE (red), empirical MSE (orange), strong noise suppression (yellow), balanced fast and smooth (green) and very fast (blue) filters. All empirical filters are based on the periodogram:
  in-sample (left plots) and out-of-sample (right plots) for a1=0.1", sep = "")
  cat("\\label{z_box_plot_emp_per_perf_inout_2}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@



<<label=z_box_plot_emp_per_perf_mse_inout_2.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_emp_per_perf_mse_inout_2", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=2.5in, width=5in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distributions
  of Sample MSEs of best theoretical MSE (red), empirical MSE (orange), strong noise suppression (yellow), balanced fast and speed (green) and very fast (blue) filters. All empirical filters are based on the periodogram:
  in-sample (left plots) and out-of-sample (right plots) for a1=0.1", sep = "")
  cat("\\label{z_box_plot_emp_per_perf_mse_inout_2}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@

\textbf{Findings}
\begin{itemize}
\item The customized designs perform as expected. In particular the balanced design (green) outperforms the benchmark MSE-filter both in terms of Curvature as well as Peak-Correlation, in- and out-of-sample. 
\item In- and out-of-sample performances are congruent\footnote{The distributions of the integer-valued Peak Correlations are almost identical due, in part, to discretization effects.}.  
\item A comparison of in- and out-of-sample MSE-performances of benchmark (red) and empirical (orange) filters is indicative for overfitting: the orange filter performs slightly better in-sample but it is slightly outperformed out-of-sample. Interestingly, overfitting is moderate\footnote{In contrast to classic forecast approaches, which target an allpass filter, our examples target a lowpass filter with a wide stop-band. A close fit of the target in the stopband mimics, in some ways, classic shrinkage approaches or, stated otherwise, degrees of freedom are implicitly controlled by the DFA/MDFA-criteria.} even in the case of richly parametrized designs ($L=$\Sexpr{L} coefficients are estimated in samples of length $T=$\Sexpr{len}).
\item The customized designs are systematically outperformed in terms of MSE-performances in- and out-of-sample, as expected. 
\end{itemize}




\subsubsection{Real-Time Filter Outputs}


Benchmark-MSE (red) and customized-balanced (green) filter-outputs are compared in fig.\ref{z_dfa_cust_ats_mba_per_e} (both time series are standardized for ease of visual inspection). The chosen time-span covers in-sample as well as out-of-sample periods. 
<<echo=False>>=
amp_shift_mat_sim<-for_sim_obj$amp_shift_mat_sim
amp_sim_per<-for_sim_obj$amp_sim_per
shift_sim_per<-for_sim_obj$shift_sim_per
xff_sim<-for_sim_obj$xff_sim
xff_sim_sym<-for_sim_obj$xff_sim_sym
ats_sym<-for_sim_obj$ats_sym
dim_names<-for_sim_obj$dim_names
for (ki in 2:2) #ki<-2  
{
file = paste("z_dfa_cust_ats_mba_per_e.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

  xf_per<-xff_sim[940:(940+2*len),,ki,10]
  dimnames(xf_per)[[2]]<-dim_names[[1]]
  anf<-1
  enf<-2*len
  sel<-1:dim(amp)[2]
  mplot<-scale(cbind(xf_per[,1],xf_per[,4])[anf:enf,])  #head(xf_per)
  plot(as.ts(mplot[,1]),type="l",axes=F,col="red",ylim=c(min(na.exclude(mplot)),
  max(na.exclude(mplot))),ylab="",xlab="",
  main=paste("Benchmark MSE (red) vs. Customized balanced (green)",sep=""),lwd=2)
  mtext("in sample",side = 3, line = -1,at=60,col="black")
  mtext("out-of-sample",side = 3, line = -1,at=180,col="black")
  mtext("Benchmark MSE", side = 3, line = -1,at=(enf-anf)/2,col="red")
  i<-2
  lines(as.ts(mplot[,i]),col=colo[4],lwd=2)
  mtext(paste("Customized: ",dimnames(xf_per)[[2]][4],sep=""), side = 3, line = -i,at=(enf-anf)/2,col=colo[4])
  abline(v=120)
  axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
  labels=as.integer(anf+(0:6)*(enf-anf)/6))
  axis(2)
  box()

invisible(dev.off())
}
@

<<label=z_dfa_cust_ats_mba_per_e.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats_mba_per_e", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Outputs of benchmark MSE (red) and customized balanced (green) filters: a1=0.1. In-sample (left half) and out-of-sample (right half)", sep = "")
  cat("\\label{z_dfa_cust_ats_mba_per_e}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")

@
The customized filter outperforms the benchmark-MSE design on both accounts: it lies on the left (faster) and it is smoother both in-sample as well as out-of-sample. 







\section{Univariate Customized Design vs. Bivariate MSE Leading-Indicator}\label{ucdvbmseli}

In this section we stiffen the competition by benchmarking the univariate customized filter against a bivariate MSE-design with a leading indicator. The empirical design is otherwise identical to the previous section. 



\subsubsection{Data and Contenders}

We emphasize performances in the case of the second process ($a_1=0.1$) and compare the univariate MSE-DFA $\lambda=\eta=0$ (orange), the univariate balanced customized DFA $\lambda=30$,$\eta=1$ (green)) and the bivariate MDFA-MSE design proposed in section \ref{bimdfaudfa}.
<<echo=True>>=
# Second process
a1<-0.1
# Customization settings DFA
lambda_vec<-c(0,30)
eta_vec<-c(0,1)
@
<<echo=False>>=
# target
cutoff<-pi/12
len1<-2000
len<-120
L<-24
Lag<-0
i1<-i2<-F
# MDFA: MSE design
lambda_mdfa<-eta_mdfa<-0
@
<<echo=True>>=   
# Run the competition: the new function handles the multivariate case
cust_leading_obj<-mdfa_mse_leading_indicator_vs_dfa_customized(anzsim,
                  a1,cutoff,L,lambda_vec,eta_vec,len1,len,i1,i2,Lag,
                  lambda_mdfa,eta_mdfa,troikaner)  
@



\subsection{Performances: Curvature}

Box-plots of in-sample and out-of-sample Curvature-scores are depicted in fig.\ref{z_curv_dfacust_leadind}.
<<echo=False>>=
file = paste("z_curv_dfacust_leadind.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
boxplot(list(cust_leading_obj$perf_in_sample[,1,1],cust_leading_obj$perf_in_sample[,1,2],cust_leading_obj$perf_in_sample[,1,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("Curvature in-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))

boxplot(list(cust_leading_obj$perf_out_sample[,1,1],cust_leading_obj$perf_out_sample[,1,2],cust_leading_obj$perf_out_sample[,1,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("Curvature out-of-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))
invisible(dev.off())
@
<<label=z_curv_dfacust_leadind.pdf,echo=FALSE,results=tex>>=
  file = paste("z_curv_dfacust_leadind", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Curvature: mean-square DFA (orange), customized DFA (green) and MSE-MDFA leading indicator (brown): a1=0.1 In-sample (left) and out-of-sample (right).", sep = "")
  cat("\\label{z_curv_dfacust_leadind}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The empirical distributions of the univariate designs coincide with those reported in the previous section (same random-seed). The customized design (green) outperforms both contenders in-sample as well as out-of-sample. In- and out-of-sample figures are remarkably similar considering that the bivariate design (brown) relies on \Sexpr{2*L} estimated coefficients, in realizations of length $T=$\Sexpr{len}, only. 



\subsection{Performances: Peak-Correlation}

Box-plots of in-sample and out-of-sample Peak-Correlation scores are shown in fig.\ref{z_peak_cor_dfacust_leadind}.

<<echo=False>>=
file = paste("z_peak_cor_dfacust_leadind.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(1,2))
boxplot(list(cust_leading_obj$perf_in_sample[,2,1],cust_leading_obj$perf_in_sample[,2,2],cust_leading_obj$perf_in_sample[,2,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("Peak-Correlation in-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))

boxplot(list(cust_leading_obj$perf_out_sample[,2,1],cust_leading_obj$perf_out_sample[,2,2],cust_leading_obj$perf_out_sample[,2,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("Peak-Correlation out-of-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))

invisible(dev.off())
@
<<label=z_peak_cor_dfacust_leadind.pdf,echo=FALSE,results=tex>>=
  file = paste("z_peak_cor_dfacust_leadind", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Peak Correlation: mean-square DFA (orange), customized DFA (green) and MSE-MDFA leading indicator (brown): a1=0.1 In-sample (left) and out-of-sample (right).", sep = "")
  cat("\\label{z_peak_cor_dfacust_leadind}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The customized design (green) substantially outperforms both contenders: it anticipates the DFA-MSE (orange) by \Sexpr{round(median(cust_leading_obj$perf_in_sample[,2,2])-median(cust_leading_obj$perf_in_sample[,2,1]),1)} time-units in the median; remarkably, it also outperforms the leading-indicator design (brown) by \Sexpr{round(median(cust_leading_obj$perf_in_sample[,2,2])-median(cust_leading_obj$perf_in_sample[,2,3]),1)} time-unit\footnote{Setting $\lambda=100$ would result in an anticipation of the customized design by 2 time-units, at costs of Accuracy and Smoothness.}. As expected, the univariate MSE-DFA (orange) is lagging the MSE-leading-indicator design (brown) by \Sexpr{round(median(cust_leading_obj$perf_in_sample[,2,1])-median(cust_leading_obj$perf_in_sample[,2,3]),1)} time-unit which reflects the lead-time provided by  the additional leading indicator.




\subsection{Performances: MSE}


Box-plots of in-sample and out-of-sample MSE-scores are shown in fig.\ref{z_MSE_dfacust_leadind}.
<<echo=False>>=
file = paste("z_MSE_dfacust_leadind.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
boxplot(list(cust_leading_obj$perf_in_sample[,3,1],cust_leading_obj$perf_in_sample[,3,2],cust_leading_obj$perf_in_sample[,3,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("MSE in-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))

boxplot(list(cust_leading_obj$perf_out_sample[,3,1],cust_leading_obj$perf_out_sample[,3,2],cust_leading_obj$perf_out_sample[,3,3]),outline=T,names=c(paste("DFA(",lambda_vec,",",eta_vec,")",sep=""),"MDFA-MSE Leading Indicator"),main=paste("MSE out-of-sample, a1=",a1,sep=""),cex.axis=0.8,col=c("orange","green","brown"))
invisible(dev.off())
@
<<label=z_MSE_dfacust_leadind.pdf,echo=FALSE,results=tex>>=
  file = paste("z_MSE_dfacust_leadind", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=3in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{MSE: mean-square DFA (orange), customized DFA (green) and MSE-MDFA leading indicator (brown): a1=0.1 In-sample (left) and out-of-sample (right).", sep = "")
  cat("\\label{z_MSE_dfacust_leadind}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")

@
The customized design (green) is outperformed by the MSE-designs, as expected, in-sample as well as out-of-sample: re-scaling of filter coefficients would mitigate, at least to some extent, the observed losses, see section \ref{l_e_geq_0}. The customized design seems to be less sensitive to overfitting: an explanation has been provided in section \ref{smoo_on}, recall fig.\ref{z_box_plot_coef_S_1}. 





\subsection{Filter Outputs}

The filter outputs corresponding to the last realization of the process are plotted in fig.\ref{z_dfa_cust_mdfa_leading_indicator}(the series are scaled for ease of visual inspection).
<<echo=FALSE>>=
# Plots
file = paste("z_dfa_cust_mdfa_leading_indicator.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
mplot<-scale(cust_leading_obj$filter_output_in_sample) 
dimnames(mplot)[[2]]<-dimnames(cust_leading_obj$filter_output_in_sample)[[2]]
colo_cust<-c("orange","green","brown")
plot(as.ts(mplot[,1]),type="l",axes=F,col=colo_cust[1],ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))),ylab="",xlab="",main=paste("Filter outputs: last realization",sep=""),lwd=1)
mtext(dimnames(mplot)[[2]][1], side = 3, line = -1,at=nrow(mplot)/2,col=colo_cust[1])
for (i in 2:(ncol(mplot)-1))
{
  lines(mplot[,i],col=colo_cust[i],lwd=1)
  mtext(dimnames(mplot)[[2]][i], side = 3, line = -i,at=nrow(mplot)/2,col=colo_cust[i])
}
axis(1,at=c(1,rep(0,6))+as.integer((0:6)*nrow(mplot)/6),
labels=c(1,rep(0,6))+as.integer((0:6)*nrow(mplot)/6))
axis(2)
box()
invisible(dev.off())

@
<<label=z_dfa_cust_mdfa_leading_indicator.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_mdfa_leading_indicator", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Scaled outputs of DFA-MSE (orange), DFA-balanced (green) and bivariate MDFA-MSE (brown): a1=0.9", sep = "")
  cat("\\label{z_dfa_cust_mdfa_leading_indicator}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")

@
The customized DFA (green) appears faster and smoother, as expected. Note that the leading-indicator design (brown) is systematically faster than DFA-MSE (orange) in the turning-points.  \\

We retain from the above example that a suitably customized \emph{univariate} design can outperform a classic MSE bivariate leading-indicator design, in-sample and out-of-sample, in terms of Peak Correlation and Curvature. A direct comparison of univariate and bivariate MSE-designs illustrates that the latter outperforms the former merely in terms of Peak Correlation (due to the leading indicator); gains in terms of MSE- or Curvature are moderate, at least out-of-sample.



\section{An Extended Customization Triplet}\label{customization_triplet}

Overemphasizing the Timeliness contribution to the MSE-norm, in the schematic criterion \ref{ats_cust} or in criterion \ref{dfatp}, provides a direct control of the magnitude of the time-shift of a real-time filter in the passband: as a result the filter-output shifts to the left, as confirmed by the Peak-Correlation statistic. As a possible alternative, the time-shift contribution to the MSE-norm can be overemphasized by targeting a lead $l>0$ of the signal 
\begin{equation}\label{forecast_targ_spe}
\exp(i l \omega)\Gamma(\omega)
\end{equation}
where $\Gamma(\omega)$ designates the original target. Note that $l$ corresponds to $-Lag$ in our R-code. In order to simplify the exposition we assume that $\Gamma(\omega)\geq 0$ is the transfer function of the ideal trend. Then 
\[
\Phi(\omega)=-Arg(\exp(i l \omega)\Gamma(\omega))=-l\omega
\]
and the mean-square error becomes
\begin{eqnarray}
MSE&=&\frac{2\pi}{ T} \sum_{\textrm{Passband}} (\Gamma(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (\Gamma(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4\Gamma(\omega_k)\hat{A}(\omega_k)\sin\left(\frac{\hat{\Phi}(\omega_k)+l\omega_k}{2}\right)^2 I_{TX}(\omega_k) \label{once_more_timeliness}
\end{eqnarray}
where we used the fact that $\Gamma(\omega)=A(\omega)$ since the transfer function is positive everywhere.
Since our target is the ideal lowpass, we expect that $\hat{\Phi}(\omega_k)>0$ in the passband: the output is shifted to the right with respect to the target. Therefore 
\[
\sin\left(\frac{\hat{\Phi}(\omega_k)+l\omega_k}{2}\right)^2>\sin\left(\frac{\hat{\Phi}(\omega_k)}{2}\right)^2
\]
if $l>0$. As a result, the Timeliness term \ref{once_more_timeliness} inflates, in contrast to Accuracy and Smoothness, which remain unaffected by the lead-time. The resulting effect must be similar, though not identical, to a specific magnification of Timeliness in the ATS-trilemma and therefore we expect the output of the resulting \emph{forecast} filter ($Lag=-l<0$) to lie to the left of the corresponding nowcast filter ($Lag=0$).\\


Instead of interpreting the forecast lead as a possible (indirect) alternative for controlling Timeliness, in lieu of $\lambda$, say, we envisage a combination of $l$ and of the customization parameters: we consider the triplet $(l, \lambda, \eta)$ as a natural extension of the original customization pair $(\lambda,\eta)$. The additional `customization' parameter $l>0$ is helpful and effective if the time-shift of a particular real-time design is small already, in which case Timeliness cannot be substantially reduced anymore by selecting $\lambda>0$: a corresponding example is provided in section \ref{customization_cf} (customization of the Christiano Fitzgerald bandpass design). More generally and more formally, Timeliness is a measure of the phase error (the contribution of the phase to MSE)
\[
\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin\left(\frac{\hat{\Phi}(\omega_k)-\Phi(\omega_k)}{2}\right)^2
I_{TX}(\omega_k) 
\]
where $\Phi(\omega_k)=-l \omega_k$. Selecting $\lambda>0$, in criteria \ref{ats_cust} or \ref{dfatp}, shrinks the Timeliness term by pulling $\hat{\Phi}(\omega_k)$ towards $-l \omega_k$ or, equivalently, by attracting the time-shift $\hat{\phi}(\omega_k)=\hat{\Phi}(\omega_k)/\omega_k$ towards $-l$. In contrast to the original customization pair $(\lambda,\eta)$, the extended triplet $(l,\lambda,\eta)$ allows to design filters with more general lead or lag properties. A worked-out example is provided in chapter \ref{rep_sec}, section \ref{customization_cf}.





\section{Summary}

\begin{itemize}
\item The MSE-norm can be split into Accuracy, Timeliness and Smoothness components\footnote{The Residual either vanishes or is negligible in practice.}. 
\item The MSE-paradigm is replicated by weighting ATS-components equally. Equal-weighting reflects one particular `diffuse' research priority.
\item A strict MSE-approach is unable, per definition, to address Timeliness and Smoothness, either separately or all together.
\item The ATS-trilemma degenerates to a AT-dilemma in the case of classic (allpass) forecasting. Stated otherwise: classic (quasi maximum likelihood) forecast approaches have a blind spot. 
\item Curvature and  Peak-Correlation performances can be addressed simultaneously by customized designs. 
%\item In a growth-perspective -- data and signals in {first differences} -- suitably customized designs improve `mechanically'  MSE-performances at the important turning-points of a time series;  at costs of MSE-performances at all other time points, between consecutive turning-points. 
\item The DFA and the resulting ATS-trilemma are generic concepts: they allow for arbitrary target signals as well as arbitrary spectral estimates. Model-based targets and (pseudo) spectral densities are addressed in chapter \ref{rep_sec}.
\item The generic customization triplet $(l,\lambda,\eta)$ extends the action of the ATS-trilemma to arbitrary real-valued lead or lag specifications. 
\item The DFA/MDFA optimization criteria appear to be surprisingly robust against overfitting. Emphasizing Smoothness reinforces this statement.
\end{itemize}
%Paradoxon 1: (real-time) filters with enhanced speed and noise-suppression properties do not necessarily perform better in terms of MSE-performances, quite the contrary. Paradoxon 2: directional forecast properties are not always improved by emphasizing Smoothness and Timeliness in the ATS-trilemma. For this to happen, it requires a time series -- a process --  which supports directional inference.  

In the previous chapters we analyzed the impact on real-time filters of the level and time shift constraints, namely 
 the parameters $i1$, $i2$ (Chapter 4), and {\it Lag} (Chapter 3).  
%We now discuss $\lambda, \eta$.  
  In this chapter we explore an extension of the classical MSE criterion by means of a decomposition of
signal extraction error into components corresponding to Accuracy, Timeliness, and Smoothness (ATS).  
 With this decomposition, we are empowered to address important user-priorities that are relevant to nowcasting
and forecasting applications.

% (backcasts were discussed in chapter 3).

\section{Introduction}
%
% DFA-MSE tackles the relevant problem structure. But user-priorities are not addressed yet.
% Replicate Smoothness and Tilmeliness performances (use function in MDFA-code) of leading
%indicator design in section 2.7.2 by a univariate customized design. Costs: Accuracy worsens.


The evaluation of economic data and monitoring of the economy is often concerned with an assessment of mid- and long-term dynamics of
time series (trend and/or cycle). For this purpose a broad range of time-proven techniques are available
to the analyst: ARIMA-based approaches (e.g., TRAMO-SEATS (Maravall and Caparello, 2004)), State-Space methods
(e.g., STAMP (Koopman, Harvey, Doornik, Shepherd (2000)), and classic filters (e.g., Hodrick-Prescott (HP) or
Cristiano-Fitzgerald (CF); see Hodrick and Prescott (1997) and Cristiano and Fitzgerald (2003))
are widely-used benchmarks.  
%
%Frequently, one is interested in the most recent estimate of a target signal; the most recent estimate depends on only
%present and past data, and is referred to as a real-time estimate.  Unfortunately, performances of
%real-time (or concurrent) filters generally differ from performances of historical
%estimates (these historical estimators are sometimes called ``smoothers") because, trivially, the former
%cannot rely on future data, i.e., real-time filters are asymmetric. We here address performances
%of real-time designs (although the method can be easily extended to smoothing) by proposing a generic
%Direct Filter Approach (DFA) that was first introduced in Wildi (2005). 
%
 Interest focuses on the most recent estimate of a target signal, and the DFA can be modified to enforce
 the presence of certain desirable properties -- such as smoothness and timeliness -- by direct design of the
real-time filter.

In order to impose these desirable properties on a filter, we proceed by decomposing the filter  MSE 
 into components that correspond to the key properties of a filter.  It is well-known
(Priestley, 1981) that any linear filter is characterized by its frequency response function (frf), i.e.,
the Discrete Fourier Transform (DFT) of its filter coefficients, and the frf can be decomposed in terms
of a gain function and a phase function.  The former governs smoothness of filter estimates,
describing the so-called pass-band and stop-band, whereas the latter governs timeliness of the filter,
controlling the advance or retardation in time of underlying harmonics.  Here, we make a novel connection
of these concepts with the signal extraction MSE.  We decompose the ordinary MSE into
Accuracy, Timeliness and Smoothness error components, and we propose a new two-dimensional tradeoff between these
conflicting terms, the so-called ATS-trilemma.  This is analogous, but not mathematically equivalent to, the
bias-variance tradeoff that arises from the classical decomposition of MSE of a parameter estimator.

With this formalism, we are able to derive a general class of
optimization criteria that allow the user to address specific research priorities, in terms of
the Accuracy, Timeliness and Smoothness properties of the resulting real-time filter.  We call such a
criterion a  {\it customization}.  Although customization for real-time signal extraction can be achieved via
other methodologies, the MSE decomposition developed below offers the most direct and
compelling connection between parameters and the corresponding characteristics of a real-time filter.
For example, in a model-based framework (e.g., TRAMO-SEATS or STAMP) one could adjust model orders and/or
model parameters to achieve modifications to smoothness (e.g., increase the integration order for the
trend to obtain a smoother real-time trend), but the connection of such adjustments to the phase function
of the concurrent filter arising from such models is much harder to discern; moreover, knowing the
phase function alone does not provide information about its impact on MSE -- and one is ultimately
concerned about signal extraction error.  If instead one advocates using an asymmetric version of a
popular nonparametric filter, such as the HP, CF, or ideal low-pass filter, there are few parameters
 available to adjust the phase function, and the story is much the same as in the model-based framework.

The relationship between filter parameters and filter behavior is explicit in the ATS paradigm.
%
% So while we acknowledge that other methodologies provide a connection between parameters and vague
% notions of smoothness and phase, our contribution here is to make these connections mathematically
% explicit through the ATS decomposition.
  The ATS decomposition can in fact be used to analyze and
 customize a model-based concurrent filter, or even a nonparametric concurrent filter.  We advocate
 working with a richer class of concurrent filters, that allow for more flexibility in terms of gain
 and phase functions, than is typically possible with model-based or nonparametric approaches.

 If one were to weight squared bias and variance by some convex combination -- where equal weights
 correspond to the estimator's MSE -- one could emphasize either aspect, depending upon user priorities.
 Such a customization results in a one-dimensional space -- or curve -- of criteria, the classical MSE
 being but the central point.  Similarly, real-time signal extraction customization results in a
 two-dimensional space -- or triangle -- of criteria, wherein the MSE lies in the center, and the
 vertices correspond to exclusive emphasis on either Accuracy, Timeliness, or Smoothness. Thus,
traditional approaches can be replicated perfectly by DFA, and can
also be customized. 

Interestingly, the two-dimensional tradeoff collapses to a bipolar dilemma in a classic
one- or multi-step ahead forecast perspective, which we demonstrate below (essentially because the notions
of pass-band and stop-band are irrelevant in forecasting problems).  The key facet here is that MSE can be
decomposed into a summation of constituent error measures that correspond to useful quantities of interest.
Just as with a classical estimator, where it is useful to examine both its squared bias and its variance --
described heuristically as accuracy and precision respectively -- here in the context of signal
extraction  it is useful to decompose MSE into three components of Accuracy, Timeliness, and
Smoothness.  As in nonparametric density estimation, where altering the bandwidth can decrease bias
at the expense of higher variance, or vice versa, so here an improvement of one of the ATS components
typically entails a deterioration in one or both of the other components.
%
%The main concepts are introduced in Section \ref{ats-tr}. We then replicate and customize a simple
%generic model-based filter in Section \ref{repl_cust}, to demonstrate that DFA includes the classical
%approaches, while being more flexible.   We propose classic time-domain performance measures,
%namely peak-correlation (for Timeliness) and
%curvature (mean second-order differences for Smoothness) and illustrate that the customized design can outperform
%model-based filters in both aspects simultaneously, out-of-sample.
%Due to space limitations we here restrict the exposition to univariate approaches, though customization
%and the underlying trilemma readily extend to multivariate approaches; this is currently being
%investigated by the authors.


\section{The ATS-Trilemma and Customization} \label{ats-tr}

We introduce the concepts through the univariate case.
As in earlier chapters, we now assume that $\{ x_t \}$ is a weakly
stationary process with a continuous spectral density $h$ (the DFT of the autocovariance function)
such that
\begin{eqnarray}\label{eq:chapats_dfa_fd}
 \EE \left[(y_{t}-\widehat{y}_{t})^2\right]&=&\frac{1}{2 \pi} \,
 \int_{-\pi}^{\pi}|\Gamma(\omega)-\widehat{\Gamma}(\omega)|^2h(\omega)d\omega,
\end{eqnarray}
 which follows from the fact that the filter error $y_t - \widehat{y}_t = ( \Gamma (B) - \widehat{\Gamma} (B) ) x_t$, which
 has spectral density 
\begin{equation}
 \label{eq:chapats_filterErr}
 {| \Gamma (e^{-i \omega}) - \widehat{\Gamma} (e^{-i \omega}) |}^2 \, h(\omega),
\end{equation}
as discussed in Brockwell and Davis (1991).  Replication of traditional model-based (or classical) filter-designs is obtained by
plugging the corresponding target signal $\Gamma(\omega)$ and the corresponding
spectral density $h$ into (\ref{eq:chapats_dfa_fd}).  Now decomposing (\ref{eq:chapats_filterErr}) we obtain the 
 following identity:
\begin{align}
|\Gamma(\omega)-\widehat{\Gamma}(\omega)|^2 &= A(\omega)^2+\widehat{A}(\omega)^2-2A(\omega)
\widehat{A}(\omega)\cos\left(\Phi (\omega)- \widehat{\Phi} (\omega)\right)  \notag \\
  &=  (A(\omega)-\widehat{A}(\omega))^2 + 4 \, A(\omega)\widehat{A}(\omega)\sin\left(\frac{\Phi (\omega)-\widehat{\Phi} (\omega)}{2}\right)^2
   \label{eq:chapats_etrigid}
\end{align}
where $A(\omega)=|\Gamma(\omega)|$, $\widehat{A}(\omega)=|\widehat{\Gamma}(\omega)|$ are amplitude
functions and $\Phi(\omega)=Arg(\Gamma(\omega))$, $\widehat{\Phi}(\omega)=Arg(\widehat{\Gamma}(\omega))$ are
phase functions of the filters involved.
% In the case of typical signal extraction problems --
% where $\Gamma$ is a symmetric filter -- the phase of the target vanishes $\Phi(\omega)=0$; in the case of $h-$step ahead forecasting,
%the phase becomes $\Phi(\omega)=h\omega$. 
We now plug (\ref{eq:chapats_etrigid}) into
(\ref{eq:chapats_dfa_fd}) and obtain the following decomposition of the signal extraction MSE:
\begin{eqnarray*}
\frac{1}{2 \pi} \,
 \int_{-\pi}^{\pi}|\Gamma(\omega)-\widehat{\Gamma}(\omega)|^2h(\omega)d\omega&=&\int_{-\pi}^{\pi}
(A(\omega)-\widehat{A}(\omega))^2 h(\omega)d\omega\label{unboptidioe}\\
&&+4\int_{-\pi}^{\pi}A(\omega)\widehat{A}(\omega))\sin\left(\frac{\Phi (\omega)- \widehat{\Phi} (\omega)}{2}\right)^2 \,
h(\omega) \, d\omega.
\end{eqnarray*}
In the case of signal extraction it is useful to introduce the concepts of pass-band and stop-band, which respectively
refer to those frequencies in the data that we either wish our filter to transmit or to annihilate.  For example, 
we can specify pass-bands and stop-bands of a filter by
\begin{eqnarray*}
\textrm{pass-band}&=&\{\omega|A(\omega)\geq 0.5\}                          \\
\textrm{stop-band}&=&[-\pi,\pi] \setminus \textrm{pass-band}
\end{eqnarray*}
 Of course, there are many other ways in which the pass-band and stop-band might be defined.  Given such 
a definition, the original MSE can be decomposed additively into the following four terms:
\begin{eqnarray}
\textrm{A(ccuracy)}&:=&\int_{\textrm{pass-band}}(A(\omega)-\widehat{A}(\omega))^2 \, h(\omega) \, d\omega\label{eq:chapats_Ats}\\
\textrm{T(imeliness)}&:=&4\int_{\textrm{pass-band}}A(\omega)\widehat{A}(\omega)\sin\left(\frac{\Phi (\omega)- \widehat{\Phi} (\omega)}{2}\right)^2
 \, h(\omega) \, d\omega\label{eq:chapats_aTs}\\
\textrm{S(moothness)}&:=&\int_{\textrm{stop-band}}(A(\omega)-\widehat{A}(\omega))^2 \, h(\omega) \, d\omega\label{eq:chapats_atS}\\
\textrm{R(esidual)}&:=&4\int_{\textrm{stop-band}}A(\omega)\widehat{A}(\omega)\sin\left(\frac{\Phi (\omega)- \widehat{\Phi} (\omega)}{2}\right)^2 \,
h(\omega) \, d\omega\label{eq:chapats_atsR}
\end{eqnarray}
Accuracy measures the contribution to the MSE when the phase (time-shift) and the noise suppression
 in the stop-band are ignored. This would correspond to the performance of a symmetric filter
  (no time-shift) with perfect noise suppression, i.e., $\widehat{A}(\cdot)=0$ in
the stop-band, and with the same amplitude as the considered real-time filter in the pass-band. A corresponding
symmetric filter could be easily constructed by inverse Fourier transformation.

Smoothness measures the MSE contribution attributable to the leakage of the real-time
filter in the stop-band, corresponding to undesirable high-frequency noise. This quantity is linked to
the time-domain ``curvature" measure consisting of the variance of second differences,
 because 
\[
 \EE {[ {(1-B)}^2 (\widehat{y}_t - y_t ) ]}^2  = \frac{1}{2 \pi} \int_{-\pi}^{\pi} {(2 - 2 \cos \omega)}^2  \, 
  |\Gamma(\omega)-\widehat{\Gamma}(\omega)|^2h(\omega)d\omega,
\]
 and the function $ {(2 - 2 \cos \omega)}^2 $ plays the role of a stop-band.  If $A$ is compactly supported in the 
pass-band, then the above is equal to the integral of $ {(2 - 2 \cos \omega)}^2 \widehat{A}^2 (\omega)$, which
resembles the Smoothness term.  Timeliness measures the MSE contribution generated by the time-shift. It is linked
to the time-domain ``peak-correlation" concept, which measure the correlation between $y_t$ and $\widehat{y}_{t+j}$ for 
various lags $j$.   

Finally, the Residual is
that part of the MSE which is not attributable to any of the above error components. Since the
product $A(\omega)\widehat{A}(\omega)$ is small in the stop-band (possibly vanishing, as in the
 case of the ideal low-pass) the Residual is generally negligible. From a slightly different
  perspective, user priorities are rarely concerned about time-shift properties of components
  in the stop-band,  which ought to be damped or eliminated anyways. For the sake of simplicity we henceforth
  focus on the ideal low-pass target, wherein the Residual vanishes completely.

The MSE can be easily generalized to provide a customized measure
 by assigning weights to the terms of its ATS decomposition:
\begin{equation}\label{eq:chapats_ats_cust}
 \mathcal{M} (\lambda_1, \lambda_2) = \lambda_1 \, \textrm{Timeliness}+ \lambda_2 \,
 \textrm{Smoothness} + ( 1 - \lambda_1 - \lambda_2) \, \textrm{Accuracy}.
\end{equation}
The parameters $\lambda_1, \lambda_2 \in [0,1]$ allow one to assign priorities to single or
pairwise combinations of MSE error terms: the underlying three-dimensional
tradeoff is called the {\it ATS-trilemma} and the
optimization paradigm (\ref{eq:chapats_ats_cust}) is called a {\it customized criterion}.
 The user is free to navigate on the
customization triangle according to specific research priorities: the ordinary MSE is
obtained by setting $\lambda_1=\lambda_2=1/3$, whereas complete emphasis on Smoothness arises from the
 choice $\lambda_1 = 0, \lambda_2 =1$, etc.


The above trilemma was obtained by assuming that the target filter $\Gamma(\cdot)$ discriminates components into
pass- and stop-bands. In contrast, $H$-step ahead forecasting is concerned with the approximation of an
anticipative allpass filter. Since the stop-band is non-existant, the Smoothness and Residual
error components are irrelevant and
the above trilemma collapses into a forecasting dilemma:
\begin{equation}
 \label{eq:dilemma}
 \mathcal{M} (\lambda_1) = \lambda_1 \, \textrm{Timeliness} + (1 - \lambda_1) \, \textrm{Accuracy}
\end{equation}
 While this paradigm is sufficient to address all-pass filtering problems, such as multi-step ahead forecasting,
 it cannot emphasize Smoothness, and hence is less flexible than (\ref{eq:chapats_ats_cust}).

The proposed method can replicate and customize traditional (ARIMA or State-Space)
model-based approaches as well as classic filter designs (HP, CF, Henderson), by plugging the corresponding
spectral densities as well as the target signals into equations (\ref{eq:chapats_Ats}), (\ref{eq:chapats_aTs}), (\ref{eq:chapats_atS}), 
and (\ref{eq:chapats_atsR}), followed by minimization of (\ref{eq:chapats_ats_cust}) subject to $\lambda_1 = \lambda_2 = 1/3$, 
including the Residual term to achieve perfect replication.  

We could combine these operations in any order.  As an example, we could target a HP filter by supplying
a model-based spectral density. Moreover, we could use alternative spectral
estimates (for example, nonparametric) or targets, allowing for a flexible
implementation of general real-time problems.  When the target is non-model-based, but our estimate of 
$h$ is model-based, we call the resulting method a {\it hybrid} DFA.  Examples of hybrid trend extraction 
and sesaonal adjustment are given in Wildi and McElroy (2014).  Observe that the customization interface provided by (\ref{eq:chapats_ats_cust}) allows
 one to emphasize particular filter characteristics, so as to align with research priorities.

\section{Customization Examples}

Here we study several examples originally introduced in the basic DFA treatment.   In particular, we consider HP Low-Pass,  Na\"ive Seasonal Adjustment,  $H$-step ahead forecasting (which will utilize the dilemma (\ref{eq:dilemma})), and the Ideal Low-Pass.  These examples can then be applied to data with the accompanying R code.

\subsection{HP Low-Pass Filtering}
 The HP filter was defined earlier as having frf
\[
 \Gamma (z) = \frac{ r}{ r + {(1-z)}^2 {(1- \overline{z})}^2 },
\]
 where $z = e^{-i \lambda}$ and $r > 0$ is the signal-to-noise ratio.  This is our target filter, so that $y_t = \Gamma (B) x_t$ is our target.  Explicit formulas for the coefficients are given in McElroy (2008, EJ); there are infinitely many non-zero coefficients, and the filter is symmetric.   We seek a concurrent filter $\widehat{\Gamma} (B)$, and will consider the class $\mathcal{M}_q$ of moving average filters of order $q$.   Moreover, we wish to impose a level constraint so that the forecast filter can be applied to interesting $I(1)$ time series.  Recall that the level constraint states that $\Gamma (1) = \widehat{\Gamma} (1)$. 
 Now it's easy to see that $\Gamma(1) = 1$, but in the following derivations we write $\Gamma(1) $ instead, so that the calculations are valid for filters that don't satisfy $\Gamma (1) = 1$.    Because $\widehat{\Gamma} \in \mathcal{M}_q$, it can be written
\[
   \widehat{\Gamma}  (B) = \sum_{j=0}^q \theta_j B^j,
\]
 with no {\it a priori} restrictions on the coefficients $\theta_j$.  Imposing the constraint and solving for $\theta_0$ yields
\[
 \theta_0 = \Gamma (1) - \sum_{j=1}^q \theta_j.
\]
 The vector of unconstrained parameters is denoted $\theta = {[ \theta_1, \theta_2, \cdots, \theta_q ]}^{\prime}$.
 Substituting into $\widehat{\Gamma} (B)$, we can algebraically manipulate the concurrent filter as follows:
\begin{align*}
 \widehat{\Gamma} (B) & = \Gamma(1) + \theta_1 (B-1) + \theta_2 (B^2 - 1) + \cdots + \theta_q (B^q - 1) \\
 	& = \Gamma(1) +  \sum_{j=1}^q \theta_j \, (B^j - 1),
\end{align*}
  It follows that  the filter error is
\[
 \Gamma (B) -   \widehat{\Gamma} (B)   = \Gamma (B) - \Gamma(1) -  \sum_{j=1}^q \theta_j \, (B^j - 1).
 \]
 Recall the notation $\langle \cdot \rangle$ for the average integral over $[-\pi, \pi]$, and for short let $b_j (z) = z^j - 1$, written as a vector of functions $b(z)$.  Then the filter error in frequency domain becomes $\Gamma (z) - \Gamma (1 ) - \theta^{\prime} b (z)$.  Also let $g(z) = \Gamma (z) - \Gamma(1)$.
 Substituting into the DFA criterion (\ref{eq:chapats_dfa_fd}) yields
\begin{equation}
 \label{eq:HPdfaMSE}
  \langle g( e^{-i \cdot} ) g(e^{i \cdot}) h \rangle -  \theta^{\prime} \, \langle \left( b (e^{-i \cdot}) g(e^{i \cdot}) +  b (e^{i \cdot}) g(e^{-i \cdot}) \right) h \rangle + \theta^{\prime} \, \langle b( e^{-i \cdot}) b^{\prime} (e^{i \cdot}) h \rangle \, \theta,
\end{equation}
 which is a quadratic function of the parameter $\theta$.  The theoretical solution is obtained when $h$ is the true data spectrum, while an estimate $\widehat{\theta}$ is obtained when $h$ is the periodogram, or some other estimator of the spectral density.  In either case, the minimizer is
\[ 
 \theta = { \left[  \langle b( e^{-i \cdot}) b^{\prime} (e^{i \cdot}) h \rangle \right] }^{-1} \,  \langle \left( b (e^{-i \cdot}) g(e^{i \cdot}) +  b (e^{i \cdot}) g(e^{-i \cdot}) \right) h \rangle.
\]
 However, when computing the customized optimum $\theta$ the criterion becomes $\mathcal{M} (\lambda_1, \lambda_2)$ of (\ref{eq:chapats_ats_cust}),
 which in general is no longer a quadratic function.   For the HP filter there is no phase, so that $A (\omega) = \Gamma (e^{-i \omega})$.  On the other hand, the squared amplitude for the concurrent filter can be expressed
\begin{equation}
 \label{eq:HPsquaredamp}
  \widehat{A}^2 (\omega) = 
\Gamma(1)^2 +  \Gamma (1) \, \theta^{\prime} \, \left( b (e^{-i \cdot}) +  b (e^{i \cdot})  \right)  +  \theta^{\prime} \,  b( e^{-i \cdot}) b^{\prime} (e^{i \cdot})  \, \theta.
\end{equation}
 In the Accuracy and Smoothness terms, wee compare the square root of this function to the HP frf.   As for phase, we have $\Phi (\omega) = 0$ and the concurrent phase is obtained most easily by computing the arctangent of the ratio of imaginary and real portions of the given frf.  Noting that $\Re b_j (z) = 
  \cos (\omega j ) - 1$  and $\Im b_j (z) = - \sin (\omega j) - 1$, we can write
\begin{equation}
 \label{eq:HPphase}
 \widehat{\Phi} (\omega) = \tan^{-1} \left( \frac{ \Gamma(1) + \theta^{\prime} \, \Im b (z)  }{ \Gamma(1) + \theta^{\prime} \, \Re b (z) } \right).
\end{equation}
  Finally, the cut-off for the pass-band is computed by solving
\[
 \frac{1}{2} =  \frac{ r}{ r + {( 2 - 2 \cos \omega)}^2 }
\]
 for $\omega$, which yields $\omega = \cos^{-1} (1 - \sqrt{r}/ 2)$.  At this point, Accuracy, Timeliness, and Smoothness can be calculated numerically.

TO DO: write R code for the functions, and use integrate with inputted h.

For a very basic illustration, we suppose the filter is applied to white noise, so that $h$ is a constant (and can be essentially ignored).  Then, having fixed some values of $r$ ahead of time, we compute the optimal $\theta$ under various values of $\lambda_1$ and $\lambda_2$, illustrating the dependence on differing emphases in the trilemma criterion.

INSERT: table of values

REPEAT: with $h$ corresponding to an AR(1) process with known $\phi$



\subsection{Na\"ive Seasonal Adjustment}
 Earlier we introduced a fairly primitive seasonal adjustment filter, defined as
\[
 \Gamma (B) = s^{-2} U(B) U(F),
\]
 where $s$ is the number of seasons in the year (e.g., $s=4$ for
 quarterly data and $s=12$ for monthly data) and $U(B) = 1 + B + B^2
 + \cdots + B^{s-1}$.   Typically this would be applied to data that exhibits seasonality, and in particular corresponds to 
a non-stationary seasonal process requiring so-called ``differencing" by $U(B)$ to be rendered stationary.  Our full
treatment of non-stationary processes is left until later, and we focus on application to stationary series for now.  As in the previous illustration, 
we are interested in concurrent moving average filters.  However, the filter constraints are more subtle when dealing with seasonality.

Observe that the roots of $U(z)$ are $e^{-i 2 \pi j /s }$ for $1 \leq j \leq s-1$, which indicates that  the frf $\Gamma (z)$ is zero at the ``seasonal frequencies" $2 \pi j /s$ for $1 \leq j leq s-1$.  Because the frf is real and even, it suffices to focus on the seasonal frequencies for $1 \leq j \leq s/2$ (when $s$ is even).  These zeroes in the target frf are the essential property of a seasonal adjustment filter, which should be a ``comb" filter (i.e., a filter having frf equal to zero at frequencies that are to be suppressed).  In order for the concurrent filter to be a causal comb filter, it should also have this zero structure, which we can impose by seeking it to be of the form
\begin{equation}
 \label{eq:naiveSAfilt}
 \widehat{\Gamma} (B) = U(B) \, \Theta (B),
\end{equation}
 for some $\Theta (B) \in \mathcal{M}_q$ (for example).  Up to reparametrization, imposing the structure of (\ref{eq:naiveSAfilt}) on the class of concurrent filter  is the same as considering the class $\mathcal{M}_{q+s-1}$ with the imposition of $\widehat{\Gamma} (z) = \Gamma (z)$ for all seasonal frequencies $z$.  Proceeding, the corresponding filter error is
\[
 \Gamma (B) -  \widehat{\Gamma} (B)  = U(B) \, \left( s^{-2} U(F) - \Theta (B) \right),
\]
 so that the DFA problem amounts to approximating an anti-causal filter $U(F)$ by a causal one $\Theta (B)$.  
The resulting DFA MSE is similar to the HP case (\ref{eq:HPdfaMSE}).  Let $\theta = {[ \theta_0, \theta_1, \cdots, \theta_q ]}^{\prime}$, and denote the functions $b_j (z) = z^j $ for $0 \leq j \leq q$, written as a $(q+1)$-vector of functions $b(z)$.  Then the filter error in frequency domain is 
$\Gamma (z) - U(z) \theta^{\prime} b(z)$, and the DFA MSE is
\begin{equation}
 \label{eq:SeasdfaMSE}
  \langle \Gamma ( e^{-i \cdot} ) h \rangle -  \theta^{\prime} \, \langle \left( b (e^{-i \cdot}) U(e^{-i \cdot}) \Gamma(e^{i \cdot}) 
	+  b (e^{i \cdot}) U(e^{i \cdot}) \Gamma (e^{-i \cdot}) \right) h \rangle + \theta^{\prime} \, \langle b( e^{-i \cdot}) b^{\prime} (e^{i \cdot})
   \Gamma (e^{-i \cdot})  h \rangle \, \theta.
\end{equation}
 As in the HP case, this is a quadratic function of the parameter $\theta$.  The theoretical solution is obtained when $h$ is the true data spectrum, while an estimate $\widehat{\theta}$ is obtained when $h$ is the periodogram, or some other estimator of the spectral density.  In either case, the minimizer is
\[ 
 \theta = { \left[  \langle b( e^{-i \cdot}) b^{\prime} (e^{i \cdot}) \Gamma (e^{-i \cdot}) h \rangle \right] }^{-1} \,  \langle \left( b (e^{-i \cdot}) U(e^{-i \cdot}) \Gamma(e^{i \cdot}) 
	+  b (e^{i \cdot}) U(e^{i \cdot}) \Gamma (e^{-i \cdot}) \right) h \rangle.
\]
 However, when computing the customized optimum $\theta$ the criterion becomes $\mathcal{M} (\lambda_1, \lambda_2)$ of (\ref{eq:chapats_ats_cust}),
 which in general is no longer a quadratic function.   For the Na\"ive Seasonal Adjustment filter there is no phase, so that $A (\omega) = \Gamma (e^{-i \omega})$.  On the other hand, the concurrent filter has squared amplitude 
\[
  \widehat{A} (\omega) = \Gamma (\omega) \, {| \Theta (e^{-i \omega}) |}^2 = \Gamma (\omega) \, \theta^{\prime} \, b(z) \, b^{\prime} (\overline{z}) \, \theta.
\]
  The phase function can be written
\[
 \widehat{\Phi} (\omega) = \tan^{-1} \left( \frac{ \theta^{\prime} \, \Im [U(z) b(z) ] }{ \theta^{\prime} \, \Re [ U(z) b(z) ] } \right).
\]
 Finally, to find the pass-band we observe that the complementary stop-band is the union of several disjoint intervals located about the seasonal frequencies, their boundaries being determined as the multiple solutions $\omega$ to
\[
  s^2/2 = U(e^{-i \omega}) U(e^{i \omega}).
\]
 Now we can compute the ATS components numerically, given a spectrum $h$.

TO DO: extend HP code to this case



\subsection{Multi-step Ahead Forecasting}
 Here our target is $y_t = x_{t+H} = F^H x_t$, and $\Gamma (B) = B^{-H}$.  We seek a concurrent filter $\widehat{\Gamma} (B)$, and will consider the class  $\mathcal{M}_q$ of moving average filters of order $q$.  As with the HP example, we wish to impose a level constraint so that the forecast filter can be applied to interesting $I(1)$ time series.   We can take the HP calculations a few steps further: we can show that the filter error is divisible by $1-B$, which is important when treating $I(1)$ series later in this book. The concurrent filter can be expressed as
\begin{align*}
 \widehat{\Gamma} (B) &  = 1 + (B-1) \, \sum_{j=1}^q \theta_j \, ( \sum_{\ell=0}^{j-1} B^{\ell} ) \\
	& = 1 + (B-1) \, \sum_{j=0}^{q-1}  ( \Theta_q - \Theta_{j} ) B^j ,
\end{align*}
 where $\Theta_j = \sum_{\ell=1}^j \theta_{\ell}$, and $\Theta_0 = 0$.   Then the filter error divided by $1-B$ is
\begin{align*}
  \frac{ B^{-H} -  \widehat{\Psi} (B)  }{ 1 - B} & = B^{-H} \, \frac{ 1 - B^{H} + (1-B)  \, \sum_{j=0}^{q-1}  ( \Theta_q - \Theta_{j} ) B^{j+H} }{1-B} \\
   & = B^{-H} \, \left( \sum_{j=0}^{H-1} B^j +  \sum_{j=0}^{q-1}  ( \Theta_q - \Theta_{j} ) B^{j+H} \right) .
\end{align*}
 Now when forecasting $I(1)$ processes, the DFA criterion  (\ref{eq:chapats_dfa_fd})  gets modified by division of the integrand by ${|1 - z |}^2$, in which case the DFA MSE is 
\[
   [1, 1, \cdots, 1, \Theta_q - \Theta_0, \Theta_q - \Theta_1, \cdots, \theta_q ] \, \Sigma (h) \,  { [1, 1, \cdots, 1, \Theta_q - \Theta_0, \Theta_q - \Theta_1, \cdots, \theta_q ] }^{\prime},
\]
 where $\Sigma(h)$ is the Toeplitz covariance matrix corresponding to $h$, with dimension $H+q$.   In the stationary case there is no such factor in the denomminator needed, and the above formula applies but within ${|1 - z|}^2$ multiplying $h$.   Writing $\iota$ for the vector of $H$ ones, and $C$ the cumulation matrix given by ones in the upper triangular portion, the criterion can be rewritten
\[
 \iota^{\prime} \Sigma_{11} (f) \iota + 2 \iota^{\prime} \Sigma_{21} (f) \, C \theta + \theta^{\prime} C^{\prime} \Sigma_{22} (f)  \, C \theta,
\]
 where $f (\omega ) = h(\omega) (2 -2 \cos(\omega))$, and $\Sigma (f) $ has been partitioned into blocks $\Sigma_{11} (f)$, $\Sigma_{21} (f)$, $\Sigma_{12} (f)$, and $\Sigma_{22} (f)$.   Optimizing this quadratic functional of $\theta$ yields the minimizer
\[
 \theta (f) = - C^{-1} \, {\Sigma_{22} (f)}^{-1} \, \Sigma_{21} (f) \, \iota.
\]
Return now to the customized criterion.  Because the target filter's amplitude function is identically equal to one, the notions of pass-band and stop-band become trivial: the pass-band is $[- \pi, \pi]$ and the stop-band is the null set.  Thus we only need to compute the Accuracy and Timeliness factors.  We then have the simplified criterion (\ref{eq:dilemma}), where $A \equiv 1$ and $\Phi \equiv 0$.  For pedagogical value only, we repeat  equations (\ref{eq:chapats_Ats}) and (\ref{eq:chapats_aTs}) for this special case:
\begin{eqnarray}
\textrm{A(ccuracy)}&:=&\int_{\textrm{pass-band}}(1-\widehat{A}(\omega))^2 \, h(\omega) \, d\omega\label{eq:chapats_Ats2}\\
\textrm{T(imeliness)}&:=&4\int_{\textrm{pass-band}}  \widehat{A}(\omega)\sin\left( \widehat{\Phi} (\omega) /2\right)^2
 \, h(\omega) \, d\omega.  \label{eq:chapats_aTs2}
\end{eqnarray}
 Of course the pass-band is $[-\pi,\pi]$, because the target filter is an all-pass; the above formulas will also be applicable when the target filter is ``locally all-pass", i.e., has unit magnitude on a pass-band smaller than $[-\pi, \pi]$.  

The squared amplitude of the concurrent filter is given by equation (\ref{eq:HPsquaredamp}) of the HP case, setting $\Gamma(1) = 1$, and phase given by (\ref{eq:HPphase}).  Then the Accuracy and Timeliness components can be computed numerically.  




TO DO: extend HP code to this case

\subsection{Ideal Low-Pass}
 Finally we consider a target  defined as the output of the Ideal Low-Pass filter, which has frf
\[
 \Gamma (e^{-i \omega}) = 1_{[-\mu, \mu]} (\omega)
\]
 for a cutoff $\mu \in (0, \pi)$.  The pass-band is identical with $[-\mu, \mu]$, while the stop-band is the complementary set.  Typically $\mu$ is a small positive value, and the filter is used to obtain trend dynamics of some desired maximum frequency $\mu$.  The concurrent filter is usually chosen to match the level constraint of $\Gamma (1) = 1$, and if using the class of moving average filters, the mathematics are the same as the HP case.  The target filter has amplitude funcion $\Gamma$ with $\Phi \equiv 0$, and the concurrent filter has squared amplitude and phase functions given by (\ref{eq:HPsquaredamp}) and (\ref{eq:HPphase}) respectively.  The Accuracy and Timeliness terms are computed via (\ref{eq:chapats_Ats2}) and (\ref{eq:chapats_aTs2}); the Smoothness term is as (\ref{eq:chapats_Ats2}), but integrated over the stop-band instead.

TO DO: numerical stuff



\section{Customization Applications}

In this final section, we consider several time series, and apply trend estimation and forecasting filters.

NOTES: use ATS paper example as guide.  Consider ideal low-pass and multi-step forecasting, and begin with simulated AR(1).  Fit AR(1) and use for $h$; also use perodogram.  Tabulate results.  Then repeat with real data, using h given by periodogram and AR(p) estimator.
