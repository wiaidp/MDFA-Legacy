
\chapter{Mean-Square Error, Zero-Crossings, and Sign Accuracy}\label{SSA_criterion}





<<echo=False>>=
estim<-F
@


%\tableofcontents

\section{Introduction}




Time series forecasting aims at a coherent analysis of the main systematic dynamics of a phenomenon in view of synthesizing information about future events. Typically, the forecast process is structured by a formal optimality concept, whereby a particular forecast-error measure, such as e.g. the mean-square error (MSE), is minimized. We here argue that multiple and various characteristics of a predictor might draw attention such as the smoothing capability, i.e. the extent by which undesirable 'noisy' components of a time series are suppressed, or timeliness, as measured by relative lead or lag properties of a predictor, or sign accuracy and zero-crossings, as measured by the ability to predict the correct sign of a target. For that purpose, we here propose a generic forecast approach, referred to as simple sign-accuracy (SSA), by merging sign-accuracy and MSE performances subject to a holding-time constraint which  determines the expected number of zero-crossings of the predictor in a time interval of given length. Zero-crossings (of the growth-rate) of a time series are influential in the decision-making, for e.g. economic actors, by marking transitions between up- and down-turns, expansions and recessions, bull and bear markets, and our forecast approach contributes to such a design of the predictor. % in terms of an interpretable hyper-parameter. % While a comprehensive and formal treatment of timeliness must be deferred, corresponding issues will be considered indirectly, via an additional and interpretable tuning- or hyper-parameter, and performances in terms of leads or lags will be measured accordingly. %Some of our examples illustrate that classic predictors can be outperformed both in terms of smoothness and timeliness at once.       
%We here combine mean-square error (MSE) performances, sign accuracy, zero-crossings and smoothness characteristics in a common formal framework under suitable assumptions about the data-generating process. % and defer a lengthier theoretical treatment of timeliness which will be considered from a purely descriptive perspective, only. 
McElroy and Wildi (2019) propose an alternative methodological framework for addressing specific facets of the forecast problem but their approach does not account for zero-crossings explicitly which may be viewed as a shortcoming in some applications. Wildi (2023) proposes an application of SSA to a (real-time) business-cycle analysis, but the chosen treatment remains  largely informal. We here fill this gap by providing a complete formal treatment, including regular, singular and boundary cases, a discussion of numerical aspects as well as a derivation of the sample distribution of the predictor together with a comprehensive illustration of technical  features and peculiarities of the approach. \\

    
The analysis of zero-crossings has been pioneered by Rice (1944). Kedem (1986) and Barnett (1996) extend the concept to exploratory and inferential statistics and a theoretical overview is provided by Kratz (2006). Application fields are various in electronics and image processing, process discrimination, pattern detection in speech, music, or radar screening. However, in contrast to the analysis of current or past events, we here emphasize foremost a prospective prediction perspective. \\



The optimization criterion is derived in section \ref{zc} with a discussion of robustness and extensions of the basic methodological framework (BMF); solutions of the criterion are proposed in section \ref{theorem_SSA} with a discussion of boundary and singular cases, numerical aspects as well as the sample distribution of the estimate; section \ref{examples} illustrates applications such as ordinary forecasting, elements of signal extraction and filtering, resilience against departures from the BMF, smoothing and 'un-smoothing', a smoothness-timeliness dilemma, multiplicity and uniqueness features as well as a fully fleshed-out singular case. All empirical examples are reproducible in an open source R-package (include link to Github). Finally, section \ref{conclusion} concludes by summarizing our main findings. 









\section{Simple Sign-Accuracy (SSA-) Criterion} \label{zc}



We propose a simple BMF for presentation of our main results. Specifically, let $\epsilon_t, t \in \mathbb{Z}$, be Gaussian standard white noise\footnote{Since zero-crossings of zero-mean stationary processes are insensitive to the scaling, our approach is insensitive to $\sigma^2$: for simplicity, we will assume $\sigma^2=1$ if not stated otherwise.} and let $\gamma_k\in \mathbb{R}$ for $k \in \mathbb{Z}$ be a square summable sequence $\sum_{k=-\infty}^{\infty}\gamma_k^2<\infty$. Then $z_t=\sum_{k=-\infty}^{\infty}\gamma_k \epsilon_{t-k}$ is a stationary Gaussian zero-mean process with variance $\sum_{k=-\infty}^{\infty}\gamma_k^2$. We  consider estimation of $z_{t+\delta}$, $\delta \in \mathbb{Z}$, referred to as the \emph{target}, based on the predictor $y_t:=\sum_{k=0}^{L-1}b_{k}\epsilon_{t-k}$, where $\mathbf{b}:=(b_k)_{0\leq k\leq L-1}$ are the coefficients of a finite-length one-sided causal filter.  This problem is commonly referred to as fore-, now- or backcast, depending on $\delta>0$, $\delta=0$ or $\delta<0$. Departures from the Gaussian assumption will be discussed in sections \ref{mse_sa_zc} and \ref{resil} and an extension to autocorrelated $x_t$, such that $z_t=\sum_{k=-\infty}^{\infty}\gamma_k x_{t-k}$, is proposed in section \ref{ext_stat} with applications in section \ref{examples}. Our notation of the prediction problem in this case addresses more specifically signal extraction, whereby a particular acausal filter $\boldsymbol{\gamma}=(\gamma_k)_{|k|<\infty}$ is applied to a generally autocorrelated time series $x_t$ in order to extract pertinent components such as e.g. trends, cycles or seasonal components. Therefore, we can merge conceptually prediction and signal extraction  and we will often refer to predictors $y_t$ in terms of filters $\mathbf{b}$. %In particular, a predictor can be interpreted in terms of a filter which typically, but not always, removes or damps undesirable 'noisy' high-frequency components (lowpass design), see section \ref{examples} for illustration. 
In any case, the BFM chiefly intends to clarify exposition and to simplify notation in view of highlighting the relevant facets of the prediction problem in a decluttered formal context. 


\subsection{Sign-Accuracy, MSE and Holding-Time}\label{mse_sa_zc}

We  look for an estimate $y_t$ of $z_{t+\delta}$ such that the probability P$\Big(\sign(z_{t+\delta})=\sign(y_t)\Big)$ is maximized as a function of $\mathbf{b}$ and we refer to this criterion in terms of \emph{sign accuracy} (SA).

\begin{Proposition}
Under the BMF the sign accuracy  criterion can be stated as
\begin{eqnarray}\label{opt_crit}
\max_{\mathbf{b}}\rho(y,z,\delta)
\end{eqnarray}
where 
\[
\rho(y,z,\delta)=\frac{\sum_{k=0}^{L-1}\gamma_{k+\delta}b_{k}}{\sqrt{\sum_{k=-\infty}^\infty \gamma_k^2}\sqrt{\sum_{k=0}^{L-1}b_k^2}}
\] 
is the correlation between $y_t$ and $z_{t+\delta}$. 
\end{Proposition}
In the stipulated case of Gaussian random variables a proof follows readily from the identity $P\Big(\sign(z_{t+\delta})=\sign(y_t)\Big)=0.5+\frac{\arcsin(\rho(y,z,\delta))}{\pi}$, relying on strict monotonicity of the non-linear transformation. %Discarding the affine transformation, expression \ref{opt_crit} by monotonicity of $\arcsin()$. %Note that signs, zero-crossings or correlations are insensitive to the scalings of $y_t$ or $z_t$. 
%The MSE-estimate  $\mathbf{b}=\boldsymbol{\gamma}_{\delta}:=(\gamma_{\delta},...,\gamma_{\delta+L-1})'$ is a solution of \ref{opt_crit} 
We then infer that SA and MSE are equivalent criteria, at least down to an arbitrary scaling of $y_t$ and conditional on the Gaussian assumption.\\

\textbf{Remarks}\\
We here discard the scaling parameter from further consideration since our approach emphasizes signs, smoothness and timeliness aspects as alternative priorities. In this perspective, predictors that differ by an arbitrary (positive) normalization constant are felt equivalent. Note also that classification methods such as e.g. logit models are less suitable for the purpose at hand because fitting the signs $\textrm{sign}(z_{t+\delta})=\pm 1$, instead of the actual observations $z_{t+\delta}$, would result in a loss of efficiency under the premises of the  BMF.\\

<<label=init,results=hide>>=
# Brief empirical check of MSE estimate (intended for a later student-exercise...)
setseed<-1
len<-1000000
eps<-rnorm(len)
L_t<-5
gammak<-rep(1,L_t)
targeth<-eps
for (i in length(gammak):len)
  targeth[i]<-gammak%*%eps[i:(i-length(gammak)+1)]

explanatory<-NULL
# For any L the above MSE-estimate is obtained
L<-1
delta<-1
for (i in 1:L)
  explanatory<-cbind(explanatory,eps[(L+1-i):(len+1-i-delta)])

target<-targeth[(len-nrow(explanatory)+1):len]

summary(lm(target~explanatory-1))
@
% (zero-crossings or correlations are insensitive to arbitrary scalings.\\ %However, we maintain the above formulation which will prove insightful when generalizing the optimization concept.  \\
Consider now the expected duration between consecutive zero-crossings or sign-changes of the predictor $y_t$, which will be referred to as \emph{holding-time}.

\begin{Proposition}\label{ht_formula}
Under the BMF the holding-time $ht(y|\mathbf{b})$ of $y_t$ is 
\begin{eqnarray}\label{ht}
ht(y|\mathbf{b})=\frac{\pi}{\arccos(\rho(y,y,1))}
\end{eqnarray}
where $\rho(y,y,1)=\frac{\sum_{i=1}^{L-1}b_ib_{i-1}}{\sum_{i=0}^{L-1}b_i^2}$ is the lag-one autocorrelation of $y_t$. 
\end{Proposition}

A proof is provided by Kedem (1986). We can now formalize the concept of 'smoothness' of a predictor $y_t$ by constraining $\mathbf{b}$ such that
\begin{equation}\label{ht_const}
ht(y|\mathbf{b})= ht_1
\end{equation}
or, equivalently,
\begin{equation}\label{ht_const_z}
\rho(y,y,1)= \rho_1
\end{equation}
where $ht_1$ or $\rho_1$, linked through \ref{ht}, are proper hyper-parameters of our design. In the following, we  refer to the 'holding-time' either in terms of $ht(y|\mathbf{b})$ or $\rho(y,y,1)$, clarifying our intent in case of ambiguity. We here argue that the hyper-parameter $ht_1$ is interpretable and can be set a priori, at the onset of an analysis, according to structural elements of a prediction problem. As an example, Wildi (2023) illustrates the proceeding in a business-cycle application, where $ht_1$ matches the length of historical recession episodes. Also, the holding-time could be selected in view of taming the number of unsystematic or noisy crossings (false alarms). Furthermore, if costly strategy-adjustments or behavioral changes take place at zero-crossings, for example in an algorithmic trading framework or for macro-economic policy setting, then cumulated costs of all adjustments, being inversely proportional to $ht_1$, could be  accounted for by a suitable determination of the hyperparameter. Finally, $ht_1$ could be set according to short-, mid- or long-term i.e tactic, strategic or fundamental outlook perspectives. Concerning the proper selection of the holding-time, the following proposition sets limits for admissible constraints in the basic framework.

\begin{Proposition}\label{maxrho}
Under the BMF, maximal and minimal lag-one autocorrelations $\rho_{max}(L),\rho_{min}(L)$ of $y_t$ are $ \rho_{max}(L)=-\rho_{min}(L)=\cos(\pi/(L+1))$. The corresponding MA-coefficients $b_{max,k}:=\sin\left(\displaystyle{\frac{(1+k)\pi}{L+1}}\right)$, $k=0,...,L-1$, and $b_{min,k}:=(-1)^kb_{max,k}$ are uniquely determined down to arbitrary scaling and sign.  
\end{Proposition}

We refer to  N. Davies, M. B. Pate and M. G. Frost (1974) for a proof, see also proposition \ref{stationary_eigenvec} further down. 
%is a hyper-parameter that controls for the \emph{smoothing}-capability of the filter $\mathbf{b}$. %y_t$ by imposing a mean-length between consecutive zero-crossings. 
Consider now the sign accuracy criterion  \ref{opt_crit} endowed with the holding-time constraint \ref{ht_const_z}:
\begin{eqnarray}\label{crit1}
\left.\begin{array}{cc}
&\max_{\mathbf{b}}\displaystyle{\frac{\sum_{k=0}^{L-1}\gamma_{k+\delta}b_{k}}{\sqrt{\sum_{k=-\infty}^\infty \gamma_k^2}\sqrt{\sum_{k=0}^{L-1}b_k^2}}}\\
&\displaystyle{\frac{\sum_{k=1}^{L-1}b_{k-1}b_{k}}{\sum_{k=0}^{L-1}b_k^2}=\rho_1}
\end{array}\right\}
\end{eqnarray}
This optimization problem is called \emph{simple sign-accuracy} or SSA-criterion: simplicity here refers to the elementary structure of the predictor, as derived in theorem \ref{lambda}, as well as to the scope of the criterion which does not yet allow for a formal treatment of timeliness or lead/lag issues, see section \ref{time_smooth} for an informal treatment and Wildi (2023) for additional illustration. We allude  to solutions of this criterion by the acronym SSA or SSA($ht_1,\delta$) or SSA($\rho_1,\delta$) to stress the dependence of the predictor on the pair of hyper-parameters, see section \ref{time_smooth} for reference. The SSA-criterion merges MSE, sign accuracy and smoothing requirements in a flexible and consistent way. Departures from the Gaussian assumption can be accommodated in the sense that $y_t$ or $z_t$ can be 'nearly Gaussian' even if $x_t=\epsilon_t$ is not, due to the central limit theorem, see Wildi (2023) for an application to financial data (equity index) and section \ref{resil}. Finally, the  criterion remains appealing outside of a strict holding-time or zero-crossing perspective by complementing the classic predictor with a generic smoothing constraint.        
<<label=init,results=hide>>=
# Purposes
# 0. Use Gauss or student-t (if skewed then one has to shift by mean)
# 1. Check that MSE/correlation has same sign-accuracy as logit, in-sample
# 2. Non-zero crossings can be addressed by simple shift (mu!=0 in code below)
# 3. MSE estimate has much smaller estimation variance (efficiency): should perform better out-of-sample!
len<-10000
L<-10
gamma<-rep(1/L,L)
Gauss_or_t<-F
set.seed(23)
if (Gauss_or_t)
{  
# Gauss
  x<-rnorm(len)
} else
{  
# Student-t
  df<-10
# Keep symmetric design: otherwise target z will be biased (easier to predict)
  skew<-0
  x<-rt(len, df,skew)
}
# Target: 
# Non-zero crossings are obtained by selecting mu!=0
mu<-0.5
x<-x
z<-x
for (i in L:len)
  z[i]<-gamma%*%x[i:(i-L+1)]+mu

ts.plot(cbind(x,z),col=c("black","red"))

delta<-min(5,L-1)

y<-x
for (i in L:len)
  y[i]<-gamma[1:(L-delta)]%*%x[i:(i-L+delta+1)]+mu

# Sign accuracy MSE
length(which(sign(y[1:(len-delta)])==sign(z[(1+delta):len])))/len

ts.plot(cbind(y,z),col=c("blue","red"))

#------------------------------
# Logit
target<-(1+sign(z)[(1+2*delta-1):len])/2
length(target)
explanatory<-x[delta:(len-delta)]
if (delta>1)
{
  for (i in 2:delta)
  {
    explanatory<-cbind(explanatory,x[(delta-i+1):(len-delta-i+1)])
  }
}
dim(explanatory)
# data set
sample<-data.frame(cbind(target,explanatory))


model <- glm(target ~.,family=binomial(link='logit'),data=sample)

summary(model)

# Advantage MSE over logistic model: variance of estimates is much smaller!!!!
summary(lm(z[(1+len-nrow(explanatory)):len]~explanatory))


fitted.results <- predict(model,newdata=subset(sample,select=1+1:delta),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != target)
# Same performance
print(paste('Accuracy',1-misClasificError))
ht_ex<-round((acos(2/3)/pi)^{-1},3)
@
%, and the criterion aims at matching 'directly' signs of forecast and of target. In contrast, the sign inference for a logit-model is obtained 'indirectly', via the determination of an additional discrete  decision rule determined typically by the logit-output being above or below a $50\%$ score.}. 


\subsection{Extension to Stationary Processes}\label{ext_stat}

Let 
\begin{eqnarray*}
x_t&=&\sum_{i=0}^{\infty}\xi_i\epsilon_{t-i}\\
z_t&=&\sum_{|k|<\infty}\gamma_k x_{t-k}
\end{eqnarray*} 
be stationary Gaussian processes and designate by $\xi_i$ the weights of the (purely non-deterministic) Wold-decomposition of $x_t$. %In this general framework, forecasting or signal extraction are obtained by selecting suitable $\delta$ or $\gamma_k$ as shown in the empirical examples below. 
%The  estimate $y_t=\sum_{k=0}^{L-1}b_kx_{t-k}$ of $z_{t+\delta}$ is then called a forecast, a nowcast or a backcast depending on $\delta>0,\delta=0$ or $\delta<0$. 
Then target and predictor can be formally re-written as 
\begin{eqnarray*}
z_t&=&\sum_{|k|<\infty}(\gamma\cdot\xi)_k \epsilon_{t-k}\\
y_t&=&\sum_{j\geq 0} (b\cdot\xi)_j\epsilon_{t-j}
\end{eqnarray*} 
where $(\gamma\cdot\xi)_k=\sum_{m\leq k} \xi_{k-m}\gamma_m$ and $(b\cdot\xi)_j=\sum_{n=0}^{\min(L-1,j)} \xi_{j-n}b_n $ are convolutions of the sequences $\gamma_k$ and $b_j$ with the Wold-decomposition $\xi_i$ of $x_t$. The SSA-criterion then becomes 
\begin{eqnarray}\label{gen_stat_x}
\max_{(\mathbf{b}\cdot\boldsymbol{\xi})}\frac{\sum_{k\geq 0} (\gamma\cdot\xi)_{k+\delta} (b\cdot\xi)_k}{\sqrt{\sum_{|k|<\infty} (\gamma\cdot\xi)_k^2}\sqrt{\sum_{j\geq 0} (b\cdot\xi)_j^2}}\\
\frac{\sum_{j\geq 1}(b\cdot\xi)_{j-1}(b\cdot\xi)_j}{\sum_{j\geq 0}(b\cdot\xi)_j^2}=\rho_1\nonumber
\end{eqnarray}
which can be solved for $(b\cdot\xi)_j, j=0,1,...$, see theorem \ref{lambda}.  The  sought-after filter coefficients $b_k$ can then be obtained from $(b\cdot\xi)_j$ by inversion or deconvolution, see section \ref{example_autocor}. Note that non-stationary integrated processes could be addressed in a similar vein, assuming some initialization settings, see e.g. McElroy and Wildi (2020). However, since the concept of a holding-time, i.e. the expected duration between consecutive zero-crossings, would generally not be properly defined anymore, we henceforth assume non-stationary trending data to be suitably transformed or differenced. Also, we refer to standard results in textbooks for a derivation of $\xi_k$ or $\epsilon_t$ based on a finite sample $x_1,...,x_T$, see e.g. Brockwell and Davis (1993): worked-out examples are provided in sections \ref{example_autocor}, \ref{smooth_unsmooth} and \ref{conv_amp}.  Finally, for notational convenience we henceforth rely on the BMF, acknowledging that straightforward modifications would apply in the case of autocorrelation.     
  %From an empirical perspective, we argue that growth-rates of a wide range of economic time series are in accordance with our simplifying assumption, see e.g. the so-called 'typical spectral shape' of an economic variable in Granger (1966). %To conclude, we note that the procedure could be extended to non-stationary integrated processes. % and its utility would be questionable in the context of suitably transformed  data, typically differences or log-returns, at least if the transformation does not impede the analysis.   % assumption in terms of  conditional heteroscedasticity (vola-clustering) or so-called 'fat tails' (large kurtosis, outliers) is analyzed in section \ref{robustness_SSA}.








\section{Solution of the SSA-Criterion: Frequency-Domain}\label{theorem_SSA}



%The structure of the problem is analyzed in section \ref{gen_sol} together with a numerical optimization algorithm and a special case closed-form solution is elaborated in section \ref{ar1closed} . 

%\subsection{General Solution and Numerical Optimization}\label{gen_sol}

The following proposition re-formulates the target specification in terms of the MSE-predictor. 
\begin{Proposition}
Under the BMF let $\hat{z}_{t,\delta}=\sum_{k=0}^{L-1}\gamma_{k+\delta}\epsilon_{t-k}=\boldsymbol{\gamma}_{\delta}'\boldsymbol{\epsilon}_{t}$ designate the classic MSE-predictor of $z_{t+\delta}$. Then the original target $z_{t+\delta}$ can be replaced by $\hat{z}_{t,\delta}$ in the SSA-criterion.
\end{Proposition}
Proof\\

A proof follows from 
\begin{eqnarray*}
&&\textrm{Arg}\left(\max_{\mathbf{b}}\rho(y,\hat{z},\delta)|\rho_1\right)=
\textrm{Arg}\left(\left.\max_{\mathbf{b}}\frac{\sum_{k=0}^{L-1}b_k\gamma_{k+\delta}}{\sqrt{\sum_{k=0}^{L-1}b_k^2}\sqrt{\sum_{k=0}^{L-1}\gamma_{k+\delta}^2}}\right|{\rho_1}\right)\\
&=&\textrm{Arg}\left(\left.\max_{\mathbf{b}}\frac{\sum_{k=0}^{L-1}b_k\gamma_{k+\delta}}{\sqrt{\sum_{k=0}^{L-1}b_k^2}\sqrt{\sum_{k=-\infty}^{\infty}\gamma_{k+\delta}^2}}\right|{\rho_1}\right)=\textrm{Arg}\left(\max_{\mathbf{b}}\rho(y,{z},\delta)|\rho_1\right)
\end{eqnarray*}
where $\cdot|\rho_1$ denotes conditioning, subject to the holding-time constraint, and $\textrm{Arg}(\cdot)$ means the solution or argument of the optimization. \\

The proposition suggests that the SSA-predictor $y_t$ should 'fit' the MSE-predictor $\hat{z}_{t,\delta}$ while complying with the holding-time constraint: if $\hat{z}_{t,\delta}$ matches the constraint then SSA and MSE coincide and the constraint could be dropped (so-called 'degenerate' case). Therefore, we henceforth refer to $\hat{z}_{t,\delta}$ (or $\boldsymbol{\gamma}_{\delta}$) as an equivalent target specification. Let then  
\[
M=\left(\begin{array}{ccccccccc}0&0.5&0&0&0&...&0&0&0\\
0.5&0&0.5&0&0&...&0&0&0\\
...&&&&&&&&\\
0&0&0&0&0&...&0.5&0&0.5\\
0&0&0&0&0&...&0&0.5&0
\end{array}\right)
\]
of dimension $L*L$ designate the so-called autocovariance-generating matrix so that $\rho(y,y,1)=\displaystyle{\frac{\mathbf{b'Mb}}{\mathbf{b'b}}}$. The following proposition relates stationary points of the lag-one autocorrelation $\rho(y,y,1)$ with eigenvectors and eigenvalues of   $\mathbf{M}$. 


\begin{Proposition}\label{stationary_eigenvec}
Under the BMF, the vector $\mathbf{b}:=(b_0,...,b_{L-1})'\neq 0$ is a stationary point of the lag-one autocorrelation $\rho(y,y,1)=\displaystyle{\frac{\mathbf{b'Mb}}{\mathbf{b'b}}}$ if and only if $\mathbf{b}$ is an eigenvector of the autocovariance-generating matrix 
with corresponding eigenvalue $\rho(y,y,1)$. The extremal values $\rho_{min}(L)$ and $\rho_{max}(L)$ defined in proposition \ref{maxrho} correspond to $\min_i\lambda_i$ and $\max_i\lambda_i$ where $\lambda_i$, $i=1,...L$ are the eigenvalues of $\mathbf{M}$. 
\end{Proposition}

Proof\\

Assume, for simplicity, that $\mathbf{b}\neq\mathbf{0}$ is defined on the unit-sphere so that  
\begin{eqnarray*}
\mathbf{b'b}&=&1\\
\rho(y,y,1)&=&\frac{\mathbf{b'Mb}}{\mathbf{b'b}}=\mathbf{b'Mb}
\end{eqnarray*}
A stationary point of $\rho(y,y,1)$ is found by equating the derivative of the Lagrangian $\mathfrak{L}=\mathbf{b'Mb}-\lambda(\mathbf{b'b}-1)$ to zero i.e.
\[
\mathbf{Mb}=\lambda\mathbf{b}
\]
We deduce that $\mathbf{b}$ is a stationary point if and only if it is an eigenvector of $\mathbf{M}$. Then 
\[
\rho(y,y,1)=\frac{\mathbf{b}'\mathbf{Mb}}{\mathbf{b}'\mathbf{b}}=\lambda_i\frac{\mathbf{b}'\mathbf{b}}{\mathbf{b}'\mathbf{b}}=\lambda_i
\]
for some $i\in\{1,...,L\}$ and therefore $\rho(y,y,1)$ must be the corresponding eigenvalue, as claimed. Since the  unit-sphere is free of boundary-points we conclude that the extremal values $\rho_{min}(L)$, $\rho_{max}(L)$ must be stationary points i.e. $\rho_{min}(L)=\min_i\lambda_i$ and $\rho_{max}(L)=\max_i\lambda_i$.\\


By abuse of terminology we  now identify filter coefficients and corresponding filter outputs (targets or predictors) so that e.g. $y_t$ and $\mathbf{b}$ will  both be referred to as predictor or estimate (and similarly for the target(s)).  
Let then   $\lambda_{i},\mathbf{v}_{i}$ denote the pairings of eigenvalues and eigenvectors of $\mathbf{M}$, ordered according to the increasing size of $\lambda_{i}=-\cos(\omega_i)$, where $\omega_i=i\pi /(L+1)$ are the discrete Fourier frequencies see e.g. Anderson (1975), 
<<label=init,echo=FALSE,results=hide>>=
# Check cosine formula for eigenvalues, see e.g. Anderson
L<-11
M<-matrix(nrow=L,ncol=L)
M[L,]<-rep(0,L)
M[L-1,]<-c(rep(0,L-1),0.5)
for (i in 1:(L-2))
  M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
M<-M+t(M)
  
eigen(M)$values
-cos(pi*(1:L)/(L+1))
# Cancel each other
-cos(pi*(1:L)/(L+1))+eigen(M)$values
@ 
and let $\mathbf{V}$ designate the orthonormal basis of $\mathbb{R}^{L}$ based on the (Fourier) column-vectors $\mathbf{v}_i$, $i=1,...,L$. We then consider  the spectral decomposition of the target $\boldsymbol{\gamma}_{\delta}\neq \mathbf{0}$   
\begin{equation}\label{specdec}
\boldsymbol{\gamma}_{\delta}=\sum_{i=n}^{m}w_i\mathbf{v}_i=\mathbf{V}\mathbf{w}
\end{equation}
with (spectral-) weights $\mathbf{w}=(w_1,...,w_L)'$,  where $1\leq n\leq m \leq L$ and  $w_{m}\neq 0,w_n\neq 0$. %If $n=m$ then $\boldsymbol{\gamma}_{\delta}=w_n\mathbf{v}_n$ is an eigenvector of $\mathbf{M}$. 
If $n>1$ or $m<L$ then $\boldsymbol{\gamma}_{\delta}$ is called \emph{band-limited}. Also, we refer to $\boldsymbol{\gamma}_{\delta}$ as having \emph{complete} (or \emph{incomplete}) spectral support depending on $w_i\neq 0$ for $i=1,...,L$ (or not). %: a band-limited target has incomplete spectral support but the converse does not hold, in general. 
Finally, denote by $NZ:=\{i|w_i\neq 0\}$ the set of indexes of  non-vanishing weights $w_i$. The following theorem derives a parametric functional form of the SSA solution under various assumptions about the problem specification.




\begin{Theorem}\label{lambda}
Consider the SSA optimization problem \ref{crit1} under the BMF and consider the following set of regularity assumptions:
\begin{enumerate}
\item $\boldsymbol{\gamma}_{\delta}\neq 0$ (identifiability) and $L\geq 3$ (smoothing).
%\item $\mathbf{b}$ is not an eigenvector of $\mathbf{M}$% $\rho_1\neq \lambda_{i_0N}$ for all $i_0$ such that $w_{i_0}\neq 0$ in the spectral decomposition \ref{specdec} of $\boldsymbol{\gamma}_{\delta}$ (indeterminacy)
\item The SSA estimate $\mathbf{b}$ is not proportional to $\boldsymbol{\gamma}_{\delta}$, denoted by $\mathbf{b}\not\propto\boldsymbol{\gamma}_{\delta}$ (non-degenerate case).
\item $|\rho_1|<\rho_{max}(L)$ (admissibility of the holding-time constraint).%\footnote{In the non-degenerate case $n\neq m$, see the proof of the theorem. Furthermore, the eigenvectors $\lambda_{i}$ of $\mathbf{M}$ are pairwise different.}
\item The MSE-estimate $\boldsymbol{\gamma}_{\delta}$ has complete spectral support (completeness).
\end{enumerate}
Then
\begin{enumerate}

\item \label{ass5}If the third regularity assumption is violated (admissibility) and if $|\rho_1|>\rho_{max}(L)$, then the problem cannot be solved unless the filter-length $L$ is increased such that $|\rho_1|\leq\rho_{max}(L)$. On the other hand, if $\rho_1=\lambda_1=-\rho_{max}(L)$ or $\rho_1=\lambda_L=\rho_{max}(L)$ (limiting cases), then $\textrm{sign}(w_1)\mathbf{v}_{1}$ or $\textrm{sign}(w_L)\mathbf{v}_{L}$ are the corresponding solutions of the SSA-criterion (up to arbitrary scaling), where $w_i$ are the spectral weights in \ref{specdec} and where it is assumed that $w_1\neq 0$, if $\rho_1=\lambda_1$, or $w_L\neq 0$, if $\rho_1=\lambda_L$.   

%\item \label{ass1} If the third assumption (admissibility) does not hold, then $\mathbf{b}=\mathbf{v}_n$ or $\mathbf{b}=\mathbf{v}_m$ with corresponding $\rho_1=\lambda_n$ or $\rho_1=\lambda_m$. In this case the holding-time constraint overrides the criterion and the problem could be addressed by allowing for a larger filter-length $L'>L$.
\item \label{ass1}If all regularity assumptions hold,  then the SSA-estimate $\mathbf{b}$ has the parametric functional form
\begin{eqnarray}\label{diff_non_home}
\mathbf{b}=D\boldsymbol{\nu}^{-1}\boldsymbol{\gamma}_{\delta}=D\sum_{i=1}^L \frac{w_i}{2\lambda_{i}-\nu}\mathbf{v}_{i}
\end{eqnarray}
where $D\neq 0$, $\nu\in \mathbb{R}-\{2\lambda_i|i=1,...,L\}$, and $\boldsymbol{\nu}:=2\mathbf{M}-\nu\mathbf{I}$ is an invertible $L*L$ matrix. Although $b_{-1},b_L$ do not explicitly appear in $\mathbf{b}$ it is at least implicitly assumed that $b_{-1}=b_L=0$ (implicit boundary constraints). Furthermore, $\mathbf{b}$ is uniquely determined by the scalar $\nu$, down to the arbitrary scaling term $D$, whereby the sign of $D$ is determined by requiring a positive criterion-value.


\item \label{ass3}If all regularity assumptions hold, then the lag-one autocorrelation of $\mathbf{b}$ in \ref{diff_non_home} is 
\begin{eqnarray}\label{rho_fd}
\rho(\nu):=\frac{\mathbf{b}'\mathbf{Mb}}{\mathbf{b}'\mathbf{b}}=\frac{\sum_{i=1}^L\lambda_{i}w_i^2\frac{1}{(2\lambda_{i}-\nu)^2}}{\sum_{i=1}^Lw_i^2\frac{1}{(2\lambda_{i}-\nu)^2}}
\end{eqnarray}
and $\nu=\nu(\rho_1)$ can always be selected such that the SSA-solution $\mathbf{b}=\mathbf{b}(\nu(\rho_1))$ in \ref{diff_non_home} complies with the holding-time constraint.

\item \label{ass4} If $|\nu|>2\rho_{max}(L)$ %and if the vector $\boldsymbol{\gamma}_{\delta}$ with components $\gamma_{k+\delta}, k=0,...,L-1$ is not an eigenvector of $\mathbf{M}$ 
then $\rho(\nu)$ as defined in \ref{rho_fd} is a strictly monotonic function in $\nu$ and the parameter  $\nu$  in \ref{diff_non_home} is determined uniquely by the holding-time constraint $\rho(\nu)=\rho_1$. 

\end{enumerate}
\end{Theorem}




Proof\\

The SSA-problem  \ref{crit1} can be rewritten as
\begin{eqnarray}
\textrm{max}_{\mathbf{b}}~\boldsymbol{\gamma}_{\delta}'\mathbf{b}&&\nonumber\\
\mathbf{b}'\mathbf{b}&=&1\nonumber\\
\mathbf{b}'\mathbf{M}\mathbf{b}&=&\rho_1\label{nonconvex}
\end{eqnarray}
where $\mathbf{b}'\mathbf{b}=1$ is an arbitrary scaling rule. 
Consider the spectral decomposition  
\begin{eqnarray}\label{specdecdecb}
\mathbf{b}:=\sum_{i=1}^L\alpha_i\mathbf{v}_i
\end{eqnarray}
of $\mathbf{b}$. Since $\mathbf{v}_i$ is an orthonormal basis, the length-constraint $\mathbf{b}'\mathbf{b}=1$ implies $\sum_{i=1}^L\alpha_i^2=1$ (unit-sphere constraint); moreover, from the holding-time constraint and from  orthogonality of $\mathbf{v}_i$  we infer
\begin{eqnarray*}
\rho_1=\mathbf{b}'\mathbf{Mb}=\sum_{i=1}^L \alpha_i^2\lambda_i
\end{eqnarray*}
so that 
\begin{eqnarray*}
\alpha_{j_0}=\pm \sqrt{\frac{\rho_1}{\lambda_{j_0}}-\sum_{k\neq j_0}\alpha_k^2\frac{\lambda_k}{\lambda_{j_0}}}
\end{eqnarray*}
where $j_0$ is such that $\lambda_{j_0}\neq 0$\footnote{If $L$ is an even integer, then $\lambda_i\neq 0$ for all $i$, $1\leq i\leq L$. Otherwise, $\lambda_{i_0}=0$ for $i_0=1+(L-1)/2$.}. The SSA-problem can be solved if the hyperbola, defined by the holding-time constraint, intersects the unit-sphere. For this purpose we  plug the former equation into the latter:
\[
\alpha_{i_0}^2=1-\sum_{i\neq i_0}\alpha_i^2=1-\left(\frac{\rho_1}{\lambda_{j_0}}-\sum_{k\neq j_0}\alpha_k^2\frac{\lambda_k}{\lambda_{j_0}}\right)-\sum_{i\neq i_0,j_0}\alpha_i^2
\]
where $i_0\neq j_0$. 
Solving for $\alpha_{i_0}$ then leads to
\begin{eqnarray}\label{ai0}
\alpha_{i_0}=\pm\sqrt{\frac{\lambda_{j_0}-\rho_1}{\lambda_{j_0}-\lambda_{i_0}}-\sum_{k\neq i_0,k\neq j_0}\alpha_k^2\frac{\lambda_{j_0}-\lambda_k}{\lambda_{j_0}-\lambda_{i_0}}}
\end{eqnarray}
Under the case posited in assertion \ref{ass5} $\rho_1=\lambda_{i_0}$ with either $i_0=1$, i.e. $\rho_1=-\rho_{max}(L)$, or $i_0=L$, i.e. $\rho_1=\rho_{max}(L)$. Let then $i_0=1$ so that \ref{ai0} becomes
\begin{eqnarray*}\label{ai0n}
\alpha_{1}=\pm\sqrt{1-\sum_{k\neq 1,k\neq j_0}\alpha_k^2\frac{\lambda_{j_0}-\lambda_k}{\lambda_{j_0}-\lambda_{1}}}
\end{eqnarray*}
Assume also $j_0=2$ (a similar proof can be derived for arbitrary $j_0\leq 2$, see footnote \ref{footnval})
so that $\lambda_{2}-\lambda_k<0$ in the nominator  and $\lambda_{2}-\lambda_{1}>0$ in the denominator in the last expression. Therefore, the term under the square-root is larger than one if $\alpha_k\neq 0$ for some $k>2$ which would imply $|\alpha_{1}|>1$ thus contradicting the unit-sphere constraint\footnote{\label{footnval}Similar contradictions could be derived for any $j_0>1$ since $\left|\frac{\lambda_{j_0}-\lambda_k}{\lambda_{j_0}-\lambda_{i_0}}\right|<1$ if $i_0=1$ so that if any $\alpha_k\neq 0$, for $k\neq 1,j_0$, then equation \ref{ai0} would conflict with the unit-sphere constraint.}. We then deduce $\alpha_k=0$ for $k>2$ so that $\alpha_{1}=\pm 1$ and  $\alpha_{2}=0$ and therefore $\pm \mathbf{v}_1$ are the only admissible potential solutions of the SSA-problem: the contacts of unit-sphere and hyperbola are tangential at the vertices $\pm\mathbf{v}_1$. Since $w_1\neq 0$ by assumption, the solution must be $\mathbf{b}:=\textrm{sign}(w_1)\mathbf{v}_1$ because it maximizes the criterion value $\boldsymbol{\gamma}_{\delta}'\mathbf{b}=\textrm{sign}(w_1)w_1>0$. 
If $w_1=0$ then the problem is ill-conditioned in the sense that the only possible solutions $\pm \mathbf{v}_1$ do not correlate with the target $z_{t+\delta}$ anymore.  
Note that  similar reasoning applies if $i_0=L$, setting $j_0=L-1$ in \ref{ai0} and assuming $w_L\neq 0$.\\
To show the second assertion we now assume that all regularity assumptions hold and we define the Lagrangian function 
\begin{eqnarray}\label{lag_SSA}
L:=\boldsymbol{\gamma}_{\delta}'\mathbf{b}-\lambda_1(\mathbf{b}'\mathbf{b}-1)-\lambda_2(\mathbf{b}'\mathbf{M}\mathbf{b}-\rho_1)
\end{eqnarray}
Since the unit-sphere $\mathbf{b'b}=1$ is free of boundary points, the solution $\mathbf{b}$ of the SSA-problem must conform to the stationary Lagrangian or vanishing gradient equations
\[
\boldsymbol{\gamma}_{\delta}=\lambda_1 2\mathbf{b}+\lambda_2 (\mathbf{M}+\mathbf{M}')\mathbf{b}=\lambda_1 2\mathbf{b}+\lambda_2 2\mathbf{M}\mathbf{b}
\]
Note that the second regularity assumption (non-degenerate case) implies that the holding-time constraint \ref{nonconvex} is 'active' i.e. $\lambda_2\neq 0$.  Dividing by $\lambda_2$ then leads to 
\begin{eqnarray}\label{diff_non_hom_matrix}
D\boldsymbol{\gamma}_{\delta}&=& \boldsymbol{\nu}\mathbf{b}\\
\boldsymbol{\nu}&:=&(2\mathbf{M}-\nu\mathbf{I})\label{labelNu}
\end{eqnarray}
where $D=1/\lambda_2$  and $\nu=-2\frac{\lambda_1}{\lambda_2}$. By orthonormality of $\mathbf{v}_i$ the  objective function is
\[\boldsymbol{\gamma}_{\delta}'\mathbf{b}=\sum_{i=1}^L\alpha_iw_i\]
where we rely on the spectral decomposition \ref{specdecdecb} of $\mathbf{b}$. 
By assumption $L\geq 3$ (smoothing) so that $\boldsymbol{\alpha}=(\alpha_1,...,\alpha_L)' $ is defined on a  $L-2\geq 1$ dimensional intersection of unit-sphere and holding-time constraints. We then infer that the objective function is not overruled by the constraint i.e. $|\lambda_2|<\infty$ so that $D\neq 0$ in \ref{diff_non_hom_matrix}, as claimed. Furthermore, equation \ref{diff_non_hom_matrix} can be written as 
\begin{eqnarray}\label{ar2}
b_{k+1}-\nu b_k+b_{k-1}&=&D\gamma_{k+\delta}~,~1\leq k\leq L-2\\
b_{1}-\nu b_0&=&D\gamma_{\delta}~,~k=0\nonumber\\
-\nu b_{L-1}+b_{L-2}&=&D\gamma_{L-1+\delta}~,~k=L-1\nonumber
\end{eqnarray}
for $k=0,...,L-1$ so that $b_{-1}=b_L=0$ are implicitly assumed for the natural extension $(b_{-1},\mathbf{b},b_L)'$ of the time-invariant linear filter. 
The eigenvalues of $\boldsymbol{\nu}$ are $2\lambda_{i}-\nu$ with corresponding eigenvectors $\mathbf{v}_{i}$.  We note that if $\mathbf{b}$ is the solution of the SSA-problem, then $\nu/2$ cannot be an eigenvalue of $\mathbf{M}$ since otherwise $\boldsymbol{\nu}$ in \ref{diff_non_hom_matrix} would map one of the eigenvectors in the spectral decomposition of $\mathbf{b}$ to zero which would contradict the last regularity assumption (completeness: see corollary \ref{incomplete_spec_sup} for a corresponding extension) since $D\neq 0$. Therefore we can assume that $\boldsymbol{\nu}^{-1}$ exists and
\[
\boldsymbol{\nu}^{-1}=\mathbf{V}\mathbf{D}_{\nu}^{-1}\mathbf{V}'
\] 
where the diagonal matrix $\mathbf{D}_{\nu}^{-1}$ has entries $\frac{1}{2\lambda_{i}-\nu}$. We can then solve  \ref{diff_non_hom_matrix} for $\mathbf{b}$ and obtain
\begin{eqnarray}\label{diff_non_hom_matrixe}
\mathbf{b}&=&D\boldsymbol{\nu}^{-1}\boldsymbol{\gamma}_{\delta}\\
&=&D\mathbf{V}\mathbf{D}_{\nu}^{-1}\mathbf{V}' \mathbf{V}\mathbf{w}\nonumber\\
&=&D\sum_{i=1}^L \frac{w_i}{2\lambda_{i}-\nu}\mathbf{v}_{i}\label{specdecb}
\end{eqnarray}
where we inserted \ref{specdec}. Since $\boldsymbol{\nu}$ has full rank, the solution of the SSA-problem is uniquely determined by $\nu$, at least down to arbitrary scaling, hereby completing the proof of assertion \ref{ass1}.\\ % is a solution of the homogeneous equation
%\[
%\mathbf{b}_1/D_1-\mathbf{b}_2/D_2= \boldsymbol{\nu}^{-1}\mathbf{0}=\mathbf{0}
%\]
%so that the (unit vector) solution of the SSA-problem is uniquely identified by $\nu$, hereby completing the proof of assertion \ref{ass1}.\\
We next proceed to assertion \ref{ass3} and consider
\begin{eqnarray}
\rho(\nu)&=&\rho(y(\nu),y(\nu),1)=\frac{\mathbf{b}'\mathbf{M}\mathbf{b}}{\mathbf{b}'\mathbf{b}}\nonumber\\
&=&\frac{\left(D\sum_{i=1}^L \frac{w_i}{2\lambda_{i }-\nu}\mathbf{v}_i\right)'\mathbf{M}\left(D\sum_{i=1}^L \frac{w_i}{2\lambda_{i }-\nu}\mathbf{v}_i\right)}{\left(D\sum_{i=1}^L \frac{w_i}{2\lambda_{i }-\nu}\mathbf{v}_i\right)'\left(D\sum_{i=1}^L \frac{w_i}{2\lambda_{i }-\nu}\mathbf{v}_i\right)}\nonumber\\
&=&\frac{\sum_{i=1}^L \displaystyle{\frac{\lambda_{i }w_i^2}{(2\lambda_{i }-\nu)^2}}}{\sum_{i=1}^L \displaystyle{\frac{w_i^2}{(2\lambda_{i }-\nu)^2}}}\label{specdecrho}
\end{eqnarray}
where we inserted \ref{specdecb} and made use of orthonormality $\mathbf{v}_i'\mathbf{v}_j=\delta_{ij}$. The last expression implies $\lim_{\nu\to2\lambda_{i }}\rho(\nu)=\lambda_{i }$ for all $i=1,...,L$. Since  $\lambda_1=-\rho_{max}(L)$ and $\lambda_L=\rho_{max}(L)$, by proposition \ref{stationary_eigenvec}, we infer that lower and upper boundaries $\pm\rho_{max}(L)$ can be reached by $\rho(\nu)$, asymptotically. Continuity of $\rho(\nu)$ and the intermediate-value theorem then imply that any $\rho_1\in ]-\rho_{max}(L),\rho_{max}(L)[$ is admissible for the holding-time constraint under the posited assumptions.\\
We now proceed to assertion \ref{ass4} by showing that the parameter $\nu$ is determined uniquely by $\rho_1$ in the holding-time constraint if $|\nu|>2\rho_{max}(L)$. Note that all eigenvalues $2\lambda_{i}-\nu$ of ${\boldsymbol{\nu}}$ must be (strictly) negative, if $\nu>2\rho_{max}(L)$, or strictly positive, if $\nu<-2\rho_{max}(L)$, so that all eigenvalues of ${\boldsymbol{\nu}}^{-1}$, being the reciprocals of the former, must be of the same sign, either  all positive or all negative. Finally, the eigenvalues of ${\boldsymbol{\nu}}$ or ${\boldsymbol{\nu}}^{-1}$ must be pairwise different since the eigenvalues of ${\mathbf{M}}$ are so. We then obtain
\begin{eqnarray}
\frac{\partial\rho\Big(y(\nu),y(\nu),1\Big)}{\partial\nu}&=&\frac{\partial}{\partial\nu}\left(\frac{\mathbf{b}'\mathbf{Mb}}{\mathbf{b}'\mathbf{b}}\right)=\frac{\partial}{\partial\nu}\left(\frac{\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-1}~'{\mathbf{M}}{\boldsymbol{\nu}}^{-1}\boldsymbol{\gamma}_{\delta}}{\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-1}~'{\boldsymbol{\nu}}^{-1}\boldsymbol{\gamma}_{\delta}}\right)=\frac{\partial}{\partial\nu}\left(\frac{\boldsymbol{\gamma}_{\delta}'{\mathbf{M}}{\boldsymbol{\nu}}^{-2}\boldsymbol{\gamma}_{\delta}}{\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-2}\boldsymbol{\gamma}_{\delta}}\right)\nonumber\\
&=&\frac{2\boldsymbol{\gamma}_{\delta}'{\mathbf{M}}{\boldsymbol{\nu}}^{-3}\boldsymbol{\gamma}_{\delta}\mathbf{b'}\mathbf{b}/D-(2\mathbf{b}'{\mathbf{M}}\mathbf{b}/D)\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-3}\boldsymbol{\gamma}_{\delta}}{( (\mathbf{b}'\mathbf{b})^2/D^2)}\nonumber\\
&=&\frac{2\mathbf{b}'{\mathbf{M}}{\boldsymbol{\nu}}^{-1}\mathbf{b}\mathbf{b'}\mathbf{b}/D^2-2\mathbf{b}'{\mathbf{M}}\mathbf{b}\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}/D^2}{\mathbf{b}'\mathbf{b}/D^2}\nonumber\\
&=&2\mathbf{b}'{\mathbf{M}}{\boldsymbol{\nu}}^{-1}\mathbf{b}\mathbf{b'}\mathbf{b}-2\mathbf{b}'{\mathbf{M}}\mathbf{b}\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}\label{vgrt2}
\end{eqnarray}
where commutativity of the matrix multiplications (used in deriving the third and next-to-last equations)  follows from the fact that the matrices are symmetric and simultaneously diagonalizable (same eigenvectors); also ${\boldsymbol{\nu}}^{-1}~'={\boldsymbol{\nu}}^{-1}$ (symmetry) and we relied on generic matrix differentiation rules in the third equation\footnote{$\frac{\partial({\boldsymbol{\nu}}^{-1})}{\partial\nu}={\boldsymbol{\nu}}^{-2}$ and $\frac{\partial({\boldsymbol{\nu}}^{-2})}{\partial\nu}=2{\boldsymbol{\nu}}^{-3}$. For the first equation the general rule is $\frac{\partial(\boldsymbol\nu^{-1})}{\partial\nu}=-\boldsymbol\nu^{-1}\frac{\partial\boldsymbol\nu}{\partial\nu}\boldsymbol\nu^{-1}$, noting that $\frac{\partial\boldsymbol\nu}{\partial\nu}=-\mathbf{I}$. The second equation follows by inserting the first equation into $\frac{\partial(\boldsymbol\nu^{-2})}{\partial\nu}=\frac{\partial(\boldsymbol\nu^{-1})}{\partial\nu}{\boldsymbol{\nu}}^{-1}+{\boldsymbol{\nu}}^{-1}\frac{\partial({\boldsymbol{\nu}}^{-1})}{\partial\nu}$.};  finally we relied on $\mathbf{b}'\mathbf{b}=1$ in the last equation. We can now insert \[{\mathbf{M}}{\boldsymbol{\nu}}^{-1}=\frac{\nu}{2}{\boldsymbol{\nu}}^{-1}+0.5\mathbf{I}\]
which is a reformulation of $(2{\mathbf{M}}-\nu\mathbf{I}){\boldsymbol{\nu}}^{-1}=\mathbf{I}$  into the first summand  in \ref{vgrt2} to obtain
\begin{eqnarray*}
2\mathbf{b}'{\mathbf{M}}{\boldsymbol{\nu}}^{-1}\mathbf{b}\mathbf{b'}\mathbf{b}=\left(\nu\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}+\mathbf{b'}\mathbf{b}\right)\mathbf{b'}\mathbf{b}
\end{eqnarray*}
We can now insert this expression into \ref{vgrt2} and isolate $\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}$ to obtain
\begin{eqnarray}
\frac{\partial\rho\Big(y(\nu),y(\nu),1\Big)}{\partial\nu}&=&-\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}\left(2\mathbf{b}'\mathbf{{M}b}-\nu\mathbf{b}'\mathbf{b}\right)+(\mathbf{b}'\mathbf{b})^2\nonumber\\
&=&-\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}\mathbf{b}'\left(2{\mathbf{M}}-\nu{\mathbf{I}}\right)\mathbf{b}+(\mathbf{b}'\mathbf{b})^2\nonumber\\
&=&-\mathbf{b}'{\boldsymbol{\nu}}^{-1}\mathbf{b}\mathbf{b}'{\boldsymbol{\nu}}\mathbf{b}+(\mathbf{b}'\mathbf{b})^2\nonumber\\
&=&-\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-3}\boldsymbol{\gamma}_{\delta}\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-1}\boldsymbol{\gamma}_{\delta}+(\boldsymbol{\gamma}_{\delta}'{\boldsymbol{\nu}}^{-2}\boldsymbol{\gamma}_{\delta})^2\nonumber\\
&=&-\boldsymbol{\gamma}_{\delta}'\mathbf{V}\mathbf{D}^{-3}\mathbf{V}'\boldsymbol{\gamma}_{\delta}\boldsymbol{\gamma}_{\delta}'\mathbf{V}\mathbf{D}^{-1}\mathbf{V}'\boldsymbol{\gamma}_{\delta}+(\boldsymbol{\gamma}_{\delta}'\mathbf{V}\mathbf{D}^{-2}\mathbf{V}'\boldsymbol{\gamma}_{\delta})^2\nonumber\\
&=&-\boldsymbol{\tilde{\gamma}}_{+\delta}'\mathbf{D}^{-3}\boldsymbol{\tilde{\gamma}}_{+\delta}\boldsymbol{\tilde{\gamma}}_{+\delta}'\mathbf{D}^{-1}\boldsymbol{\tilde{\gamma}}_{+\delta}+(\boldsymbol{\tilde{\gamma}}_{+\delta}'\mathbf{D}^{-2}\boldsymbol{\tilde{\gamma}}_{+\delta})^2\nonumber
\end{eqnarray}
where ${\boldsymbol{\nu}}^{-k}=\mathbf{V}\mathbf{D}^{-k}\mathbf{V}'$ and $\mathbf{D}^{-k}$, $k=1,2,3$, is diagonal with eigenvalues $\lambda_{i\nu}^{-k}:=(2\lambda_i-\nu)^{-k}$ being all (strictly) positive, 
if $\nu<-2\rho_{max}(L)$, or either all (strictly) negative or all (strictly) positive depending on the exponent $k$ being odd or even, if $\nu>2\rho_{max}(L)$; also,  $\boldsymbol{\tilde{\gamma}}_{+\delta}=\mathbf{V}'\boldsymbol{{\gamma}}_{+\delta}=(w_1,...,w_L)'$. Therefore
\begin{eqnarray}
\frac{\partial\rho\Big(y(\nu),y(\nu),1\Big)}{\partial\nu}&=&-\sum_{j=0}^{L-1}w_j^2\lambda_{j\nu}^{-3}\sum_{j=0}^{L-1}w_j^2\lambda_{j\nu}^{-1}+\left(\sum_{j=0}^{L-1}w_j^2\lambda_{j\nu}^{-2}\right)^2\nonumber\\
&=&-\sum_{i> k}w_i^2w_k^2 \Big(\lambda_{i\nu}^{-1}\lambda_{k\nu}^{-3}+\lambda_{i\nu}^{-3}\lambda_{k\nu}^{-1}-2\lambda_{i\nu}^{-2}\lambda_{k\nu}^{-2}\Big)\label{dfgtree}
\end{eqnarray}
where the terms in $w_j^4$ cancel. Consider now
\begin{eqnarray}\lambda_{i\nu}^{-1}\lambda_{k\nu}^{-3}+\lambda_{i\nu}^{-3}\lambda_{k\nu}^{-1}-2\lambda_{i\nu}^{-2}\lambda_{k\nu}^{-2}&=&\lambda_{i\nu}^{-1}\lambda_{k\nu}^{-1}\Big(\lambda_{i\nu}^{-2}+\lambda_{k\nu}^{-2}-2\lambda_{i\nu}^{-1}\lambda_{k\nu}^{-1}\Big)=\lambda_{i\nu}^{-1}\lambda_{k\nu}^{-1}\Big(\lambda_{i\nu}^{-1}-\lambda_{k\nu}^{-1}\Big)^{2}~>~0\nonumber
\end{eqnarray}
where the strict inequality holds because $\lambda_{i\nu}^{-1}=(2\lambda_i-\nu)^{-1}$ are all of the same sign, pairwise different and non-vanishing if $|\nu|>2\rho_{max}(L)$. Since  $w_i\neq 0$ (last regularity assumption: completeness) we deduce $w_i^2w_k^2\neq 0$ in \ref{dfgtree}. Therefore, the latter expression is strictly negative and we conclude that $\rho\Big(y(\nu),y(\nu),1\Big)$ must be a strictly monotonic function of $\nu$ if $|\nu|>2\rho_{max}(L)$, as claimed. \\
<<label=init,echo=FALSE,results=hide>>=
# Check next formula for derivative of rho with respect to nu
if (recompute_calculations)
{
  len<-10
  set.seed(1)
  gammak<-rnorm(len)
  
  
  
  L<-len
  M<-matrix(nrow=L,ncol=L)
  
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  
  M%*%M
  
  eig<-eigen(M)
  k<-1
  ts.plot(eig$vectors[,k])
  
  nu<-rnorm(1)
  nu<-3
  Nu<-2*M-nu*diag(rep(1,L))
  eigen(solve(Nu))$values
  t(eigen(solve(Nu)%*%solve(Nu))$vector)%*%eigen(solve(Nu)%*%solve(Nu))$vector
  
  b<-solve(Nu)%*%gammak
  eigen(diag(rep(sum(b^2),len))-b%*%t(b))$values
  
  
  solve(Nu)%*%M%*%solve(Nu)-M%*%solve(Nu)%*%solve(Nu)
  solve(Nu)-t(solve(Nu))
  
  eigen(solve(Nu)%*%M)$values
  eigen(solve(Nu))$values
  eigen(solve(Nu)%*%solve(Nu))$values
  eigen(solve(Nu)%*%solve(Nu)%*%solve(Nu))$values
  solve(Nu)-t(solve(Nu))
  # Geometric series exapnsion conflicts with fact that eigenavlues are negative if nu>0
  
  eigen(diag(rep(sum(b^2),len))-b%*%t(b))$values
  
  0.5*eigen(diag(rep(1,L))+nu*solve(Nu))$values
  
  A<-diag(rep(sum(b^2),len))-b%*%t(b)
  #B<-diag(rep(1,L))+nu*solve(Nu)
  B<-solve(Nu)%*%M
  eigen(A%*%B)$values
  
  t(b)%*%(A%*%B)%*%b
  eigen(solve(Nu)%*%A%*%B%*%solve(Nu))$values
  eigen(M%*%A%*%solve(Nu))$values
  #----------------------------------
  (2*t(gammak)%*%(M%*%solve(Nu)%*%solve(Nu)%*%solve(Nu))%*%gammak*sum(b^2)-
    2*(t(b)%*%(M%*%b))*(t(gammak)%*%(solve(Nu)%*%solve(Nu)%*%solve(Nu))%*%gammak))/sum(b^2)^2
  
  rho0<-t(b)%*%(M%*%b)/(t(b)%*%b)
  
  delta<-0.0001
  nu1<-nu+delta
  Nu1<-2*M-nu1*diag(rep(1,L))
  
  b1<-solve(Nu1)%*%gammak
  
  rho1<-t(b1)%*%(M%*%b1)/(t(b1)%*%b1)
  # Fourth equation checked
  (rho1-rho0)/delta
  
  #------------------------------------------------------------
  # Here again the 6.th equation 
  (2*t(b)%*%(M%*%solve(Nu))%*%b*sum(b^2)-
     2*(t(b)%*%(M%*%b))*(t(b)%*%(solve(Nu))%*%b))/sum(b^2)^2
  
# Here we check the next variant of the 6.th equation 
# Note that in the proof it is assumed that b'b=1 which is not the case here
# The R-code is more general   
  -(t(b)%*%solve(Nu)%*%b)*(t(b)%*%(Nu)%*%b)/sum(b^2)^2+1
#-------------------------------------------------
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Attention: the signs in the next formula are wrong (should be changed: code is pasted from old code where sign was wrong)
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!    
  # Check next set of formula  
  (as.double(t(b)%*%solve(Nu)%*%b)*(2*as.double(t(b)%*%M%*%b)-nu*as.double(t(b)%*%b))-(as.double(t(b)%*%b))^2)/sum(b^2)^2
  
  (t(b)%*%solve(Nu)%*%b*t(b)%*%Nu%*%b-(t(b)%*%b)^2)/sum(b^2)^2
  V<-eigen(M)$vectors
  gammaktilde<-t(V)%*%gammak
  
  dnu<-2*eigen(M)$values-nu
  Dnu<-diag(dnu)
# Last formula with vectors/Matrices 
  as.double(t(gammaktilde)%*%solve(Dnu^3)%*%gammaktilde*t(gammaktilde)%*%solve(Dnu)%*%gammaktilde-(t(gammaktilde)%*%solve(Dnu^2)%*%gammaktilde)^2)/sum(b^2)^2
  
# Next term
  (t(gammaktilde^2)%*%dnu^{-3}*t(gammaktilde)^2%*%dnu^{-1}-(t(gammaktilde^2)%*%dnu^{-2})^2)/sum(b^2)^2
  
  sumc<-0
  for (i in 1:L)
  {
    if (i<L)
    {  
      for (j in (i+1):L)
      {
        if (i!=j)
        {
          sumc<-sumc+gammaktilde[i]^2*gammaktilde[j]^2*(dnu[i]^{-1}*dnu[j]^{-3}+dnu[j]^{-1}*dnu[i]^{-3}-2*dnu[i]^{-2}*dnu[j]^{-2})
        }
      }
    }  
  }
  sumc/sum(b^2)^2
  
# Final term
  sumc<-0
  for (i in 1:L)
  {
    if (i<L)
    {  
      for (j in (i+1):L)
      {
        if (i!=j)
        {
          sumc<-sumc+gammaktilde[i]^2*gammaktilde[j]^2*dnu[i]^{-1}*dnu[j]^{-1}*(dnu[i]^{-1}-dnu[j]^{-1})^2
        }
      }
    }  
  }
  sumc/sum(b^2)^2


}
@



\textbf{Remarks}\\
Extensions to autocorrelated $x_t$ are straightforward, see section \ref{ext_stat} for background and sections \ref{example_autocor}, \ref{smooth_unsmooth} and \ref{conv_amp} for illustration. Also, Gaussianity is not required in the derivation of the above proof because the SSA-criterion \ref{crit1} relies solely on correlations. The Gaussian hypothesis is needed when  establishing formal links between correlations and sign-accuracy or holding-time concepts but  $y_t$ or $z_t$ can be nearly Gaussian even if $\epsilon_t$ isn't, see also section \ref{resil} for illustration. More generally, the proof applies to constraints of the form $\mathbf{b}'\tilde{\mathbf{M}}\mathbf{b}=c\mathbf{b}'\mathbf{b}$ for arbitrary symmetric $\tilde{\mathbf{M}}$, with pairwise different eigenvalues $\tilde{\lambda}_i$ whereby pairwise difference is required for a proof of the last assertion only. Note also that the limiting cases $|\nu|\to\infty$ correspond to the degenerate case $\mathbf{b}\propto\boldsymbol{\gamma}_{\delta}$ since then $\boldsymbol{\nu}/\nu\to-\mathbf{I}$ (the 'correct' sign can be accommodated by $D$). Equivalently, the difference-equation \ref{ar2} morphs into an identity, up to arbitrary scaling. Another interesting limiting case occurs when the structure of the prediction problem is such that $\mathbf{b}$ approaches one of the eigenvectors $\mathbf{v}_i$ of $\mathbf{M}$, denoted by $\mathbf{b}\to\mathbf{v}_i$. Then $D\boldsymbol{\gamma}_{\delta}=\boldsymbol{\nu}\mathbf{b}\to(2\lambda_i-\nu)\mathbf{v}_i$. If $\boldsymbol{\gamma}_{\delta}$ is a fixed target with complete spectral support, then we conclude that $|D|\to 0$ or, equivalently,  $|\lambda_2|\to\infty$ which would imply $i=1$ or $i=L$ and $|\rho_1|\to\rho_{max}(L)$. %\footnote{The intersection of unit-sphere and holding-time hyperbola is an $L-2\geq 1$ dimensional space and therefore $\lambda_2\to\infty$ implies $|\rho_1|\to\rho_{max}(L)$.}. 
On the other hand, if $\boldsymbol{\gamma}_{\delta}$ is not fixed and is allowed to approach $\mathbf{v}_i$ too,  denoted by  $\boldsymbol{\gamma}_{\delta}\to\mathbf{v}_i$, and if $|D|\not\to 0$ then $D\boldsymbol{\gamma}_{\delta}=\boldsymbol{\nu}\mathbf{b}\to(2\lambda_i-\nu)\mathbf{b}$ so that $\mathbf{b}\propto \boldsymbol{\gamma}_{\delta}$ (degenerate case) and $\rho_1\to\lambda_i$ (note that in this particular singular degenerate case, $\nu$ can remain bounded). % (from the previous limiting case we then infer $|D|\to\infty)$. 
We conclude that if $\mathbf{b}\to\mathbf{v}_i$ then $i\in\{1,L\}$ and $|\rho_1|\to\rho_{max}(L)$ (boundary cases of admissibility) or $\mathbf{b}\propto\boldsymbol{\gamma}_{\delta}\to \mathbf{v}_i$ and $\rho_1\to\lambda_i$ for any $i\in\{1,...,L\}$ (asymptotically singular degenerate case with   incomplete spectral support). The case of incomplete spectral support is now addressed formally in the following corollary. 



\begin{Corollary}\label{incomplete_spec_sup}
Let all regularity assumptions of the previous theorem hold except completeness so that $NZ\subset \{1,...,L\}$ or, stated otherwise, there exists $i_0$ such that $w_{i_0}=0$ in \ref{specdec}. Then:
\begin{enumerate}
\item For $\nu\in \mathbb{R}-\{2\lambda_i|i=1,...,L\}$ the functional form of the SSA-estimate is 
\begin{eqnarray}\label{diff_non_home_singular}
\mathbf{b}(\nu)=D\sum_{i\in NZ} \frac{w_i}{2\lambda_{i}-\nu}\mathbf{v}_{i}
\end{eqnarray}
with corresponding lag-one autocorrelation function (acf) 
\begin{eqnarray}\label{sefrhobnotcomp}
\rho(\nu)=\frac{\sum_{i\in NZ}\frac{\lambda_iw_i^2}{(2\lambda_i-\nu)^2}}{\sum_{i\in NZ}\frac{w_i^2}{(2\lambda_i-\nu)^2}}=:\frac{M_{1}}{M_{2}}
\end{eqnarray}
where $M_{1},M_{2}$ are identified with nominator and denominator in this expression. 

\item Let $\nu=\nu_{i_0}:=2\lambda_{i_0}$ where $i_0\notin NZ$ with adjoined rank-defficient $\boldsymbol{\nu}_{i_0}=2\mathbf{M}-\nu_{i_0}\mathbf{I}$. Consider $\mathbf{b}(\nu_{i_0})$, $\rho(\nu_{i_0})$ and $M_{i_01},M_{i_02}$ as defined in the previous assertion. In this case, the functional form of $\mathbf{b}(\nu_{i_0})$ can be 'spectrally completed' as in 
\begin{eqnarray}\label{b_new_comp}  
\mathbf{b}_{i_0}(\tilde{N}_{i_0}):=\mathbf{b}(\nu_{i_0})+D\tilde{N}_{i_0}\mathbf{v}_{i_0}
\end{eqnarray}
with lag-one acf
\begin{eqnarray}\label{sefrhobcomp}  
\rho_{{i_0}}(\tilde{N}_{i_0})=\frac{M_{i_01}+\lambda_{i_0}\tilde{N}_{i_0}^2}{M_{i_02}+\tilde{N}_{i_0}^2}
\end{eqnarray}
If $i_0$ is such that $0<\rho(\nu_{i_0})=\frac{M_{i_01}}{M_{i_02}}< \rho_1<\lambda_{i_0}$ or $0>\rho(\nu_{i_0})=\frac{M_{i_01}}{M_{i_02}}> \rho_1>\lambda_{i_0}$, then 
\begin{eqnarray}\label{N_comp}
\tilde{N}_{i_0}&=&\pm\sqrt{\frac{\rho_1M_{i_02}-M_{i_01}}{\lambda_{i_0}-\rho_1}}
\end{eqnarray}
ensures compliance with the holding-time constraint i.e. $\rho_{{i_0}}(\tilde{N}_{i_0})=\rho_1$. The 'correct' sign-combination of $D$ and $\tilde{N}_{i_0}$ is determined by the corresponding maximal criterion value.
\item Any $\rho_1$ such that $|\rho_1|<\rho_{max}(L)$ is admissible in the holding-time constraint.
\end{enumerate}
\end{Corollary}
Proof\\

The first assertion follows directly from the Lagrangian equation \ref{diff_non_hom_matrix}
\[
D\boldsymbol{\nu}^{-1}\boldsymbol{\gamma}_{\delta}= \mathbf{b}(\nu)
\]
where $\boldsymbol{\nu}$ has full rank if $\nu\in \mathbb{R}-\{2\lambda_i|i=1,...,L\}$, as assumed. Under the case posited in the second assertion $\boldsymbol{\nu}_{i_0}$ does not have full rank anymore and $\mathbf{b}_{i_0}(\tilde{N}_{i_0})$ as defined by \ref{b_new_comp} is a solution of the Lagrangian equation   
\[
D\boldsymbol{\gamma}_{\delta}= \boldsymbol{\nu}_{i_0}\mathbf{b}_{i_0}(\tilde{N}_{i_0})
\]
for arbitrary $\tilde{N}_{i_0}$ since now $\mathbf{v}_{i_0}$ belongs to the kernel of $\boldsymbol{\nu}_{i_0}$. Moreover, orthogonality of $\mathbf{V}$ implies that 
\begin{eqnarray*}
\rho_{i_0}(\tilde{N}_{i_0}):=\frac{\mathbf{b}_{i_0}(\tilde{N}_{i_0})'\mathbf{M}\mathbf{b}_{i_0}(\tilde{N}_{i_0})}{\mathbf{b}_{i_0}'(\tilde{N}_{i_0})\mathbf{b}_{i_0}(\tilde{N}_{i_0})}&=&\frac{\sum_{i\neq i_0}\lambda_{i}w_i^2\frac{1}{(2\lambda_{i}-\nu)^2}+\tilde{N}_{i_0}^2\lambda_{i_0}}{\sum_{i\neq i_0}w_i^2\frac{1}{(2\lambda_{i}-\nu)^2}+\tilde{N}_{i_0}^2}\nonumber\\
&=&\frac{M_{i_01}+\tilde{N}_{i_0}^2\lambda_{i_0}}{M_{i_02}+\tilde{N}_{i_0}^2}
\end{eqnarray*}
Solving for the holding-time constraint $\rho_{i_0}(\tilde{N}_{i_0})=\rho_1$ then leads to 
\[ 
N_{i_0}:=\tilde{N}_{i_0}^2=\frac{\rho_1M_{i_02}-M_{i_01}}{\lambda_{i_0}-\rho_1}
\]
We infer that  $N_{i_0}$ is always positive if $0<\rho(\nu_{i_0})=\frac{M_{i_01}}{M_{i_02}}< \rho_1<\lambda_{i_0}$ or $0>\rho(\nu_{i_0})=\frac{M_{i_01}}{M_{i_02}}> \rho_1>\lambda_{i_0}$, so that $\tilde{N}_{i_0}=\pm\sqrt{N_{i_0}}\in  \mathbb{R}$, as claimed. Finally, the correct sign combination of the pair $D,\tilde{N}_{i_0}$ is determined by the maximal criterion value. \\
For a proof of the third and last assertion we first assume that $\boldsymbol{\gamma}_{\delta}$ is not band-limited so that $w_1\neq 0$ and $w_L\neq 0$. Then, $\lim_{\nu\to 2\lambda_1}\rho(\nu)=\lambda_1=-\rho_{max}(L)$ and $\lim_{\nu\to 2\lambda_L}\rho(\nu)=\lambda_L=\rho_{max}(L)$, see the proof in the theorem. By continuity of $\rho(\nu)$ and by virtue of the intermediate-value theorem we then infer that any $\rho_1$ such that $|\rho_1|<\rho_{max}(L)$ is admissible for the holding-time constraint. Otherwise, if $w_1=0$ then $\mathbf{b}_{1}(\tilde{N}_{1})$, where $i_0=1$ in \ref{b_new_comp}, can 'fill the gap' and reach out the lower boundary $-\rho_{max}(L)$ as $\tilde{N}_{1}\to\infty$. A similar reasoning would apply in the case $w_L=0$ which achieves the proof of the corollary.\\ 
<<label=init,echo=FALSE,results=hide>>=
# This piece of code demonstrates that
#   1. if target gammak is eigenvector of M then b \propto gammak i.e. rho0 must be corresponding eigenvalue of M (one cannot find solution bk such that rho(b,b,1)\neq rho0 eigenvalue of M)
#   2. if target is almost eigenvector then
#   2.1. lag-one autocorrelation rho(y(nu),y(nu),1) is monotonous in nu if |nu|>2*rho_max (rho_max=max eigenvalue of M)
#   2.2  The range of possible rho0=rho(y(nu),y(nu),1) is very limited if |nu|>2*rho_max
#   2.3 lag-one autocorrelation rho(y(nu),y(nu),1) is not-monotonous in nu if |nu|<2*rho_max (rho_max=max eigenvalue of M)
#   2.3.1 rho has several dips and a peak (number dips depends on length L of filter)
#     -The lowest dip is achieved at nu=-2*rho_max and the peak is obtained at nu=+2*rho_max
#     -lowest dip and peak correspond to +/-rho_max
#   2.3.2 Recall closed-form solution in terms of palindromic polynomials...
# Important note/Remark: This problem (namely that the range of possible rho0 is very limited when |nu|>2*rho_max i.e. in monotonous region so that solution is unique) is generally not relevant, from a prtactical point of view, because most often gamma_k is not cyclical unit-root (or close to eigenvector of M) i.e. weights generally decay fast and often monotonically. In this case the range of possible rho0=rho(y(nu),y(nu),1) is quite large for $|nu|>2*rho_max$ (where solution is then unique).
#   3. Same as 2 above but with gammak=0.k^k (AR(1)-target)
#   3.1 For |nu|>2*rho_max the range of rho0=rho(y(nu),y(nu),1) now extends from rho of target (at minimum) down to rho_max i.e. ALL PRACTICALLY SETTINGS!!!!!!!!!!!!!!!
#    3.2 For |nu|<2*rho_max (unit-root cases) rho0=rho(y(nu),y(nu),1) is nomore montonous (trend with oscillation) and all other rho0<rho(target) are obtained (generally   not PRACTICALLY RELEVANT  (less smooth))
#--------------
# Let's start:
  forecast_horizon<-0

# See 1. above
  len<-L<-10
  set.seed(1)
  
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  
  # Select gammak as second eigenvector of M
  gammak<-gammak_generic<-eigen(M)$vector[,2]
  
  eig<-eigen(M)
  k<-3
  ts.plot(eig$vectors[,k])
  
  nu<-rnorm(1)
  nu<-2
  Nu<-2*M-nu*diag(rep(1,L))
  
  # gammak is also eigenvector of Nu^{-1}
  solve(Nu)%*%gammak/gammak
  
  # b is proportional to gammak
  b<-solve(Nu)%*%gammak
  b/gammak
  # b is eigenvector of Nu and M
  Nu%*%b/b
  M%*%b/b
  M%*%gammak/gammak
  
  eigen(M)$vector[,2]/b
  
  # Check eq-diff
  Nu%*%b-gammak
  
  #-----------------------------------------------------------------
  # See 2. above : Slightly perturbate gammak_generic: almost 2.eigenvector of M
  gammak<-gammak_generic<-eigen(M)$vector[,3]
#  gammak_generic[1]<-gammak_generic[1]+1.e-1
  k_component<-1
  gammak_generic[k_component]<-gammak_generic[k_component]+1.e-3#*rnorm(length(k_component))
  rho0<-0.9
  # See 2.1 above
  # Compute lag-one acf of b for nu>2
  grid_size<-10000
  # Include negative lambda1: yes/no  
  with_negative_lambda<-F
  # Target gammak_generic is univariate AR(1) as determined above   
  opt_obj<-find_lambda1_subject_to_holding_time_constraint_func(grid_size,L,gammak_generic,rho0,forecast_horizon,with_negative_lambda)
  
  rho_mat<-opt_obj$rho_mat
  nu_vec1<-opt_obj$nu_vec
  corb1<-rho_mat[,"rho_yy_best"]
  # See 2.2 above
  # rho is nearly constant i.e. |nu|>2 can address only a very small portion of all lag-one autocorrelations 
  # Note also that rho is a monotonous function as shown in paper
  ts.plot(cbind(rho_mat[,"rho_yy_best"]),col=c("blue","red","green"),main="lag-one autocorrelation rho(y,y,1)")
  
  # See 2.3 above
  # Now we look at |nu|<2rho_max: 
  # Now all values between -rho_max and +rho_max will be achieved (although grid_anz must be large for convergence)
  grid_anz<-grid_size/2
  grid_f<-c(-(grid_anz:1),1:grid_anz)
  corb2<-NULL
  maxrho<-max(eigen(M)$values)
  nu_vec2<-NULL
  for (i in grid_f)#i<-(-831) 
  {
  # We use (2*i+0.5)/(grid_anz) instead of 2*i/(grid_anz) because integer*maxrho will lead to singular Nu  
    nu<-(2*i+0.5)/(grid_anz)*maxrho
    nu_vec2<-c(nu_vec2,nu)
    Nu<-2*M-nu*diag(rep(1,L))
    bk_new<-solve(Nu)%*%gammak_generic[1:L]
    corb2<-c(corb2,t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new))
  #  if (t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new)<(-0.3))
  #  {  
  #    print(i)
  #    ts.plot(cbind(gammak_generic,bk_new),col=c("red","blue"))
  #  }
    
  }
  
  # See 2.3.1 above
  # This is a very interesting plot for lag-one autocorrelation rho
  #   -rho is no more monotonous
  #   -8 very narrow dips (depends on how close gamma_generic is to eigenvector) and one peak
  #   --rho_max is achieved at left dip and +rho_max is achieved at right peak
  ts.plot(corb2)#ts.plot(cbind(corbh,corb2),col=c("red","blue"))
  # If grid_anz is large (for example 10^5) then min/max converge towards min/max eigenvalues of M
  max(corb2)
  min(corb2)
  max(eigen(M)$values)
  mat_M<-cbind(corb1,nu_vec1,corb2,nu_vec2)
  ts.plot(mat_M[,3])
  ts.plot(mat_M[,4])
  
#-------------------- -------------------
# See 2. above : eigenvector of M but perturbation larger than above
  gammak<-gammak_generic<-eigen(M)$vector[,2]
  k_component<-1
  gammak_generic[k_component]<-gammak_generic[k_component]+1.e-1
  rho0<-0.9
  # See 2.1 above
  # Compute lag-one acf of b for nu>2
#  grid_size<-100000
  # Include negative lambda1: yes/no  
  with_negative_lambda<-F
  # Target gammak_generic is univariate AR(1) as determined above   
  opt_obj<-find_lambda1_subject_to_holding_time_constraint_func(grid_size,L,gammak_generic,rho0,forecast_horizon,with_negative_lambda)
  
  rho_mat<-opt_obj$rho_mat
  nu_vec1<-opt_obj$nu_vec
  corb1<-rho_mat[,"rho_yy_best"]
  ts.plot(corb1)
  # See 2.2 above
  # rho is nearly constant i.e. |nu|>2 can address only a very small portion of all lag-one autocorrelations 
  # Note also that rho is a monotonous function as shown in paper
  ts.plot(cbind(rho_mat[,"rho_yy_best"]),col=c("blue","red","green"),main="lag-one autocorrelation rho(y,y,1)")
  
  # See 2.3 above
  # Now we look at |nu|<2rho_max: 
  # Now all values between -rho_max and +rho_max will be achieved (although grid_anz must be large for convergence)
  grid_anz<-grid_size/2
  grid_f<-c(-(grid_anz:1),1:grid_anz)
  corb2<-NULL
  maxrho<-max(eigen(M)$values)
  nu_vec2<-NULL
  for (i in grid_f)#i<-(-831) 
  {
  # We use (2*i+0.5)/(grid_anz) instead of 2*i/(grid_anz) because integer*maxrho will lead to singular Nu  
    nu<-(2*i+0.5)/(grid_anz)*maxrho
    nu_vec2<-c(nu_vec2,nu)
    Nu<-2*M-nu*diag(rep(1,L))
    bk_new<-solve(Nu)%*%gammak_generic[1:L]
    corb2<-c(corb2,t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new))
  #  if (t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new)<(-0.3))
  #  {  
  #    print(i)
  #    ts.plot(cbind(gammak_generic,bk_new),col=c("red","blue"))
  #  }
    
  }
  
  # See 2.3.1 above
  # This is a very interesting plot for lag-one autocorrelation rho
  #   -rho is no more monotonous
  #   -8 very narrow dips (depends on how close gamma_generic is to eigenvector) and one peak
  #   --rho_max is achieved at left dip and +rho_max is achieved at right peak
  ts.plot(corb2)
  # If grid_anz is large (for example 10^5) then min/max converge towards min/max eigenvalues of M
  max(corb2)
  min(corb2)
  max(eigen(M)$values)
  mat_M_l<-cbind(corb1,nu_vec1,corb2,nu_vec2)
  ts.plot(mat_M_l[,3])
  ts.plot(mat_M_l[,4])

  #-------------------------------------------------------------------------------
  # See 3 above: Same as 2 but gammak is AR(1) 
  
  gammak_generic<-0.6^(1:L)
  
  # See 3.1 above
  # Compute lag-one acf of b for nu>2
#  grid_size<-10000
  # Include negative lambda1: yes/no  
  with_negative_lambda<-F
  # Target gammak_generic is univariate AR(1) as determined above   
  opt_obj<-find_lambda1_subject_to_holding_time_constraint_func(grid_size,L,gammak_generic,rho0,forecast_horizon,with_negative_lambda)
  
  rho_mat<-opt_obj$rho_mat
  nu_vec1<-opt_obj$nu_vec
  corb1<-rho_mat[,"rho_yy_best"]
  # rho is monotonous (see proof in paper) and range extends from 0.6 (i.e. target) down to rho_max for |nu|>2 can address only a very small portion of all lag-one autocorrelations 
  # Note also that rho is a monotonous function as shown in paper
  ts.plot(cbind(rho_mat[,"rho_yy_best"]),col=c("blue","red","green"),main="lag-one autocorrelation rho(y,y,1)")
  
  # See 3.2 above
  # Now we look at |nu|<2rho_max: 
  # Now all values between -rho_max and +rho_max will be achieved (although grid_anz must be large for convergence)
  grid_anz<-grid_size/2
  grid_f<-c(-(grid_anz:1),1:grid_anz)
  corb2<-NULL
  maxrho<-max(eigen(M)$values)
  nu_vec2<-NULL
  for (i in grid_f)#i<-(-831) 
  {
  # We use (2*i+0.5)/(grid_anz) instead of 2*i/(grid_anz) because integer*maxrho will lead to singular Nu  
    nu<-(2*i+0.5)/(grid_anz)*maxrho
    nu_vec2<-c(nu_vec2,nu)
    Nu<-2*M-nu*diag(rep(1,L))
    bk_new<-solve(Nu)%*%gammak_generic[1:L]
    corb2<-c(corb2,t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new))
  #  if (t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new)<(-0.3))
  #  {  
  #    print(i)
  #    ts.plot(cbind(gammak_generic,bk_new),col=c("red","blue"))
  #  }
    
  }
  
  # This is a very interesting plot for lag-one autocorrelation rho
  #   -rho is no more monotonous
  #   -trend with damped cycle
  #   --rho_max is achieved at left dip and +rho_max is achieved at right peak
  ts.plot(corb2)
  ts.plot(corb1)
  length(corb2)
  length(nu_vec2)
  # If grid_anz is large (for example 10^5) then min/max converge towards min/max eigenvalues of M
  max(corb2)
  min(corb2)
  max(eigen(M)$values)
  mat_ar1<-cbind(corb1,nu_vec1,corb2,nu_vec2)
  ts.plot(mat_M[,1])
  ts.plot(mat_ar1[,1])

    #-------------------------------------------------------------------------------
# See 3 above: Same as 3 but gammak is AR(1) with near unit-root 
  
  gammak_generic<-0.99^(1:L)
  
  # See 3.1 above
  # Compute lag-one acf of b for nu>2
#  grid_size<-10000
  # Include negative lambda1: yes/no  
  with_negative_lambda<-F
  # Target gammak_generic is univariate AR(1) as determined above   
  opt_obj<-find_lambda1_subject_to_holding_time_constraint_func(grid_size,L,gammak_generic,rho0,forecast_horizon,with_negative_lambda)
  
  rho_mat<-opt_obj$rho_mat
  nu_vec1<-opt_obj$nu_vec
  corb1<-rho_mat[,"rho_yy_best"]
  # rho is monotonous (see proof in paper) and range extends from 0.6 (i.e. target) down to rho_max for |nu|>2 can address only a very small portion of all lag-one autocorrelations 
  # Note also that rho is a monotonous function as shown in paper
  ts.plot(cbind(rho_mat[,"rho_yy_best"]),col=c("blue","red","green"),main="lag-one autocorrelation rho(y,y,1)")
  
  # See 3.2 above
  # Now we look at |nu|<2rho_max: 
  # Now all values between -rho_max and +rho_max will be achieved (although grid_anz must be large for convergence)
  grid_anz<-grid_size/2
  grid_f<-c(-(grid_anz:1),1:grid_anz)
  corb2<-NULL
  maxrho<-max(eigen(M)$values)
  nu_vec2<-NULL
  for (i in grid_f)#i<-(-831) 
  {
  # We use (2*i+0.5)/(grid_anz) instead of 2*i/(grid_anz) because integer*maxrho will lead to singular Nu  
    nu<-(2*i+0.5)/(grid_anz)*maxrho
    nu_vec2<-c(nu_vec2,nu)
    Nu<-2*M-nu*diag(rep(1,L))
    bk_new<-solve(Nu)%*%gammak_generic[1:L]
    corb2<-c(corb2,t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new))
  #  if (t(bk_new)%*%M%*%bk_new/(t(bk_new)%*%bk_new)<(-0.3))
  #  {  
  #    print(i)
  #    ts.plot(cbind(gammak_generic,bk_new),col=c("red","blue"))
  #  }
    
  }
  
  # This is a very interesting plot for lag-one autocorrelation rho
  #   -rho is no more monotonous
  #   -trend with damped cycle
  #   --rho_max is achieved at left dip and +rho_max is achieved at right peak
  ts.plot(corb2)
  ts.plot(corb1)
  length(corb2)
  length(nu_vec2)
  # If grid_anz is large (for example 10^5) then min/max converge towards min/max eigenvalues of M
  max(corb2)
  min(corb2)
  max(eigen(M)$values)
  mat_ar1_rw<-cbind(corb1,nu_vec1,corb2,nu_vec2)
  ts.plot(mat_M[,1])
  ts.plot(mat_ar1_rw[,1])
  
  
# Generate pdfs  
    file<-"rho_nu_ar1_ev.pdf"
    pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
    par(mfrow=c(4,2)) 

    anz<-500
#    plot(x=log(log(mat_M[,2])),y=mat_M[,1],xlab="Nu",ylab="rho(1)",main="2. EV: rho(1) for |nu|>2rho_max",ylim=c(min(mat_M[,1])-0.1,1),type="l",col="red")
    plot(x=mat_M[,4],y=mat_M[,3],xlab="nu",ylab="rho(1)",main="2. EV small delta:: |nu|<2rho_max",ylim=c(-1,1),type="l",col="blue")
        plot(x=mat_M[nrow(mat_M):(nrow(mat_M)-anz),2],y=mat_M[nrow(mat_M):(nrow(mat_M)-anz),1],xlab="nu",ylab="rho(1)",main="2. EV small delta:: |nu|>2rho_max",ylim=c(min(mat_M[nrow(mat_M):(nrow(mat_M)-anz),1])-0.1,1),type="l",col="red")

    anz<-2000
    plot(x=mat_M_l[,4],y=mat_M_l[,3],xlab="nu",ylab="rho(1)",main="EV larger delta: |nu|<2rho_max",ylim=c(-1,1),type="l",col="blue")
    plot(x=(mat_M_l[nrow(mat_M_l)-1:anz,2]),y=mat_M_l[nrow(mat_M_l)-1:anz,1],xlab="nu",ylab="rho(1)",main="2. EV larger delta: |nu|>2rho_max",ylim=c(min(mat_M_l[1:anz,1])-0.1,1),type="l",col="red")
    anz<-10000
    plot(x=mat_ar1[,4],y=mat_ar1[,3],xlab="nu",ylab="rho(1)",main="AR(1),a1=0.6: |nu|<2rho_max",ylim=c(-1,1),type="l",col="blue")
     plot(x=log(mat_ar1[1:anz,2]),y=mat_ar1[1:anz,1],xlab="log(nu)",ylab="rho(1)",main="AR(1),a1=0.6: |nu|>2rho_max",ylim=c(min(mat_ar1[1:anz,1])-0.1,1),type="l",col="red")
   
    plot(x=mat_ar1_rw[,4],y=mat_ar1_rw[,3],xlab="nu",ylab="rho(1)",main="AR(1), a1=0.99: |nu|<2rho_max",ylim=c(-1,1),type="l",col="blue")
    plot(x=log(mat_ar1_rw[1:anz,2]),y=mat_ar1_rw[1:anz,1],xlab="log(nu)",ylab="rho(1)",main="AR(1),a1=0.99: |nu|>2rho_max",ylim=c(min(mat_ar1_rw[1:anz,1])-0.1,1),type="l",col="red")


    invisible(dev.off())
    
    file<-"rho_nu_ar1.pdf"
    pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
    
# Only AR(1) examples    
    par(mfrow=c(2,2)) 


    anz<-10000
    plot(x=mat_ar1[,4],y=mat_ar1[,3],xlab="nu",ylab="rho(1)",main=expression(paste("a=0.6:  |",nu,"|<2*",rho[max],sep="")),ylim=c(-1,1),type="l",col="blue")
        abline(h=0.15,col="green")

     plot(x=log(mat_ar1[1:anz,2]),y=mat_ar1[1:anz,1],xlab="log(nu)",ylab="rho(1)",main=expression(paste("a=0.6:  |",nu,"|>2*",rho[max],sep="")),ylim=c(min(mat_ar1[1:anz,1])-0.1,1),type="l",col="red")
   
    plot(x=mat_ar1_rw[,4],y=mat_ar1_rw[,3],xlab="nu",ylab="rho(1)",main=expression(paste("a=099:  |",nu,"|<2*",rho[max],sep="")),ylim=c(-1,1),type="l",col="blue")
    abline(h=0.15,col="green")
    plot(x=log(mat_ar1_rw[1:anz,2]),y=mat_ar1_rw[1:anz,1],xlab="log(nu)",ylab="rho(1)",main=expression(paste("a=0.6:  |",nu,"|>2*",rho[max],sep="")),ylim=c(min(mat_ar1_rw[1:anz,1])-0.1,1),type="l",col="red")

    invisible(dev.off())





@

The case of incomplete spectral support is illustrated by a fleshed-out example in section \ref{incomplete_support}. In order to simplify exposition, we now assume  that $|\nu|>2$. In fact, $|\nu|\leq 2$ would imply that the solution of the homogeneous difference-equation corresponding to \ref{ar2} 
\begin{eqnarray*}
b_{k+1}-\nu b_k+b_{k-1}&=&0
\end{eqnarray*}
would be subject to a unit-root so that the coefficients would not decay to zero for increasing lag. Since the SSA-solution specified by \ref{ar2} is obtained by a suitable linear combination of solutions of non-homogeneous and homogeneous equations, whereby the latter ensures compliance with the boundary constraints $b_{-1}=b_L=0$ (formal details are skipped), we then infer that its coefficients would not decay to zero either with increasing lag, hence suggesting evidence of an ill-posed prediction problem. Typically, this issue could be addressed by selecting $L$ sufficiently large i.e. at least twice the imposed holding-time, see section \ref{unit_root_case} for illustration of the so-called unit-root case $|\nu|\leq 2$.   







\begin{Corollary}\label{lambda_num_gen}
Let the assumptions of theorem \ref{lambda} hold and assume $|\nu|>2$. Then the solution to the SSA-optimization problem \ref{crit1} is 
\begin{equation}\label{prop_sol_un_unc_fast}
\mathbf{b}(\nu_0)=\textrm{sign}_{\nu_0}\sum_{i=1}^L \frac{w_i}{2\lambda_{i}-\nu_0}\mathbf{v}_{i}
\end{equation}
where $\nu_0$ is the unique solution of the non-linear equation
\begin{eqnarray}\label{uni_unco_min}
\frac{\mathbf{b(\nu_0)}'\mathbf{M}\mathbf{b(\nu_0)}}{\mathbf{b(\nu_0)}'\mathbf{b(\nu_0)}}=\rho_1
\end{eqnarray}
The sign $\textrm{sign}_{\nu_0}=\pm 1$ is selected such that $\mathbf{b(\nu_0)}'\boldsymbol{\gamma}_{\delta}> 0$ (positive criterion value). 
\end{Corollary}



A proof follows readily from assertions \ref{ass1}-\ref{ass4} of theorem \ref{lambda}, noting that $|\nu|>2>2\rho_{max}(L)$. In this case, numerical computations are swift due to strict monotonicity and also because \ref{prop_sol_un_unc_fast} does not rely on a matrix inversion, unlike \ref{diff_non_hom_matrixe}. If $|\nu|\leq 2\rho_{max}(L)$ then strict monotonicity and uniqueness of the solution of \ref{uni_unco_min} are lost, see section \ref{mon_non_mono} for a worked-out example. To conclude, the following corollary derives the distribution of the SSA-predictor.  


\begin{Corollary}
Let all regularity assumptions of theorem \ref{lambda} hold and let $\hat{\boldsymbol{\gamma}}_{\delta}$ be a finite-sample estimate of the MSE-predictor ${\boldsymbol{\gamma}}_{\delta}$ with mean ${\boldsymbol{\mu}}_{\gamma_\delta}$ and variance ${\boldsymbol{\Sigma}}_{\gamma_\delta}$. Then mean and variance of the SSA-predictor $\hat{\mathbf{b}}$ are
\begin{eqnarray*}
{\boldsymbol{\mu}}_{\mathbf{b}}&=&D\boldsymbol{\nu}^{-1}{\boldsymbol{\mu}}_{\gamma_\delta}\\
{\boldsymbol{\Sigma}}_{\mathbf{b}}&=&D^2\boldsymbol{\nu}^{-1}{\boldsymbol{\Sigma}}_{\gamma_\delta}\boldsymbol{\nu}^{-1}
\end{eqnarray*}
If $\hat{\boldsymbol{\gamma}}_{\delta}$ is Gaussian distributed then so is $\hat{\mathbf{b}}$. 
\end{Corollary}
The proof readily follows from \ref{diff_non_home}. Note that mean, variance and (asymptotic) distribution of the MSE-estimate under various assumptions about $x_t$ are derived in standard textbooks, see e.g. Brockwell and Davis (1993).



\section{Solution of the SSA-Problem: Time-Domain}

We here consider the AR(2) difference-equation \ref{ar2} together with the boundary constraints $b_{-1}=b_L=0$ as determinants of the SSA-predictor in the time-domain. In this context, the  roots of the characteristic AR(2)-polynomial of \ref{ar2} are denoted by $\lambda_{1\rho_1}$ and $\lambda_{2\rho_1}$. If $|\nu|<2$ then the roots are complex conjugate and $\nu=2\Re(\lambda_{1\rho_1})$. If $|\nu|\geq 2$ then the roots are real inverse i.e. $\lambda_{2\rho_1}=1/\lambda_{1\rho1}$ and $\nu=\lambda_{1\rho1}+1/\lambda_{1\rho1}$. Section \ref{arma_case} proposes a solution of the SSA-problem for a target $z_t$ following an ARMA-specification; section \ref{ar1closed} derives a closed-form solution of the SSA-predictor in a special case.      



\subsection{ARMA-Target}\label{arma_case}



\begin{Corollary}\label{lambda_num_gen}
Let the regularity assumptions of theorem \ref{lambda} hold, let $|\nu|>2$ in \ref{ar2} and let $\lambda_{1\rho_1}$ designate the (real) stable root in $\nu=\lambda_{1\rho_1}+1/\lambda_{1\rho_1}$. Assume, also, that $\boldsymbol{\gamma}_{\delta}$ follows an ARMA$(p,q)$-specification with AR- and MA-parameters $a_1,...,a_p$ and $b_1,...,b_q$. Then the solution of the SSA-criterion is 
\begin{eqnarray}\label{time_domain_ssa_ARMA_solution}
b_k:=\left\{\bigg(\Big(\mathbf{A}^{k+1+\delta}-\mathbf{C}_1\lambda_{1\rho_1}^{k+1}-\mathbf{C}_2\lambda_{1\rho_1}^{L-k}\Big)(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})^{-1}\bigg)\mathbf{b}\right\}_1
\end{eqnarray}
where 
\begin{eqnarray*}
\mathbf{A}&=&\left(\begin{array}{ccccc}a_1&a_2&...&a_{r-1}&a_r\\1&0&...&0&0\\...&&&&\\0&0&...&1&0\end{array}\right)\\
\mathbf{C}_1&=&\frac{1}{1-\lambda_{1\rho_1}^{2(L+1)}}(\mathbf{A}^{\delta}-\lambda_{1\rho_1}^{L+1}\mathbf{A}^{L+1+\delta})\\
\mathbf{C}_2&=&\frac{1}{1-\lambda_{1\rho_1}^{2(L+1)}}(\mathbf{A}^{L+1+\delta}-\lambda_{1\rho_1}^{L+1}\mathbf{A}^{\delta})
\end{eqnarray*}
$\mathbf{b}'=(1,b_1,...,b_r)$, $r=\max(p,q+1)$ and the notation $\{\cdot\}_1$ in \ref{time_domain_ssa_ARMA_solution} designates the first component of the corresponding vector. 
\end{Corollary}

Proof\\

Consider first the MA-inversion 
\[\gamma_k=\left(\mathbf{A}^k\mathbf{b}\right)_{1}\]
of the ARMA-process $z_t$, where $\mathbf{A}=\left(\begin{array}{ccccc}a_1&a_2&...&a_{r-1}&a_r\\1&0&...&0&0\\...&&&&\\0&0&...&1&0\end{array}\right)$, $\mathbf{b}'=(1,b_1,...,b_r)$ and $r=\max(p,q+1)$. Since
\begin{eqnarray*}
\mathbf{A}^{k+2+\delta}-\nu\mathbf{A}^{k+1+\delta}+\mathbf{A}^{k+\delta}=\mathbf{A}^{k+\delta}(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})
\end{eqnarray*}
we deduce that $b_k':=\left\{\bigg(\mathbf{A}^{k+1+\delta}(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})^{-1}\bigg)\mathbf{b}\right\}_1$ must be a solution of the AR(2) difference equation \ref{ar2}, up to arbitrary scaling. Note that $\nu$ must be such that $\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I}$ has full rank, by virtue of the theorem, assuming all regularity assumptions to hold. If $|\nu|>2$ then the roots of the characteristic AR(2) polynomial \ref{ar2} are real inverse numbers i.e. $\nu=\lambda_{1\rho_1}+1/\lambda_{1\rho_1}$ where $\lambda_{1\rho_1}\in ]-1,1[/\{0\}$\footnote{The singular case $\lambda_{1\rho_1}=0$ would correspond to the degenerate case $\mathbf{b}\propto\boldsymbol{\gamma}_{\delta}$ i.e. the SSA-estimate is also the MSA-estimate, which is excluded by the theorem.} designates the stable root. Moreover, $\mathbf{\tilde{C}}(k):=\mathbf{\tilde{C}}_1\lambda_{1\rho_1}^{k+1}+\mathbf{\tilde{C}}_2\lambda_{1\rho_1}^{L-k}$, where $\mathbf{\tilde{C}}_1,\mathbf{\tilde{C}}_2$ are arbitrary matrices of dimension $r$, is a solution of the homogeneous difference equation
\begin{eqnarray*}
\mathbf{\tilde{C}}(k+1)-\nu\mathbf{\tilde{C}}(k)+\mathbf{\tilde{C}}(k-1)=\mathbf{0}
\end{eqnarray*}
by definition of $\lambda_{1\rho_1}$. Therefore, 
\[b_k:=\left\{\bigg(\mathbf{A}^{k+1+\delta}(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})^{-1}-\mathbf{\tilde{C}}(k)\bigg)\mathbf{b}\right\}_1\]
is also a solution of \ref{ar2}. We can now select $\mathbf{\tilde{C}}_1$ and $\mathbf{\tilde{C}}_2$ in $\mathbf{\tilde{C}}(k)$ such that the boundary constraints $b_{-1}=b_{L}=0$ hold. Specifically, let $\mathbf{C}_i:=\mathbf{\tilde{C}}_i(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})$, $i=1,2$ so that
\begin{eqnarray}\label{time_domain_ssa_ARMA}
b_k=\left\{\bigg(\Big(\mathbf{A}^{k+1+\delta}-\mathbf{C}_1\lambda_{1\rho_1}^{k+1}-\mathbf{C}_2\lambda_{1\rho_1}^{L-k}\Big)(\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I})^{-1}\bigg)\mathbf{b}\right\}_1
\end{eqnarray}
We now determine $\mathbf{C}_1,\mathbf{C}_2$ according to the boundary constraints at lags $k=-1$ and $k=L$ by
\begin{eqnarray*}
\mathbf{A}^{\delta}-\mathbf{C}_1-\mathbf{C}_2\lambda_{1\rho_1}^{L+1}&=&\mathbf{0}\\
\mathbf{A}^{L+1+\delta}-\mathbf{C}_1\lambda_{1\rho_1}^{L+1}-\mathbf{C}_2&=&\mathbf{0}
\end{eqnarray*}
Solving for $\mathbf{C}_1,\mathbf{C}_2$ leads to
\begin{eqnarray*}
\mathbf{C}_1&=&\frac{1}{1-\lambda_{1\rho_1}^{2(L+1)}}(\mathbf{A}^{\delta}-\lambda_{1\rho_1}^{L+1}\mathbf{A}^{L+1+\delta})\\
\mathbf{C}_2&=&\frac{1}{1-\lambda_{1\rho_1}^{2(L+1)}}(\mathbf{A}^{L+1+\delta}-\lambda_{1\rho_1}^{L+1}\mathbf{A}^{\delta})
\end{eqnarray*}
Inserting these expressions into \ref{time_domain_ssa_ARMA} implies that the resulting $b_k$ must be a solution of \ref{ar2} satisfying the implicit boundary constraints $b_{-1}=b_L=0$ and therefore it must be the SSA-solution, up to arbitrary scaling, by virtue of theorem \ref{lambda}.\\


\textbf{Remarks}\\
The theorem assumes that $\boldsymbol{\gamma}_{\delta}$ has complete spectral support. This is always the case for a stationary invertible ARMA-process whose spectral density is strictly positive. But non-invertible processes are allowed, too, assuming that the spectral density does not vanish at a frequency corresponding to an eigenvector $\mathbf{v}_i$, $i=1,...,L$, of $\mathbf{M}$: in this case, all spectral weights $w_i$, $i=1,...,L$ in \ref{specdec} are non-vanishing, as assumed. Note also that theorem \ref{lambda} asserts existence of the SSA-solution in the functional form derived in the above corollary and therefore  $\nu$ must be such that $\mathbf{A}^2-\nu\mathbf{A}+\mathbf{I}$ has full rank in the above derivation. Finally, in typical applications the unstable root $1/\lambda_{1\rho_1}$ or, equivalently, $\lambda_{1\rho_1}^{L-k}$ in \ref{time_domain_ssa_ARMA} can be neglected because $\mathbf{C}_2=\frac{1}{1-\lambda_{1\rho_1}^{2(L+1)}}(\mathbf{A}^{L+1+\delta}-\lambda_{1\rho_1}^{L+1}\mathbf{A}^{\delta})$ asymptotically vanishes for $L$ sufficiently large. The unit-root case $|\nu|\leq 2$ would behave differently: in particular $b_k$ would not decay to zero anymore with increasing lag $k$, suggesting an ill-posed or 'atypical' prediction problem.  




\subsection{Special Case AR(1)-Target and a Closed-Form Solution}\label{ar1closed}

We now suggest that the unknown parameter $\lambda_{1\rho_1}$ and therefore $\nu=\lambda_{1\rho_1}+1/\lambda_{1\rho_1}$ can be obtained in closed-form, without numerical optimization, under certain conditions. To simplify exposition we fix attention to a particular target $z_t$, namely an AR(1)-process, with weights $\gamma_k=\lambda^k$. Extensions to an AR(2)-target are available but for ease of exposition we here omit a corresponding more convoluted derivation, especially since the basic proceeding would remain the same.


\begin{Corollary}\label{lambda_cor_gen_case}
Let the following assumptions hold in addition to the set of regularity conditions of theorem \ref{lambda}:
\begin{enumerate}
\item The MSE-estimate $\boldsymbol{\gamma}_{\delta}$ corresponds to a stationary AR(1) i.e. $\gamma_k\propto \lambda^k$, $k=0,...,L-1$ with stable root $\lambda\neq 0$ (exponential decay)
%\item $\lambda_i\neq\lambda_{1\rho_1}$ : regular case of corollary \ref{lambda_cor} (vs. non-degenerate singular case  in corollary \ref{lambda_cor_sing})
\item $|\nu|>2$  (typical 'non unit-root' case)
\item $\lambda_{1\rho_1}, \lambda$ and $L$ are such that $\max(|\lambda_{1\rho_1}|^{2k},|\lambda|^{2k})$ is negligible for $k>L$ (sufficiently fast decaying filter-weights).
\end{enumerate}
Then the optimal invertible root $\lambda_{1\rho_1}$ in $\nu=\lambda_{1\rho_1}+1/\lambda_{1\rho_1}$ is given by (the real-valued) 
\begin{eqnarray}\label{closedformlambdarho1}
\lambda_{1\rho_1}&=&-\frac{1}{3c_3}\left(c_2+C+\frac{\Delta_0}{C}\right)
\end{eqnarray}
where
\begin{eqnarray}
C&=&\sqrt[3]{\frac{\Delta_1+\textrm{sign}(\Delta_1) \sqrt{\Delta_1^2-4\Delta_0^3}}{2}}\label{C3}\\
\Delta_0&=&c_2^2-3c_3c_1\nonumber\\
\Delta_1&=&2c_2^3-9c_3c_2c_1+27c_3^2c_0\nonumber
\end{eqnarray}
and where $c_3,c_2,c_1,c_0$ are the coefficients of a cubic polynomial which depend on the AR(1)-target specified by $\lambda$, the forecast horizon $\delta$ and the holding-time constraint $\rho_1$ according to   
\begin{eqnarray*}
c_3&=&\lambda^{2\delta-2}-\lambda^{2\delta+2}+\lambda^{2+\delta}-\rho_1\lambda^{2\delta-1}\\
c_2&=&-(\lambda^{1+\delta}+\lambda^{2\delta-1}(1-\lambda^2))-\rho_1\Big(-2\lambda^{2\delta}+\lambda^{2\delta-2}\Big)\\
c_1&=&-(\lambda^{2+\delta}+\lambda^{2\delta}(1-\lambda^2))-\rho_1\Big(\lambda^{2\delta+1}-2\lambda^{2\delta-1}\Big)\\
c_0&=&\lambda^{1+\delta}-\rho_1\lambda^{2\delta}
\end{eqnarray*}
The SZC-estimate $\mathbf{b}$ is then uniquely determined in closed-form by \ref{diff_non_home}, down to the correct sign which leads to a positive  criterion value $\mathbf{b}'\boldsymbol{\gamma}_{\delta}\geq 0$. 
\end{Corollary}


Proof\\


Let $\gamma_{k+\delta}=\lambda^{k+\delta}$. Then 
\begin{equation}\label{ar1sc}
b_k':=D\frac{\lambda^{\delta}}{\lambda^2-\nu\lambda+1} \lambda^{k+1}\propto \lambda^{k+\delta}
\end{equation}
is a solution of 
\begin{eqnarray*}
b_{k+1}'-\nu b_k'+b_{k_1}'&=&D\gamma_{k+\delta}~,~0\leq k\leq L-1\\
\end{eqnarray*}
with boundaries $b_{-1},b_L\neq 0$. The expression is well-defined because $\lambda^2-\nu\lambda+1\neq 0$ when $\lambda\neq \lambda_{1\rho_1}$, which is always the case by virtue of theorem \ref{lambda}. According to corollary \ref{lambda_num_gen} vanishing boundaries $b_{-1}=b_L=0$ can be imposed by combining $b_k'$ in \ref{ar1sc} with a suitably scaled solution of the homogeneous difference-equation: 
\begin{eqnarray}\label{sol_bk}
b_k=b_k(\lambda_{1\rho_1)}\propto\lambda^{k+\delta}+C_1\lambda_{1\rho_1}^k+C_2\lambda_{1\rho_1}^{L-k}\approx\lambda^{k+\delta}-\lambda_{1\rho_1}\lambda^{-1+\delta}\lambda_{1\rho_1}^k
\end{eqnarray}
where we neglected the backward-solution ($b_{L}\approx 0$ by the last assumption) and where the weight $C_1=-\lambda_{1\rho_1}\lambda^{-1+\delta}$ ensures compliance with the remaining constraint $b_{-1}=0$. Corollary \ref{lambda_num_gen} states also that the unknown stable root $\lambda_{1\rho_1}$ is determined uniquely by requiring  
\begin{eqnarray}\label{sdfret}
\frac{\sum_{k=1}^{L-1} b_k(\lambda_{1\rho_1})b_{k-1}(\lambda_{1\rho_1})}{\sum_{k=0}^{L-1}b_k(\lambda_{1\rho_1})^2}=\rho_1
\end{eqnarray}
We can now insert \ref{sol_bk} into this equation and solve for $\lambda_{1\rho_1}$. Specifically, the nominator  becomes 
\begin{eqnarray}\label{cte1before} 
\sum_{k=1}^{L-1} b_kb_{k-1}&=&\sum_{k=1}^{L-1}\left(\lambda^{k+\delta}-\lambda_{1\rho_1}\lambda^{-1+\delta}\lambda_{1\rho_1}^k\right)\left(\lambda^{k-1+\delta}-\lambda_{1\rho_1}\lambda^{-1+\delta}\lambda_{1\rho_1}^{k-1}\right)
\end{eqnarray}
The first cross-product of terms in parantheses is
\begin{eqnarray}
\lambda^{-1}\sum_{k=1}^{L-1}\lambda^{2(k+\delta)}=\lambda^{1+2\delta}\sum_{k=0}^{L-2}\lambda^{2k}=\lambda^{1+2\delta}\frac{1-\lambda^{2(L-1)}}{1-\lambda^2}\approx \frac{\lambda^{1+2\delta}}{1-\lambda^2}\label{cte1}
\end{eqnarray}
The second cross-product of terms in parentheses is
\begin{eqnarray}
-\lambda_{1\rho_1}\lambda^{-1+\delta}\sum_{k=1}^{L-1}\lambda^{k+\delta}\lambda_{1\rho_1}^{k-1}=-\lambda_{1\rho_1}\lambda^{2\delta}\sum_{k=0}^{L-2}(\lambda\lambda_{1\rho_1})^{k}=-\lambda_{1\rho_1}\lambda^{2\delta}\frac{1-(\lambda\lambda_{1\rho_1})^{L-1}}{1-\lambda\lambda_{1\rho_1}}\approx \frac{-\lambda_{1\rho_1}\lambda^{2\delta}}{1-\lambda\lambda_{1\rho_1}}\label{cte2}
\end{eqnarray}
The third cross-product of terms in parentheses is
\begin{eqnarray}
-\lambda_{1\rho_1}\lambda^{-1+\delta}\sum_{k=1}^{L-1}\lambda^{k-1+\delta}\lambda_{1\rho_1}^{k}\approx \frac{-\lambda_{1\rho_1}^2\lambda^{2\delta-1}}{1-\lambda\lambda_{1\rho_1}}\label{cte3}
\end{eqnarray}
The last cross-product of terms in parantheses is
\begin{eqnarray}
\lambda_{1\rho_1}^2\lambda^{-2+2\delta}\sum_{k=1}^{L-1}\lambda_{1\rho_1}^{2k-1}=\lambda_{1\rho_1}^3\lambda^{-2+2\delta}\sum_{k=0}^{L-2}\lambda_{1\rho_1}^{2k}=\lambda_{1\rho_1}^3\lambda^{-2+2\delta}\frac{1-\lambda_{1\rho_1}^{2(L-1)}}{1-\lambda_{1\rho_1}^2}
\approx \frac{\lambda_{1\rho_1}^3\lambda^{-2+2\delta}}{1-\lambda_{1\rho_1}^2}\label{cte4}
\end{eqnarray}
Note that the last assumption of the corollary is critical for the validation of the above approximations. Further, the common denominator of \ref{cte1}, \ref{cte2}, \ref{cte3} and \ref{cte4} is 
\begin{eqnarray}\label{comden}
(1-\lambda^2)(1-\lambda\lambda_{1\rho_1})(1-\lambda_{1\rho_1}^2)
\end{eqnarray}
Summing all terms in \ref{cte1}, \ref{cte2}, \ref{cte3} and \ref{cte4} under the common denominator \ref{comden} leads to a third-order polynomial 
\[
f_1(\lambda_{1\rho_1}):=a_3\lambda_{1\rho_1}^3+a_2\lambda_{1\rho_1}^2+a_1\lambda_{1\rho_1}+a_0
\]
in $\lambda_{1\rho_1}$ with coefficients 
\begin{eqnarray*}
a_3&=&(1-\lambda^2)\lambda^{2\delta-2}(\lambda^2+1)+\lambda^{2+\delta}=\lambda^{2\delta-2}-\lambda^{2\delta+2}+\lambda^{2+\delta}\\
a_2&=&-(\lambda^{1+\delta}+\lambda^{2\delta-1}(1-\lambda^2))\\
a_1&=&-(\lambda^{2+\delta}+\lambda^{2\delta}(1-\lambda^2))\\
a_0&=&\lambda^{1+\delta}
\end{eqnarray*}
Note that the coefficient of order four vanishes due to the mutual cancellation of cross-terms. The same proceeding can now be applied to the denominator $\sum_{k=0}^{L-1}b_k^2$ in \ref{sdfret}:
\begin{eqnarray}\label{bk^2}
\sum_{k=0}^{L-1} b_k^2&=&\sum_{k=0}^{L-1}\left(\lambda^{k+\delta}-\lambda_{1\rho_1}\lambda^{-1+\delta}\lambda_{1\rho_1}^k\right)^2
\end{eqnarray}
with cross-products of terms in parentheses 
\begin{eqnarray*}
\sum_{k=0}^{L-1}\lambda^{2(k+\delta)}&\approx& \frac{\lambda^{2\delta}}{1-\lambda^2}\\
-2\lambda_{1\rho_1}\lambda^{2\delta-1}\sum_{k=0}^{L-1}(\lambda\lambda_{1\rho_1})^{k}&\approx& -\frac{\lambda_{1\rho_1}\lambda^{2\delta-1}}{1-\lambda\lambda_{1\rho_1}}\\
\lambda_{1\rho_1}^2\lambda^{-2+2\delta}\sum_{k=0}^{L-1}\lambda_{1\rho_1}^{2k}
&\approx& \frac{\lambda_{1\rho_1}^2\lambda^{-2+2\delta}}{1-\lambda_{1\rho_1}^2}
\end{eqnarray*}
This will again lead to the sum of four terms whose common denominator is again \ref{comden} and whose nominator is a polynomial
\[
f_2(\lambda_{1\rho_1}):=b_3\lambda_{1\rho_1}^3+b_2\lambda_{1\rho_1}^2+b_1\lambda_{1\rho_1}+b_0
\]
with polynomial coefficients
\begin{eqnarray*}
b_3&=&\lambda^{2\delta-1}\\
b_2&=&-2\lambda^{2\delta}+\lambda^{2\delta-2}\\
b_1&=&\lambda^{2\delta+1}-2\lambda^{2\delta-1}\\
b_0&=&\lambda^{2\delta}
\end{eqnarray*}
Note that fourth-order terms do not appear in this case. After cancellation of their common denominator \ref{comden}, equation \ref{sdfret} can then be re-written as
\[%begin{equation}\label{lrhds}
\frac{f_1(\lambda_{1\rho_1})}{f_2(\lambda_{1\rho_1})}=\rho_1
\]%end{equation}
or 
\begin{equation}\label{sol_quart}
f_3(\lambda_{1\rho_1},\rho_1)=0
\end{equation}
where $f_3(\lambda_{1\rho_1},\rho_1):=f_1(\lambda_{1\rho_1})-\rho_1f_2(\lambda_{1\rho_1})$ is the asserted cubic polynomial in $\lambda_{1\rho_1}$ with coefficients
\begin{eqnarray*}
c_3=a_3-\rho_1b_3~,~c_2=a_2-\rho_1b_2~,~c_1=a_1-\rho_1b_1~,~c_2=a_0-\rho_1b_0
\end{eqnarray*}
The remainder of the proof then follows from a closed-form expression for the root of a cubic polynomial\footnote{Under particular circumstances the leading polynomial coefficient can vanish, $c_3=0$, so that the solution for $\lambda_{1\rho_1}$ simplifies to finding the root of a quadratic polynomial. }, see e.g. Cardano's formula.



\section{An Illustration of Technical Features}\label{examples}

Our examples in this section address specific methodological features of the SSA-predictor: a simple introductory forecast exercise is proposed in section \ref{one_step_fore}; elements of signal extraction are examined in section \ref{example_autocor}; %, see section \ref{ext_stat}
;  smoothing and 'un-smoothing' nowcasters are proposed  in section \ref{smooth_unsmooth}; a filtering perspective is offered in section \ref{conv_amp} with frequency-domain convolution, plancherel-identity and amplitude functions; a unit-root case is discussed in section \ref{unit_root_case}; resilience against departures from Gaussianity is explored in section \ref{resil}; section \ref{time_smooth} presents a  smoothness-timeliness dilemma and a discussion of SSA hyper-parameters; section \ref{mon_non_mono} highlights multiplicity and uniqueness results; finally, the singular case of a target with incomplete spectral support is illustrated in section \ref{incomplete_support}. SSA-solutions are based on corollary \ref{lambda_num_gen}, by search of $\lambda\in G$ where $G\subset]-1,1[-\{0\}$ is a finite set of size 1000 %\footnote{Finer resolutions are rarely useful in typical 'non-pathological' applications.} 
of equidistant grid-points and $\nu:=\lambda+1/\lambda$ is such that $|\nu|>2$, as required\footnote{Note that $\lambda$ and $1/\lambda$ in the parametrization $\nu:=\lambda+1/\lambda$ of $\nu$ correspond to stable and unstable roots of the characteristic AR(2)-polynomial of the difference-equation \ref{ar2} when $|\nu|>2$: otherwise the characteristic roots would be complex conjugate unit-roots. However we here skip  a formal 'time-domain' analysis which would be of no added value in this context.}. The solution $\lambda_0$ or, equivalently $\nu_0$, is such that the absolute error in \ref{uni_unco_min} is minimized on the grid $G$ (an open-source R-package is at disposal for replication, see (insert link to Github)). 

\subsection{Elements of Forecasting}\label{one_step_fore}


<<label=init,echo=FALSE,results=hide>>=

ht1<-round((acos(2/3)/pi)^{-1},3)
ht2<-round((acos(1/3)/pi)^{-1},3)
L<-L_short<-20
L_long<-50
ht_large<-10
rho_tt1<-rho_tt1_1<-2/3
# Mean holding-time MA(1): this will be larger/smaller than 2 depending on sign of ma1-coeff (if MA(1) is used)
ht_short<-1/(2*(0.25-asin(rho_tt1)/(2*pi)))
@
We consider a simple forecast exercise of a MA(2)-process
\[z_t=\epsilon_t+\epsilon_{t-1}+\epsilon_{t-2}\]
where $\gamma_k=1,k=0,1,2$ and with forecast horizon $\delta=1$ (one-step ahead). For comparison purposes we compute three different SSA-predictors $y_{ti},i=1,2,3$ for $z_t$: the first two are of identical length $L=\Sexpr{L}$ with dissimilar holding-times  $ht=$\Sexpr{round(ht_short,2)} and \Sexpr{ht_large}; the third predictor deviates from the second one by selecting $L=\Sexpr{L_long}$; the holding-time of the first predictor matches the lag-one autocorrelation of $z_t$  and is obtained by inserting $\rho(z,z,1)=2/3$ into \ref{ht}. In addition, we also consider the MSE forecast $\hat{z}_{t,1}^{MSE}=\epsilon_t+\epsilon_{t-1}$, as obtained by classic time series analysis, as well as a trivial 'lag-by-one' forecast $\hat{z}_{t,1}^{lag~1}=z_t$, see fig. \ref{filt_coef_example1} (an arbitrary scaling scheme is applied to SSA filters). Predictors based on the 'true' MA(2)-model of $z_t$ are virtually indistinguishable from predictors based on a fitted empirical model, see table \ref{perf_ex2e} below.  
<<label=init,echo=FALSE,results=hide>>=

target<-rep(1,3)
gamma_mse<-gammak_generic<-rep(1,2)
forecast_horizon<-1
L_short<-20
L_long<-50
rho0<-as.double(compute_holding_time_func(target)$rho_ff1)
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
with_negative_lambda<-F
grid_size<-1000

# We can specify either target with forecast horizon 1 or mse with forecast horizon 0, see proposition 3 in IJOF paper
#   -In the case of autocorrelated xt (xi is not NULL) it is often more convenient to fit the effective target with the corresponding forecast horizon delta because the function SSA_func makes automatically all adjustments (convolutions/deconvolutions)
# Here MSE nowcast
delta<-0
gamma_target<-gamma_mse
# White noise input
xi<-NULL
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

# Is the same as target forecast
delta<-forecast_horizon
gamma_target<-target
SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

ht<-ht_second<-10
rho0<-compute_rho_from_ht(ht)$rho

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

# Check holding times
compute_holding_time_func(bk_mat[,1])$ht
compute_holding_time_func(bk_mat[,2])$ht
# Criterion values (correlations)
t(bk_mat)%*%c(gamma_mse,rep(0,L_short-2))/sqrt(apply(bk_mat^2,2,sum)*sum(target^2))

bk_mat<-rbind(bk_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_mat)),ncol=ncol(bk_mat)))

SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat=cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[3]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

mplot<-cbind(bk_mat,c(target,rep(0,L_long-3)),c(gamma_mse,rep(0,L_long-2)))
colnames(mplot)[4:5]<-c("Lag-by-one","MSE")
mplot[,1]<-3*mplot[,1]
mplot[,2]<-0.9*mplot[,2]
mplot[,3]<-1.3*mplot[,3]

bk_wn<-mplot

@

<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_example1.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(1,2))
plot(mplot[,1],main="Predictors",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(0,max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot_short<-mplot[1:10,]
plot(mplot_short[,1],main="",axes=F,type="l",xlab="Lag-structure",ylab="",ylim=c(0,max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot_short)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot_short))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot_short[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot_short)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot_short),labels=-1+1:nrow(mplot_short))
axis(2)
box()


invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_example1"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{MSE-, SSA- and lag-by-one predictors with arbitrarily scaled SSA-designs. All lags (left panel) and first ten lags (right panel).", sep = "")
cat("\\label{filt_coef_example1}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
Except for the MSE (green) all other forecast-filters rely on past $\epsilon_{t-k}$ for $k>q=2$ which are required for compliance with the holding-time constraint (stronger smoothing). For a fixed filter-length $L$, a larger holding-time $ht$ asks for a slower zero-decay of filter coefficients (blue vs. red lines) and for fixed holding-time $ht$, a larger $L$ leads to a faster zero-decay but a long tail of the filter (red vs. violet lines). The distinguishing tips  of the SSA-predictors at lag one in this example are indicative of one of the two implicit boundary constraints, namely $b_{-1}=0$, see theorem \ref{lambda}.  Note that the 'lag-by-one' forecast (black) has the same holding time as the first SSA-filter (blue) so that the latter should outperform the former in terms of sign accuracy or, equivalently, in terms of correlation with the shifted target, as confirmed in table \ref{perf_ex2}.   
<<label=ats_mba_2,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
# Correlations with z_{t+1}
cor_vec<-ht_vec<-proba_vec<-NULL
for (i in 1:ncol(mplot))
{
  cor_vec<-c(cor_vec,  (mplot[1,i]+mplot[2,i])/(sqrt(3)*sqrt(sum(mplot[,i]^2,na.rm=T))))
  ht_vec<-c(ht_vec,compute_holding_time_func(mplot[,i])$ht)
  proba_vec<-c(proba_vec,1-(2*(0.25-asin(cor_vec[length(cor_vec)])/(2*pi))))
}



mat_re<-rbind(cor_vec,ht_vec,proba_vec)
rownames(mat_re)<-c("Correlation with target","Empirical holding-times","Empirical sign accuracy")
colnames(mat_re)[4]<-"Lag-by-one"
mat1<-round(mat_re,3)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(mat1, dec = 1,digits=rep(3,dim(mat_re)[2]+1),
paste("Performances of MSE and lag-by-one  benchmarks vs. SSA: All filters are applied to a sample of length 1000000 of Gaussian noise. Empirical holding-times are obtained by dividing the sample-length by the number of zero-crossings.  "),
label=paste("perf_ex2",sep=""),
center = "centering", file = "", floating = FALSE)
@
MSE outperforms all other forecasts in terms of correlation and sign accuracy  but it loses in terms of  smoothness or holding-time; SSA(\Sexpr{round(ht_first,2)},\Sexpr{delta}) outperforms the lag-by-one benchmark; both SSA(\Sexpr{round(ht_second,2)},\Sexpr{delta}) loose in terms of sign-accuracy but win in terms of smoothness and while the profiles of longer and shorter filters differ in figure \ref{filt_coef_example1}, their respective performances are virtually indistinguishable in table \ref{perf_ex2}, suggesting that the selection of $L$ is not critical (assuming it is at least twice the holding-time). The table also illustrates the tradeoff between MSE- or sign-accuracy performances of optimal designs, in the top and bottom rows, and smoothing-performances in the middle row (an explicit formal link can be obtained but is omitted here). 
<<label=init,echo=FALSE,results=hide>>=
# This is the same code as above but we rely on an estimate of the MSE-target based on a finite sample of zt
set.seed(10)
len<-50
eps<-rnorm(len)
z<-eps[3:len]+eps[2:(len-1)]+eps[1:(len-2)]
acf(z)
arima_obj<-arima(z,order=c(0,0,2))
tsdiag((arima_obj))
target<-c(1,arima_obj$coef[c("ma1","ma2")])
gamma_mse<-arima_obj$coef[c("ma1","ma2")]
if (F)
{ 
# This is the previous setting in the code above  
  target<-rep(1,3)
  gamma_mse<-gammak_generic<-rep(1,2)
}
forecast_horizon<-1
L_short<-20
L_long<-50
rho0<-as.double(compute_holding_time_func(target)$rho_ff1)
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
with_negative_lambda<-F

delta<-0
gamma_target<-gamma_mse
# white-noise input: xi<-NULL
xi<-NULL
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

ht<-ht_second<-10
rho0<-compute_rho_from_ht(ht)$rho

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

# Check holding times
compute_holding_time_func(bk_mat[,1])$ht
compute_holding_time_func(bk_mat[,2])$ht
# Criterion values (correlations)
t(bk_mat)%*%c(gamma_mse,rep(0,L_short-2))/sqrt(apply(bk_mat^2,2,sum)*sum(target^2))

bk_mat<-rbind(bk_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_mat)),ncol=ncol(bk_mat)))

SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

bk_mat=cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[3]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

bk_mat_forecast<-bk_mat
mplot<-cbind(bk_mat,c(target,rep(0,L_long-3)),c(gamma_mse,rep(0,L_long-2)))
colnames(mplot)[4:5]<-c("Lag-by-one","MSE")
mplot[,1]<-3*mplot[,1]
mplot[,2]<-0.9*mplot[,2]
mplot[,3]<-1.3*mplot[,3]

@ 
Finally, table \ref{perf_ex2e} displays results when all predictors rely on an empirical model fitted to $z_t$ on a data-sample of length \Sexpr{len}: a comparison of both tables suggests that performances are virtually unaffected by the additional estimation step. 
<<label=ats_mba_2,echo=FALSE,results=tex>>=
# Correlations with z_{t+1}
cor_vec<-ht_vec<-proba_vec<-NULL
for (i in 1:ncol(mplot))
{
  cor_vec<-c(cor_vec,  (mplot[1,i]+mplot[2,i])/(sqrt(3)*sqrt(sum(mplot[,i]^2,na.rm=T))))
  ht_vec<-c(ht_vec,compute_holding_time_func(mplot[,i])$ht)
  proba_vec<-c(proba_vec,1-(2*(0.25-asin(cor_vec[length(cor_vec)])/(2*pi))))
}



mat_re<-rbind(cor_vec,ht_vec,proba_vec)
rownames(mat_re)<-c("Correlation with target","Empirical holding-times","Empirical sign accuracy")
colnames(mat_re)<-colnames(mplot)
mat1<-round(mat_re,3)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(mat1, dec = 1,digits=rep(3,dim(mat_re)[2]+1),
paste("Same case as above but all predictors rely on an empirical model of the MA(2)-process: the model is fitted on a sample of length 50.  "),
label=paste("perf_ex2e",sep=""),
center = "centering", file = "", floating = FALSE)
@







\subsection{Elements of Signal Extraction}\label{example_autocor}


<<label=init,echo=FALSE,results=hide>>=
# Same example as above but the input xt of target zt is an ARMA-process 
# ARMA-target
ar1<--0.3
ma<-c(0.7,0.8)
# Target zt based on xt: same as above
target<-rep(1,3)
forecast_horizon<-1
L_short<-20
L_long<-50
# Ma-inversion of ARMA-target (use ARMAtoMA function of R)
xi<-NULL
xi<-c(1,ARMAtoMA(ar=ar1,ma=ma,lag.max=L_long-1))

# Alternative: formal derivation of MA-inversion
A_ar<-matrix(rbind(c(ar1,1,0),c(0,0,1),c(0,0,0)),ncol=3)
b_ma<-c(1,ma)
xi_tilde<-NULL
Ak<-diag(rep(1,3))
for (i in 1:L_long)
{
  
  xi_tilde<-c(xi_tilde,(Ak%*%b_ma)[1])
  Ak<-Ak%*%A_ar
}
# Both match perfectly
ts.plot(cbind(xi,xi_tilde))

# MSE and Lag-by-one benchmarks as applied  to white noise i.e. convolution of MA(3)-filter of target and ARMA-filter (would require deconvolution if applied to xt): these are not required for SSA-estimation
gamma_mse<-(c(xi[2:L_long],0)+xi[1:L_long]+c(0,xi[1:(L_long-1)]))/3
gamma_lag_by_one<-(xi[1:L_long]+c(0,xi[1:(L_long-1)])+c(0,0,xi[1:(L_long-2)]))/3
ts.plot(cbind(gamma_mse,gamma_lag_by_one),col=c("green","black"))

# SSA-settings
rho0<-0.6666667
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# In signal extraction cases it is often suitable to allow for negative nu or lambda if xt is strongly autocorrelated and holding-time 'small': then the signal extraction filter must un-smooth xt i.e. negative nu is required
with_negative_lambda<-T
grid_size<-1000

# Estimation
delta<-forecast_horizon
gamma_target<-gammak_generic<-target
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

crit_SSA1<-SSA_obj$crit_rhoyz
bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
bk_white_noise_mat=SSA_obj$bk_white_noise_mat
colnames(bk_mat)<-colnames(bk_white_noise_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
compute_holding_time_func(bk_white_noise_mat[,1])$ht


ht<-ht_second<-10
rho0<-compute_rho_from_ht(ht)$rho

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

crit_SSA2<-SSA_obj$crit_rhoyz
bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
bk_white_noise_mat<-cbind(bk_white_noise_mat,SSA_obj$bk_white_noise_mat)

colnames(bk_mat)[2]<-colnames(bk_white_noise_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")


bk_mat<-rbind(bk_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_mat)),ncol=ncol(bk_mat)))
bk_white_noise_mat<-rbind(bk_white_noise_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_white_noise_mat)),ncol=ncol(bk_white_noise_mat)))

SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

crit_SSA3<-SSA_obj$crit_rhoyz
bk_mat=cbind(bk_mat,SSA_obj$bk_mat)
bk_white_noise_mat<-cbind(bk_white_noise_mat,SSA_obj$bk_white_noise_mat,gamma_lag_by_one,gamma_mse)

ts.plot(bk_white_noise_mat[,4:5],col=c("black","green"))

colnames(bk_mat)[3]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
colnames(bk_white_noise_mat)[3:5]<-c(paste("SSA(",round(ht,2),",",forecast_horizon,")",sep=""),"Lag-by-one","MSE")

# Scale so that plots are comparable
bk_mat[,1]<-bk_mat[,1]/bk_mat[1,1]*bk_wn[1,1]
bk_mat[,2]<-bk_mat[,2]/bk_mat[1,2]*bk_wn[1,2]
bk_mat[,3]<-bk_mat[,3]/bk_mat[1,3]*bk_wn[1,3]
bk_white_noise_mat[,1]<-bk_white_noise_mat[,1]/bk_white_noise_mat[1,1]*bk_wn[1,1]
bk_white_noise_mat[,2]<-bk_white_noise_mat[,2]/bk_white_noise_mat[1,2]*bk_wn[1,2]
bk_white_noise_mat[,3]<-bk_white_noise_mat[,3]/bk_white_noise_mat[1,3]*bk_wn[1,3]
# This will be used in resilience example further down
bk_white_noise_mat_resilience<-bk_white_noise_mat
ts.plot(cbind(bk_white_noise_mat[,1:3],bk_wn[,1:3])[1:20,])

ts.plot(bk_mat)
ijk<-2
b<-NULL
for (i in 1:nrow(bk_mat))
  b<-c(b,bk_mat[1:i,ijk]%*%xi[i:1])
b/bk_white_noise_mat[,ijk]

if (F)
{  
# Check theoretical holding times: holding-times of bk_mat are wrong (bk_mat is applied to ARMA)
  compute_holding_time_func(bk_mat[,1])$ht
  compute_holding_time_func(bk_mat[,2])$ht
  compute_holding_time_func(bk_mat[,3])$ht
}
# Check holding times: holding-times of bk_white_noise_mat are correct (bk_white_noise_mat is applied to noise i.e. convolution of Wold decomposition and bk_mat)
compute_holding_time_func(bk_white_noise_mat[,1])$ht
compute_holding_time_func(bk_white_noise_mat[,2])$ht
compute_holding_time_func(bk_white_noise_mat[,3])$ht

#------------------------------
# Verify that empirical holding-times and criterion values match theoretical values
set.seed(6)
len<-10000
if (F)
  x<-arima.sim(n = len, list(ar =ar1, ma = ma))
# Generate ARMA with MA-inversion
eps<-rnorm(len)
x<-NULL
for (i in L_long:len)
  x<-c(x,xi%*%eps[i:(i-L_long+1)])
# Check that it is indeed an ARMA
x_obj<-arima(x,order=c(1,0,2))
x_obj
tsdiag(x_obj,gof.lag=20)

# Compute target
zt<-NULL
for (i in L_long:(len-L_long))
  zt<-c(zt,x[i:(i-length(gamma_mse)+1)]%*%gamma_mse)
zt<-NULL
for (i in (2*L_long):len)
  zt<-c(zt,eps[-1+i:(i-L_long+1)]%*%gamma_mse)

empirical_ht_arma_vec<-empirical_ht_wn_vec<-empirical_criterion_vec<-NULL
for (i_filter in 1:3)
{ 
  # Generate filter output based on ARMA target
  y_arma<-NULL
  
  for (i in L_long:(len-L_long))
    y_arma<-c(y_arma,bk_mat[,i_filter]%*%x[i:(i-L_long+1)])
  ts.plot(y_arma)
  # empirical holding time
  empirical_ht_arma_vec<-c(empirical_ht_arma_vec,(len-L_long)/length(which(y_arma[1:(length(y_arma)-1)]*y_arma[2:length(y_arma)]<0)))
  
  # Generate filter output based on white noise
  y_white_noise<-NULL
  for (i in (2*L_long):len)
    y_white_noise<-c(y_white_noise,bk_white_noise_mat[,i_filter]%*%eps[-1+i:(i-L_long+1)])
  if (F)
  {
  # Check: quatients should be constant (both output series match up to finite sample differences (coefficients do not decay to zero rapidly))
    y_arma/y_white_noise
  }
  # empirical holding time: same as above up to rounding errors...
  empirical_ht_wn_vec<-c(empirical_ht_wn_vec,(len-L_long)/length(which(y_white_noise[1:(length(y_white_noise)-1)]*y_white_noise[2:(length(y_white_noise))]<0)))
  
  
  empirical_criterion_vec<-c(empirical_criterion_vec,cor(cbind(y_arma,zt))[1,2])
}

empirical_criterion_vec
# Theoretical criteria from SSA-routine
#   Note that the correlation of first SSA is nearly one because holding-time ht_first is nearly equal to holding-time of MSE. But the corresponding bk_mat[,1] is not an identity since the MSE-filter is not the target (it lacks the future eps_{T+1}): Therefore bk_mat[,1] maps the target to nearly the MSE...
crit_SSA1
crit_SSA2
crit_SSA3

# bk
empirical_ht_arma_vec
# Convolution (b*xi)k
empirical_ht_wn_vec
# Specified holding-times: filters 2 and 3 have the same ht
ht_first
ht_second
ht_second


@
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_example1_arma.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(2,2))
mplot<-bk_mat
plot(mplot[,1],main=expression(paste("SSA applied to ",x[t],sep="")),axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot_short<-mplot[1:10,]
plot(mplot_short[,1],main="",axes=F,type="l",xlab="Lag-structure",ylab="",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot_short)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot_short))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot_short[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot_short)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot_short),labels=-1+1:nrow(mplot_short))
axis(2)
box()

mplot<-bk_white_noise_mat#[,1:3]

plot(mplot[,1],main=expression(paste("SSA applied to ",epsilon[t],sep="")),axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot_short<-mplot[1:10,]
plot(mplot_short[,1],main="",axes=F,type="l",xlab="Lag-structure",ylab="",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot_short)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot_short))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot_short[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot_short)[i],col=colo[i],line=-i)
}
mplot_short<-bk_wn#[,1:3]
if (F)
{
  for (i in 1:ncol(mplot_short))
  {
  #  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
    lines(mplot_short[,i],col=colo[i],lwd=1,lty=2)
    mtext(colnames(mplot_short)[i],col=colo[i],line=-i)
  }
}
axis(1,at=1:nrow(mplot_short),labels=-1+1:nrow(mplot_short))
axis(2)
box()


invisible(dev.off())
@
We consider one-step ahead forecasting of the slightly more complex target 
\begin{eqnarray}
z_t&=&x_t+x_{t-1}+x_{t-2}\label{trend_se}\\
x_t&=&\Sexpr{ar1}x_{t-1}+\epsilon_t+\Sexpr{ma[1]}\epsilon_{t-1}+\Sexpr{ma[2]}\epsilon_{t-2}\label{series_se}
\end{eqnarray}
where $x_t$ is a stationary ARMA(1,2)-process with MA-inversion or Wold-decomposition $x_t=\sum_{k\geq 0}\xi_k\epsilon_{t-k}$ where $\xi_k=\left(\mathbf{A}_{ar}^k\mathbf{b}_{ma}\right)_1$, $\mathbf{A}_{ar}=\left(\begin{array}{ccc}\Sexpr{ar1}&1&0\\0&0&1\\0&0&0\end{array}\right)$, $\mathbf{b}_{ma}=(1,\Sexpr{ma[1]},\Sexpr{ma[2]})'$ and where $(\cdot)_1$ denotes the first element of a vector\footnote{Invertibility of the ARMA is not required for deriving the SSA-predictor: theorem \ref{lambda} assumes that $\boldsymbol{\gamma}_{\delta}$ has complete spectral support which applies here.}. This example can be related to signal extraction, where a target filter $\boldsymbol{\gamma}$ (the equally-weighted MA(3) in \ref{trend_se}) is applied to autocorrelated data $x_t$ (the ARMA-process in \ref{series_se}) in order to extract components such as trends, cycles or seasonal components: Wildi (2023) relies on a bi-infinite symmetric Hodrick-Prescott design, see Hodrick and Prescott (1997), as the target or 'signal extraction' filter $\boldsymbol{\gamma}$ for the analysis of business-cycles; the equally-weighted MA(3) filter $\gamma_0=\gamma_1=\gamma_2=1$ in \ref{trend_se} is a simple lowpass 'trend' filter. %, displayed in figure \ref{filt_coef_example1_arma} (black lines in bottom left and right panels). 
SSA-predictors in this framework can be set-up in terms of $y_t=\sum_{k=0}^{L-1}(b\cdot\xi)_k\epsilon_{t-k}$ or $y_t=\sum_{k=0}^{L-1}b_kx_{t-k}$: the latter corresponds to the proper signal extraction filter or predictor. The convolution $(\mathbf{b}\cdot\boldsymbol{\xi})$, proposed in section \ref{ext_stat}, can be obtained directly from theorem \ref{lambda}, see the bottom panels of fig.\ref{filt_coef_example1_arma}.
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_example1_arma"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA arbitrarily scaled. All lags (left panel) and first ten lags (right panel). Predictors  as applied to $x_t$ (upper panels)  and $\\epsilon_t$ (bottom panels). ", sep = "")
cat("\\label{filt_coef_example1_arma}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The signal extraction filter $\mathbf{b}$, displayed in the top panels, can be obtained iteratively, by inversion or deconvolution of $(b\cdot\xi)_j$:
\begin{eqnarray}\label{con_inv}
b_k=\left\{\begin{array}{cc}(b\cdot\xi)_0/\xi_{0}&,k=0\\
\frac{(b\cdot\xi)_{k}-\sum_{j=0}^{k-1}\xi_{k-j}b_j}{\xi_0}&k>0\end{array}\right.
\end{eqnarray}
 









\subsection{Smoothing and Un-Smoothing}\label{smooth_unsmooth}
<<label=init,echo=FALSE,results=hide>>=
# Same example as above but the input xt of target zt is an ARMA-process 
# ARMA-target: this has much stronger lag-one acf than previous example
ar1<-0.8
ma<-c(0.5,0.4)
# Target zt based on xt: same as above
target<-rep(1,3)
forecast_horizon<-0
L_short<-20
L_long<-50
# Ma-inversion of ARMA-target: this is needed for SSA-estimation
xi<-NULL
xi<-c(1,ARMAtoMA(ar=ar1,ma=ma,lag.max=L_long-1))
ts.plot(xi)

# Convolution of target filter and ARMA-filter: this is not required for SSA-estimation
# But ht_target is required in text
xi_target<-c(xi[1],sum(xi[1:2]),xi[1:(length(xi)-2)]+xi[2:(length(xi)-1)]+xi[3:(length(xi))])
ht_arma<-compute_holding_time_func(xi)$ht
ht_target<-compute_holding_time_func(xi_target)$ht

# SSA-settings
rho0<-as.double(compute_holding_time_func(target)$rho_ff1)
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
grid_size<-1000
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"
delta<-forecast_horizon
# Specify target filter as applied to xt: convolution with xi is performed in SSA_func
gamma_target<-gammak_generic<-target
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# In signal extraction cases it is often suitable to allow for negative nu or lambda if xt is strongly autocorrelated and holding-time 'small': then the signal extraction filter must un-smooth xt i.e. negative nu is required
with_negative_lambda<-T

SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

nu_SSA1<-SSA_obj$nu_opt
crit_SSA1<-SSA_obj$crit_rhoyz
bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
bk_white_noise_mat=SSA_obj$bk_white_noise_mat
colnames(bk_mat)<-colnames(bk_white_noise_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
compute_holding_time_func(bk_white_noise_mat[,1])$ht


ht<-ht_second<-30
rho0<-compute_rho_from_ht(ht)$rho
with_negative_lambda<-F

SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

nu_SSA2<-SSA_obj$nu_opt
crit_SSA2<-SSA_obj$crit_rhoyz
bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
bk_white_noise_mat<-cbind(bk_white_noise_mat,SSA_obj$bk_white_noise_mat)

colnames(bk_mat)[2]<-colnames(bk_white_noise_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")


# Scale so that plots are comparable
bk_mat[,1]<-bk_mat[,1]/bk_mat[1,1]*bk_wn[1,1]
bk_mat[,2]<-bk_mat[,2]/bk_mat[1,2]*bk_wn[1,2]
bk_white_noise_mat[,1]<-bk_white_noise_mat[,1]/bk_white_noise_mat[1,1]*bk_wn[1,1]
bk_white_noise_mat[,2]<-bk_white_noise_mat[,2]/bk_white_noise_mat[1,2]*bk_wn[1,2]


ts.plot(bk_mat)
ts.plot(bk_white_noise_mat)
ijk<-2
b<-NULL
for (i in 1:nrow(bk_mat))
  b<-c(b,bk_mat[1:i,ijk]%*%xi[i:1])
b/bk_white_noise_mat[,ijk]

if (F)
{  
# Check holding times: holding-times of bk_mat are wrong (bk_mat is applied to ARMA)
  compute_holding_time_func(bk_mat[,1])$ht
  compute_holding_time_func(bk_mat[,2])$ht
}
# Check holding times: holding-times of bk_white_noise_mat are correct (bk_white_noise_mat is applied to noise i.e. convolution of Wold decomposition and bk_mat)
compute_holding_time_func(bk_white_noise_mat[,1])$ht
compute_holding_time_func(bk_white_noise_mat[,2])$ht

#------------------------------
# Verify that empirical holding-times match
set.seed(6)
len<-20000
if (F)
  x<-arima.sim(n = len, list(ar =ar1, ma = ma))
# Generate ARMA with MA-inversion
eps<-rnorm(len+L_long)
x<-NULL
for (i in L_long:(L_long+len))
  x<-c(x,xi%*%eps[i:(i-L_long+1)])
# Check that it is indeed an ARMA
x_obj<-arima(x,order=c(1,0,2))
x_obj
tsdiag(x_obj,gof.lag=20)

# Generate filter output based on xt (ARMA): first SSA-filter
y_arma<-NULL
i_filter<-1
for (i in L_long:len)
  y_arma<-c(y_arma,bk_mat[,i_filter]%*%x[i:(i-L_long+1)])
ts.plot(y_arma)
# empirical holding time
(len-L_long)/length(which(y_arma[1:(length(y_arma)-1)]*y_arma[2:length(y_arma)]<0))

# Compute target: shift by delta
z<-c(rep(NA,length(gamma_target)-delta-1),(x[1:(len-2)]+x[2:(len-1)]+x[3:len])/3,rep(NA,delta))
y_mat<-cbind(z[L_long:len],y_arma)

# Generate second SSA-filter
y_arma<-NULL
i_filter<-2
for (i in L_long:len)
  y_arma<-c(y_arma,bk_mat[,i_filter]%*%x[i:(i-L_long+1)])
ts.plot(y_arma)
# empirical holding time
(len-L_long)/length(which(y_arma[1:(length(y_arma)-1)]*y_arma[2:length(y_arma)]<0))

y_mat<-cbind(y_mat,y_arma)

ts.plot(y_mat[1:150,],col=c("black",colo[1:2]))
abline(h=0)

# Check criteria: empirical correlations
cor(na.exclude(y_mat))[1,2:3]
# Theoretical criterion values
crit_SSA1
crit_SSA2

#-----------------------------------
# Verify convolution in frequency-domain
K<-600
plot_T<-F
amp_bk_1<-amp_shift_func(K,bk_mat[,1],plot_T)$amp
amp_bk_2<-amp_shift_func(K,bk_mat[,2],plot_T)$amp
amp_bk_white_noise_1<-amp_shift_func(K,bk_white_noise_mat[,1],plot_T)$amp
amp_bk_white_noise_2<-amp_shift_func(K,bk_white_noise_mat[,2],plot_T)$amp
amp_arma<-amp_shift_func(K,xi,plot_T)$amp
# Check: frequency-domain convolutions match
ts.plot(scale(cbind(amp_bk_1*amp_arma,amp_bk_white_noise_1),center=F,scale=T))
ts.plot(scale(cbind(amp_bk_2*amp_arma,amp_bk_white_noise_2),center=F,scale=T))


@
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_example1_arma_su.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(1,2))
mplot<-bk_mat
plot(mplot[,1],main=expression(paste("SSA applied to ",x[t],sep="")),axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-bk_white_noise_mat
plot(mplot[,1],main=expression(paste("SSA applied to ",epsilon[t],sep="")),axes=F,type="l",xlab="Lag-structure",ylab="",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()



invisible(dev.off())

file = "output_example1_arma_su.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(1,1))

mplot<-scale(y_mat[1:149,],center=F,scale=T)

plot(mplot[,1],main="Target (black) and SSA-nowcasters",axes=F,type="l",xlab="Time",ylab="",ylim=c(min(mplot),max(na.exclude(mplot))),col="black",lwd=2)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=2)
  lines(mplot[,i],col=colo[i-1],lwd=1)
#  mtext(colnames(mplot)[i],col=colo[i-1],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


invisible(dev.off())

@
In this example we aim at fitting a time series $z_t$ conditional on different holding-times: in contrast to the previous two sections we here emphasize \emph{nowcasting} i.e.  $\delta=0$. For this purpose, we consider the target 
\begin{eqnarray*}
z_t&=&x_t+x_{t-1}+x_{t-2}\\
x_t&=&\Sexpr{ar1}x_{t-1}+\epsilon_t+\Sexpr{ma[1]}\epsilon_{t-1}+\Sexpr{ma[2]}\epsilon_{t-2}
\end{eqnarray*}
with  holding-time $ht=$\Sexpr{round(ht_target,1)}. We then apply two SSA-designs with holding-times \Sexpr{round(ht_first,1)} and \Sexpr{round(ht_second,1)}: the first nowcast 'un-smooths', the second smooths $z_t$, whilst minimizing MSE (up to arbitrary scaling). Fig.\ref{filt_coef_example1_arma_su} displays filter coefficients as applied to $x_t$, left panel, or to $\epsilon_t$,  right panel (arbitrarily scaled). Note the typical shape of the filters in the right panel, indicating presence of the left boundary constraint $(b\cdot\xi)_{-1}=0$ (the filters in the left panel are subject to different boundary-constraints, due to deconvolution). %Un-smoothing or shortening of the holding-time by the first SSA-design is obtained by the alternating signs of its coefficients (blue line, left panel): the corresponding amplitude function corresponds to a high-pass design, see fig.\ref{amp_shift_SSA} in the following section \ref{conv_amp}. 
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_example1_arma_su.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA arbitrarily scaled: smoothing nowcaster (red) and un-smoothing nowcaster (blue), as applied to $x_t$ (left) and $\\epsilon_t$ (right).", sep = "")
cat("\\label{filt_coef_example1_arma_su}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
SSA-nowcasters and target are compared in fig.\ref{output_example1_arma_su}: all series are arbitrarily scaled to unit-variance. Zero-crossings of the smoothing nowcast (red) are fewest, followed by the target (black) and the un-smoothing nowcast (blue): frequencies or occurrences of crossings are inversely proportional to holding-times, as desired. The maximized theoretical criterion values $\rho(y_{SSA_i},z,0)$, $i=1,2$, are \Sexpr{round(crit_SSA1,3)} (blue un-smoother) and \Sexpr{round(crit_SSA2,3)} (red smoother): these match empirical estimates \Sexpr{round(cor(na.exclude(y_mat))[1,2],3)} and \Sexpr{round(cor(na.exclude(y_mat))[1,3],3)} based on a sample of length \Sexpr{len} of (Gaussian) $x_t$ from which an excerpt is shown in fig.\ref{output_example1_arma_su}. 
%In both cases, the control at zero-crossings is exerted such that SSA-nowcasts match the target accurately, in terms of maximizing cross-correlations.      
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "output_example1_arma_su.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Target (black) and SSA-nowcasts (blue and red): all series scaled to unit-variance.", sep = "")
cat("\\label{output_example1_arma_su}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@




\subsection{Elements of Filtering}\label{conv_amp}

According to the extension in section \ref{ext_stat} and to theorem \ref{lambda}, the SSA-solution $(\mathbf{b}\cdot\boldsymbol{\xi})$ can be obtained from \ref{ar2} as
\begin{eqnarray}\label{ar2_conv_il}
(b\cdot\xi)_{k+1}-\nu(b\cdot\xi)_{k}+(b\cdot\xi)_{k-1}=\gamma_{k+\delta}
\end{eqnarray}
where we assume the arbitrary scaling $D=1$. In principle, this equation suggests that $(\mathbf{b}\cdot\boldsymbol{\xi})$ can be interpreted as the time-domain convolution of the (non-stationary) AR(2)-filter, with AR-coefficients $a_1=-\nu$, $a_2=1$, and the MSE-filter $\boldsymbol{\gamma}_{\delta}$. While this statement is not entirely true, see below for details, we can nevertheless interpret \ref{diff_non_home} and \ref{rho_fd} as the outcomes of frequency-domain convolutions and Plancherel-identity. Fig.\ref{amp_shift_SSA} displays the amplitude functions of both SSA-nowcasts of the previous section \ref{smooth_unsmooth}: % when applied to $\epsilon_t$ (left panel) and $x_t$ (right panel); the violet line in the left panel is the amplitude function of the ARMA-filter generating $x_t$. 
SSA-amplitudes in left and right  panels correspond to SSA-nowcasts in left and right panels of fig.\ref{filt_coef_example1_arma_su}. The additional violet line in fig.\ref{amp_shift_SSA} is the amplitude of the ARMA-filter $\xi_k$, $k\geq 0$, and the amplitude functions in the left panel $A_{SSA,x_t}(\omega)$ can be obtained by  division of SSA- by ARMA-amplitudes in the right panel $A_{SSA,x_t}(\omega)=A_{SSA,\epsilon_t}(\omega)/A_{Arma}(\omega)$: this frequency-domain deconvolution corresponds to the time-domain deconvolution \ref{con_inv}.  The 'un-smoothing' nowcast applied to $x_t$ (blue line, left panel) is a highpass filter,  emphasizing high-frequency-components of $x_t$: as a result the number of zero-crossings increases (blue vs. black line in fig.\ref{output_example1_arma_su}). The smoothing nowcast (red line, left panel fig.\ref{amp_shift_SSA}) is a lowpass filter: as a result zero-crossings are fewer (red vs. black line in fig.\ref{output_example1_arma_su}). %The protuberance around frequency $4\pi/6$  reflect the zigzag pattern of the coefficients in fig.\ref{filt_coef_example1_arma_su} (left panel) and are mainly due to a corresponding global minimum (of the amplitude) of the ARMA-filter towards that frequency, see fig.\ref{amp_shift_SSA} (right panel). 
<<label=init,echo=FALSE,results=hide>>=

amp_bk_1<-amp_shift_func(K,bk_mat[,1],plot_T)$amp
amp_bk_2<-amp_shift_func(K,bk_mat[,2],plot_T)$amp
amp_bk_white_noise_1<-amp_shift_func(K,bk_white_noise_mat[,1],plot_T)$amp
amp_bk_white_noise_2<-amp_shift_func(K,bk_white_noise_mat[,2],plot_T)$amp
# MSE-filter depends on forecast horizon delta: here nowcast i.e. delta=0
amp_arma<-amp_shift_func(K,c(xi[(delta+1):length(xi)],rep(0,delta)),plot_T)$amp

# Amplitude of SSA AR(2):
amp_ar2_SSA1<-amp_ar2_SSA2<-NULL
for (i in 0:K)
{
  omegak<-i*pi/K
  amp_ar2_SSA1<-c(amp_ar2_SSA1,abs(1/(exp(1.i*omegak)-nu_SSA1+exp(-1.i*omegak))))
  amp_ar2_SSA2<-c(amp_ar2_SSA2,abs(1/(exp(1.i*omegak)-nu_SSA2+exp(-1.i*omegak))))
}

# Check convolution in frequency-domain: both amplitudes overlap
ts.plot(scale(cbind(amp_bk_2*amp_arma,amp_bk_white_noise_2),center=F,scale=T),lty=1:2)

# Note however that convolution of ARMA with AR(2) SSA-filter (see time-domain solution of SSA problem) does not match amp_bk_white_noise_2 because bk_white_noise_2 must comply with boundary constraints b_{-1}=b_L=0: they are a mix of non-homogeneous AR(2)- and homogeneous solutions. 
ts.plot(scale(cbind(amp_ar2_SSA2*amp_arma,amp_bk_white_noise_2),center=F,scale=T),lty=1:2)

file = "amp_shift_SSA.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
# This is the finite time-domain deconvolution of bk
mplot<-scale(cbind(amp_bk_1,amp_bk_2),center=F,scale=T)
colnames(mplot)<-colnames(bk_mat)
plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="",main=expression(paste("Amplitude ",x[t],sep="")),ylim=c(min(mplot),max(mplot)),col=colo[1])
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo[i])
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1+0:3*K/3,labels=expression(0,2*pi/6,4*pi/6,pi))
axis(2)
box()


mplot<-scale(cbind(amp_bk_white_noise_1,amp_bk_white_noise_2,amp_arma),center=F,scale=T)
mplot[,3]<-1.5*mplot[,3]
colnames(mplot)<-c(colnames(bk_mat),"ARMA")
plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="Amplitude",main=expression(paste("Amplitude ",epsilon[t],sep="")),ylim=c(min(mplot),max(mplot)),col=colo[1])
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo[i])
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1+0:3*K/3,labels=expression(0,2*pi/6,4*pi/6,pi))
axis(2)
box()


dev.off()


@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "amp_shift_SSA.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Amplitude functions of SSA-nowcaster as applied to $x_t$  (left panel) and to $\\epsilon_t$(right panel) with arbitrary scaling. The amplitude of the ARMA-filter $x_t$ is displayed in the right panel (violet)", sep = "")
cat("\\label{amp_shift_SSA}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
In principle, the SSA-amplitude functions in the right panel should correspond to the convolution of AR(2)-filter and ARMA-filter linked by \ref{ar2_conv_il}. However, the solution $(\mathbf{b}\cdot\boldsymbol{\xi})$ must comply with the boundary constraints $(b\cdot\xi)_{-1}=(b\cdot\xi)_L=0$ which ensure 'stability' of $(\mathbf{b}\cdot\boldsymbol{\xi})$ even though the AR(2) is non-stationary. While technical details are omitted here, we mention that  $(\mathbf{b}\cdot\boldsymbol{\xi})$ is a linear combination of solutions of non-homogeneous and homogeneous difference-equations whereby the latter ensures compliance with the boundary-constraints. The SSA-amplitudes of the suitable mix of homogeneous and non-homogeneous AR(2)-solutions are then plotted in the right panel of fig.\ref{amp_shift_SSA}. 



\subsection{Unit-Root Case}\label{unit_root_case}


<<label=init,echo=FALSE,results=hide>>=
# Example from smooting/unsmoothing but with much longer holding-time
ar1<-0.8
ma<-c(0.5,0.4)
# Target zt based on xt: same as above
target<-rep(1,3)
forecast_horizon<-1
L_short<-20
L_long<-100
# Ma-inversion of ARMA-target: used for convolution when computing SSA-predictors
xi<-NULL
xi<-c(1,ARMAtoMA(ar=ar1,ma=ma,lag.max=L_long-1))
# Select ht such that SSA with length L_short is close to unit-root (rho~rho_max(L_short))
ht<-L_short
rho0<-compute_rho_from_ht(ht)$rho
# Only positive nu (smoothing)
with_negative_lambda<-F
grid_size<-10000
delta<-forecast_horizon
# gamma as applied to xt
gamma_target<-gammak_generic<-target
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting because the holding-time corresponds to rhomax
lower_limit_nu<-"rhomax"

SSA_obj<-SSA_func(L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

crit_unit_root<-SSA_obj$crit_rhoyz
nu_opt_short=SSA_obj$nu_opt
bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
bk_white_noise_mat=SSA_obj$bk_white_noise_mat
colnames(bk_mat)<-colnames(bk_white_noise_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")
compute_holding_time_func(bk_white_noise_mat[,1])$ht

# Same as above but longer Filter
SSA_obj<-SSA_func(L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu)

crit_no_unitroot<-SSA_obj$crit_rhoyz
nu_opt_long=SSA_obj$nu_opt
bk_mat<-cbind(c(bk_mat,rep(0,L_long-L_short)),SSA_obj$bk_mat)
bk_white_noise_mat<-cbind(c(bk_white_noise_mat,rep(0,L_long-L_short)),SSA_obj$bk_white_noise_mat)

colnames(bk_mat)[2]<-colnames(bk_white_noise_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")


# Scale so that plots are comparable
bk_mat[,1]<-0.1*bk_mat[,1]/bk_mat[1,1]
bk_mat[,2]<-0.1*bk_mat[,2]/bk_mat[1,2]
bk_white_noise_mat[,1]<-0.1*bk_white_noise_mat[,1]/bk_white_noise_mat[1,1]
bk_white_noise_mat[,2]<-0.1*bk_white_noise_mat[,2]/bk_white_noise_mat[1,2]

apply(bk_white_noise_mat,2,compute_holding_time_func)

ts.plot(bk_white_noise_mat)

nu_opt_short
nu_opt_long
crit_unit_root
crit_no_unitroot

# Criterion values

@
Typically, the coefficients of a SSA-predictor (or -nowcaster) $\mathbf{b}$ decay towards zero at a rate determined by $\boldsymbol{\gamma}_{\delta}$ and the holding-time constraint $\rho_1$ or $ht_1$. Figure \ref{filt_coef_example4} displays two SSA-designs for the nowcast problem in section \ref{smooth_unsmooth}: both nowcasters are subject to the same holding-time $\Sexpr{round(ht,1)}$ but they differ in terms of filter-lengths: $L=\Sexpr{L_short}$ (blue) vs. $L=\Sexpr{L_long}$ (red). The shorter filter is subject to a unit-root, since $\nu_0=\Sexpr{round(nu_opt_short,3)}<2$, where $\nu_0$ is the (numerical) solution to the holding-time equation \ref{uni_unco_min}. As a result the forecast weights do not follow an exponential-law at higher lags: the pattern is close to the eigenvector $\mathbf{v}_{20}$ of the largest eigenvalue $\lambda_{20}=\rho_{max}(20)$ of $\mathbf{M}$ (upper half of a sinusoid). Indeed, the imposed holding-time $ht=\Sexpr{ht}$ approaches the upper limit of admissibility $ht_{max}(\Sexpr{L_short})=\Sexpr{L_short+1}$ so that 'smoothing' starts to supplant 'forecasting'. Increasing the filter length $L$ from $\Sexpr{L_short}$ to $\Sexpr{L_long}$ reinstates stability as illustrated by the exponential decay of the 'long' SSA-nowcaster at higher lags (red line, right panel). In applications, the unit-root case typically occurs when $L$ and $ht_1$ are mismatched. In such a case, an increase of the filter-length can improves MSE-performances by unleashing degrees of freedom: in our example the criterion value $\rho(y,z,\delta)$ rises from $\Sexpr{round(crit_unit_root,2)}$, for the short nowcaster, to $\Sexpr{round(crit_no_unitroot,2)}$, for the long nowcaster. %????The right panel illustrates a tip or lobe which is due to the initial boundary constraint. This typical SSA-pattern implies that most recent data is weighted less than data at lags corresponding roughly to $ht/2$. The corresponding 'bump' aims at reconciling the original MSE-filter with a sinusoid of frequency $\omega=2\pi/ht$ and this combination aims at a smooth transitions at the zero-line such that the number of random crossings is contained.       
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_example4.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(1,2))
mplot<-as.matrix(bk_white_noise_mat[1:L_short,1])
plot(mplot[,1],main="Short SSA nowcaster",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-as.matrix(bk_white_noise_mat[,2])
plot(mplot[,1],main="Long SSA nowcaster",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[2],lwd=1)
mtext(colnames(mplot)[1],col=colo[2],line=-1)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


invisible(dev.off())



@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_example4.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Arbitrarily scaled SSA nowcasters for identical holding-time ht=20 but different filter lengths L=20 (blue) and L=100 (red).", sep = "")
cat("\\label{filt_coef_example4}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@



\subsection{Resilience}\label{resil}


Wildi (2023) applies the SSA-approach to (the log-returns of) a broadly diversified equity index, the Standard and Poors 500, as well as  to industrial production indices of a selection of countries with long and consistent sample histories. Empirical and theoretical holding-times, wrongly assuming Gaussianity, match virtually perfectly for the financial time series, despite volatility clustering, non-vanishing mean (drift) and extreme observations during financial and pandemic crises. Discrepancies observed in the case of the macro-indicators were attributable to autocorrelation and could be alleviated by the extension to autocorrelated processes illustrated in the above sections. 
<<label=init,echo=FALSE,results=hide>>=
set.seed(14)
len<-100000
# Use SSA-designs of previous example
bk_mat<-bk_white_noise_mat_resilience[,1:3]
apply(bk_mat,2,compute_holding_time_func)
# Degrees of freedom of t-distribution
df_vec<-c(2,4,6,8,10)
ht_mat<-NULL
for (ijk in 1:length(df_vec))#ijk<-1
{
  eps<-rt(len,df_vec[ijk])
#  eps<-rnorm(len)
  output<-matrix(nro=len,ncol=ncol(bk_mat))
  empirical_ht_vec<-NULL
  for (j in 1:ncol(bk_mat))
  {  
    for (i in nrow(bk_mat):len)
    {
      output[i,j]<-bk_mat[,j]%*%eps[i:(i-nrow(bk_mat)+1)]
    }
    empirical_ht_vec<-c(empirical_ht_vec,(len-nrow(bk_mat))/length(which(output[(nrow(bk_mat)+1):len,j]*output[(nrow(bk_mat)):(len-1),j]<0)))
  }
  ht_mat<-rbind(ht_mat,empirical_ht_vec)
}
rownames(ht_mat)<-paste("t-dist.: df=",df_vec,sep="")
colnames(ht_mat)<-colnames(bk_mat)
colnames(ht_mat)[2]<-paste(colnames(ht_mat)[2],":L=20",sep="")
colnames(ht_mat)[3]<-paste(colnames(ht_mat)[3],":L=50",sep="")
ht_mat
round(ht_mat,2)

@
We here complement these findings, which document resilience of the approach against departures from Gaussianity, by an application of SSA to white noise $x_t=\epsilon_t$ where $\epsilon_t$ is t-distributed with degrees of freedom ranging from $df=\Sexpr{df_vec[1]}$ (heavy tails) to $df=\Sexpr{df_vec[length(df_vec)]}$ (nearly Gaussian). We then compare empirical holding-times, i.e. length of filter-outputs divided by number of zero-crossings, to theoretical holding-times, wrongly assuming Gaussianity, based on long samples of size \Sexpr{as.integer(len)} of $\epsilon_t$, see table \ref{emp_ht}.
<<label=ats_mba_2,echo=FALSE,results=tex>>=
xtable(ht_mat, dec = 1,digit=2,
paste("Empirical holding-times of SSA designs as applied to t-distributed white noise"),
label=paste("emp_ht",sep=""),
center = "centering", file = "", floating = FALSE)
@
Our findings suggest that an increased incidence of extreme observations (first and second rows of the table) leads to a positive bias of the empirical holding-times for filters with a larger $ht$ or lag-one acf. This phenomenon can be explained by the impulse response of the filters which is triggered by extreme observations and which does not change sign because all filter coefficients are positive: a longer  tail of the filter then implies fewer crossings and a positive bias of the empirical holding-time. But the magnitude of the bias seems to be well controlled, overall, even in the presence of series with heavy-tails and the bias could be reduced further by application of outlier techniques (not shown here).


\subsection{A Smoothness-Timeliness Dilemma}\label{time_smooth}

<<label=init,echo=FALSE,results=hide>>=
# 0. Data: white noise

set.seed(31)
len<-100000
series<-rnorm(len)


#---------------------------------------
# 1. MA-target
setseed<-1
L<-100
gammak_generic<-rep(1/L,L)
forecast_horizon_vec<-c(20,40)
gamma_mse<-c(gammak_generic[(forecast_horizon_vec[1]+1):L],rep(0,forecast_horizon_vec[1]))

# Comaprison of holding-times of MSE and target
compute_holding_time_func(gammak_generic)$ht
compute_holding_time_func(gamma_mse)$ht

#--------------------------------------------------
# 2. SSA hyperparameters

# Holding-time constraint: the same as target
ht<-30
rho0<-as.double(compute_rho_from_ht(ht))
grid_size<-1000
# Include negative lambda1: yes/no  
with_negative_lambda<-F


#---------------------------------------------------
# 3. SSA filter, Assumption: data is white noise

# 3.1 Compute SSA filters
# Discarding xi and lower_limit_nu in the function call assumes default values (white noise and rhomax) 
SSA_obj<-SSA_func(L,forecast_horizon_vec,grid_size,gammak_generic,rho0,with_negative_lambda)#,lower_limit_nu)
  
bk_mat=SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",ht,",",forecast_horizon_vec,")",sep="")

# 3.2 Specify filter matrix with benchmarks (HP MSE and HP trend concurrent) and SSA filters
#   Filter data
colo<-c("green","red","blue")

filter_mat<-cbind(gamma_mse,bk_mat)
colnames(filter_mat)<-c("MSE",colnames(bk_mat))
ts.plot(scale(filter_mat,center=F,scale=T),col=colo)
for (i in 1:ncol(filter_mat))
  mtext(colnames(filter_mat)[i],col=colo[i],line=-i)


filter_obj<-SSA_filter_func(filter_mat,L,series)

y_mat=filter_obj$y_mat


# number of crossings
number_cross<-rep(NA,ncol(filter_mat))
names(number_cross)<-colnames(filter_mat)
for (i in 1:ncol(y_mat))
{
  if (is.xts(y_mat))
  {  
    number_cross[i]<-length(which(sign(y_mat[,i])!=sign(lag(y_mat[,i]))))
  } else
  {
    number_cross[i]<-length(which(sign(y_mat[1:(nrow(y_mat)-1),i])!=sign(lag(y_mat[2:nrow(y_mat),i]))))
  }
}

number_cross
# empirical holding time: larger than ht
nrow(na.exclude(y_mat))/number_cross

# Plot
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Output of filter whose delta exceeds 2*ht is 'negative', see above remarks
ts.plot(scale(y_mat[1:min(len-L,1000),],center=F,scale=T),col=colo,xlab="")
abline(h=0)
for (i in 1:ncol(filter_mat))
  mtext(colnames(filter_mat)[i],col=colo[i],line=-i)



#-----------------------------------------------------
# 4 Peak correlation

max_lead<-41
cor_mse_20<-cor_20_40<-NULL
for (i in 1:max_lead)
{
  cor_mse_20<-c(cor_mse_20,cor(y_mat[i:(nrow(y_mat)),"MSE"],y_mat[1:(nrow(y_mat)-i+1),"SSA(30,20)"]))
  cor_20_40<-c(cor_20_40,cor(y_mat[i:(nrow(y_mat)),"MSE"],y_mat[1:(nrow(y_mat)-i+1),"SSA(30,40)"]))
}
# Invert time ordering
cor_mse_20<-cor_mse_20[max_lead:1]
cor_20_40<-cor_20_40[max_lead:1]
# Compute other tail
for (i in 1:(max_lead-1))
{
  cor_mse_20<-c(cor_mse_20,cor(y_mat[(i+1):(nrow(y_mat)),"SSA(30,20)"],y_mat[1:(nrow(y_mat)-i),"MSE"]))
  cor_20_40<-c(cor_20_40,cor(y_mat[(i+1):(nrow(y_mat)),"SSA(30,40)"],y_mat[1:(nrow(y_mat)-i),"MSE"]))
}


plot(cor_mse_20,col="green",main="Peak correlations",axes=F,type="l",
     xlab="Lead/lag",ylab="Correlation")
lines(cor_20_40,col="blue")
abline(v=which(cor_20_40==max(cor_20_40)),col="blue")
abline(v=which(cor_mse_20==max(cor_mse_20)),col="green")
at_vec<-c(1,11,21,31,41,51,61,71,81)
axis(1,at=at_vec,labels=at_vec-max_lead)
axis(2)
box()

#-------------------------------------------------------------
# Crossings at zero line: reference SSA delta=40 against SSA delta=20
# Skip all crossings with lead/lag>ht (outliers i.e. different cycle estimates)
skip_larger<-ht
# Index of series with more crossings: this is measured against the crossings of the reference series
con_ind<-1
# Index of reference series: this one has less crossings and shift is measured with reference to thse crossings only
ref_ind<-2
# Select closest crossing of same sign (last_crossing_or_closest_crossing<-F) or 
#   last crossing of same sign in a vicinity of reference crossing (last_crossing_or_closest_crossing<-T)
# The setting last_crossing_or_closest_crossing<-T is closer to applications though still a bit optimistic #   because one doesn't know that a particular crossing will be the last in the vicinity
# The setting last_crossing_or_closest_crossing<-F is unrealistic since the contender filter might generate additional noisy crossings after the closest one 
last_crossing_or_closest_crossing<-F
if (last_crossing_or_closest_crossing)
{
  # Size of vicinity to look for turning-point: +/- vicinity around a reference crossing: one picks the last
  #  (of correct sign) in this vicinity
  # Select equal to holding-time (beyond that point signs could change, in the mean)  
  vicinity<-ht
} else
{
  vicinity<-NULL
}

dim(y_mat)
colnames(y_mat)
select_vec<-c(2,3)
mplot<-y_mat[,select_vec]
colnames(mplot)

lead_lag_cross_obj<-new_lead_at_crossing_func(ref_ind,con_ind,mplot,last_crossing_or_closest_crossing,vicinity)

number_cross_trend<-lead_lag_cross_obj$number_crossings_per_sample
shift<-c(lead_lag_cross_obj$cum_ref_con[1],diff(lead_lag_cross_obj$cum_ref_con))
remove_tp<-which(abs(shift)>skip_larger)
if (length(remove_tp)>0)
{  
  shift_trend<-shift[-remove_tp]
} else
{
  shift_trend<-shift
}
# Positive drift i.e. lead of SSA filter
ts.plot(cumsum(shift_trend))
# Mean lead (positive) or lag (negative) of reference filter (after removing outliers)
mean_lead_ref_con<-mean(shift_trend)
mean_lead_ref_con
# Mean shift including outliers
mean_lead_with_outliers<-lead_lag_cross_obj$mean_lead_ref_con

#------------------------
# Mean lead (positive) or lag (negative) of reference filter (after removing outliers)
lead_business_cycle<-mean(shift_trend)
lead_business_cycle
# Test for significance of shift
t_teste<-t.test(shift,  alternative = "two.sided")$p.value
# Strongly significant lead
t_teste
t_stat<-t.test(shift,  alternative = "two.sided")$statistic

@
Often, stronger noise-rejection or smoothing by an optimal (nowcast or forecast) filter is associated with increased lag or 'right-shift' of its output: the following example illustrates that the mentioned tradeoff, a so-called smoothness-timeliness dilemma, does not hold in general. For illustration, we  rely on a simple empirical framework where the target is an equally-weighted MA-filter of length $L=$\Sexpr{L} applied to simulated Gaussian noise $\epsilon_t$: $z_t=\frac{1}{\Sexpr{L}}\sum_{k=0}^{\Sexpr{L-1}}\epsilon_{t-k}$. The target must be forecasted at the horizon $\Sexpr{forecast_horizon_vec[1]}$ by a classic MSE as well as a SSA(\Sexpr{ht},\Sexpr{forecast_horizon_vec[1]})-filter, whose holding-time $ht=\Sexpr{ht}$ exceeds that of the MSE design $ht=\Sexpr{round(compute_holding_time_func(gamma_mse)$ht,1)}$ by a safe margin. Out of curiosity, we also supply a second SSA(\Sexpr{ht},\Sexpr{forecast_horizon_vec[2]})-filter optimized for forecast horizon \Sexpr{forecast_horizon_vec[2]}: the two hyperparameters $ht,\delta$ of the two SSA-designs suggest that for an identical smoothing capability or holding-time, the second filter should have improved timeliness properties in terms of a lead or left-shift. The three (arbitrarily scaled) forecast filters are displayed in fig.\ref{filters_smooth_time}\footnote{The early rise at the left edge reveals the presence of the left-side boundary constraint $b_{-1}=0$, recall theorem \ref{lambda}.} and filter outputs, arbitrarily scaled to unit-variance, are compared in fig.\ref{filters_smooth_time_out}. 
<<label=init,echo=FALSE,results=hide>>=

colo<-c("green","red","blue")

file = "filter_smooth_time.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

filter_mat<-cbind(gamma_mse,bk_mat)
colnames(filter_mat)<-c("MSE",colnames(bk_mat))
ts.plot(scale(filter_mat,center=F,scale=T),col=colo,xlab="Lag")
for (i in 1:ncol(filter_mat))
  mtext(colnames(filter_mat)[i],col=colo[i],line=-i)
dev.off()

file = "filter_smooth_time_out.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

ts.plot(scale(y_mat[1:1000,],center=F,scale=T),col=colo,xlab="")
abline(h=0)
for (i in 1:ncol(filter_mat))
  mtext(colnames(filter_mat)[i],col=colo[i],line=-i)
dev.off()

file = "filter_smooth_time_peak_corr.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

plot(cor_mse_20,col="green",main="Peak correlations",axes=F,type="l",
     xlab="Lag                  Lead",ylab="Correlation")
lines(cor_20_40,col="blue")
abline(v=which(cor_20_40==max(cor_20_40)),col="blue")
abline(v=which(cor_mse_20==max(cor_mse_20)),col="green")
at_vec<-c(1,11,21,31,41,51,61,71,81)
axis(1,at=at_vec,labels=(at_vec-max_lead))
axis(2)
box()
dev.off()



@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filter_smooth_time.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Forecast filters: MSE (green), SSA(30,20) (red) and SSA(30,40) (blue) with arbitrary scaling", sep = "")
cat("\\label{filters_smooth_time}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filter_smooth_time_out.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Outputs of forecast filters: MSE (green), SSA(30,20) (red) and SSA(30,40) (blue)", sep = "")
cat("\\label{filters_smooth_time_out}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filter_smooth_time_peak_corr.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Correlation of shifted SSA(30,20) vs. MSE (green) and SSA(30,40) (blue). Positive numbers correspond to a relative lead of SSA(30,20) over the contenders. Peak correlations are indicated by vertical lines.", sep = "")
cat("\\label{filters_smooth_time_peak_cor}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
As expected, the output of SSA(\Sexpr{ht},\Sexpr{forecast_horizon_vec[2]}) (blue line in fig.\ref{filters_smooth_time_out}) appears left-shifted. Fig.\ref{filters_smooth_time_peak_cor} displays cross-correlations at various leads and lags of the reference SSA(30,20): the relative shift can be inferred from the  peak-correlation i.e. the lead or lag at which the maximum is achieved. The figure suggests that SSA(30,20) and MSE are on par (green line) and that SSA(30,20) lags or, equivalently, that SSA(30,40) leads by \Sexpr{-(-(length(cor_20_40)-1)/2-1+which(cor_20_40==max(cor_20_40)))} time-units (blue line). %, which lies more or less in the center of the extended plateau of the blue-line. 
Finally, the empirical holding-times in table \ref{smooth_time_emp_ht}, computed on a sample of length \Sexpr{as.integer(len)}, conform to expected values, as based on \ref{ht}.
<<label=init,echo=FALSE,results=hide>>=

mat_ht<-matrix(round(nrow(na.exclude(y_mat))/number_cross,1),nrow=1) 
colnames(mat_ht)<-colnames(filter_mat)
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
xtable(mat_ht, dec = 1,digit=1,
paste("Empirical holding-times of MSE and SSA designs"),
label=paste("smooth_time_emp_ht",sep=""),
center = "centering", file = "", floating = FALSE)
@
We conclude that for identical smoothing capabilities, SSA(\Sexpr{ht},\Sexpr{forecast_horizon_vec[2]}) has improved timeliness characteristics in terms of a systematic lead; moreover, SSA(\Sexpr{ht},\Sexpr{forecast_horizon_vec[2]}) outperforms MSE in terms of timeliness and smoothness; also, timeliness and smoothness can be addressed explicitly by specifying hyper-parameters $(\rho_1,\delta)$. 
In this abstract context, the pair $(\rho_1,\delta)$ spans a two-dimensional space of predictors SSA($\rho_1,\delta$), for a particular target $z_{t+\delta_0}$, with distinct smoothness and timeliness characteristics entailed by the hyper-parameters: we argue that $\rho_1,\delta$ can be selected in view of matching particular research priorities, see e.g. Wildi (2023). Classic MSE-performances can be replicated by selecting $\delta=\delta_0$ and $\rho_1=\rho_{MSE}$, the lag-one acf of the mean-square predictor. 



\subsection{Monotonicity vs. Non-Monotonicity}\label{mon_non_mono}


We here illustrate uniqueness or multiplicity of the solution of the non-linear holding-time equation \ref{uni_unco_min}, depending on $|\nu|>2\rho_{max}(L)$, see assertion \ref{ass4} of theorem \ref{lambda}. Fig. \ref{rho_nu_ar1}  displays the lag-one autocorrelation  $\rho(\nu)$ in \ref{rho_fd} for a SSA-nowcast ($\delta=0$) as a function of $\nu$ for two different AR(1)-targets $\boldsymbol{\gamma}_{0}(a_1)=(1,a_1,...,a_1^9)'$ of length $L=10$  with $a_1=0.99$ (bottom panels) and $a_1=0.6$ (top panels). The panels on the left correspond to $|\nu|<2\rho_{max}(10)$ and illustrate non-monotonicity of $\rho(\nu)$; the panels on the right correspond to  $\nu>2\rho_{max}(10)$ and illustrate strict monotonicity\footnote{The abscissa of the right hand panels are based on transformed $\log(\nu)$ for a better visualisation of the monotonic shape.}. 
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
    file<-"rho_nu_ar1.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Lag-one autocorrelation as a function of $\\nu$ when the target is a classic AR(1) with $a_1=0.6$ (top) and $a_1=0.99$ (bottom): the left/right-split of the panels corresponds to $|\\nu|\\leq 2 \\rho_{max}(L)$ (left) and $\\nu>2 \\rho_{max}(L)$  (right)", sep = "")
cat("\\label{rho_nu_ar1}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
Non-monotonicity generally leads to multiple solutions of $\nu$ for given $\rho_1$ for the holding-time equation \ref{uni_unco_min}, whereby the multiplicity generally depends on $\rho_1$, $L$ as well as on the target $\gamma_{k+\delta}$: as can be seen the green horizontal line corresponding to $\rho_1=0.15$ in fig.\ref{rho_nu_ar1} intersects the acf four times in the upper (left) panel and $L+1=11$ times in the bottom (left) panel. Monotonicity, on the other hand, means that $\nu$ is determined uniquely by $\rho_1$.   






\subsection{Incomplete Spectral Support}\label{incomplete_support}



<<label=init,results=hide>>=
# We here use a band-limited target gammak which is missing the eigenvector v_m corresponding to the largest eigenvalue of M
# We then derive an optimal estimate b based on the space of eiegnevectors spanning gammak (i.e. without v_m)
# We then verify that there does not exist a better estimate including v_m
if (recompute_calculations==T)
{
  len<-L<-10
  set.seed(1)
  
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  
# We skip the first/largest eigenvalue
  w<-c(0,rep(1,L-1))
  w<-w/sqrt(as.double(t(w)%*%w))
  
  gammak<-eigen(M)$vector%*%w
  ts.plot(gammak)
  
  eig<-eigen(M)
  
  eig$values
  smallest_eigen_gammak<-eig$values[which(w!=0)[length(which(w!=0))]]
  largest_eigen_gammak<-eig$values[which(w!=0)[1]]
  
  
  opt_b<-function(lambda)
  {  
    nu<-Re(lambda+1/lambda)#nu<--2  lambda<-0.5
    Nu<-2*M-nu*diag(rep(1,L))
    b<-solve(Nu)%*%gammak#ts.plot(b)  eigen(Nu)$values
    rho<-t(b)%*%M%*%b/(t(b)%*%b)
    crit<-as.double(rho)
    return(list(crit=crit,nu=nu))
  }
  
  resolution<-100
  
  #----------------------------
  #1. |nu|>2
  # For |nu|>2 lambda is in [-1,1]
  lambda_vec<-c(-resolution:(-1),1:resolution)/(resolution)
  
  nu_vec<-crit_vec<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
    lambda<-lambda_vec[i]
    optobj<-opt_b(lambda)
    crit_vec[i]<-optobj$crit
    nu_vec[i]<-optobj$nu
    
  }  
  
  ts.plot(crit_vec)
  # Minimum lag-one acf comes close to minimal eigenvalue of gammak (must use unit-roots to get it exactly, see below)
  min(crit_vec)
  smallest_eigen_gammak
  # Maximal lag-one acf is smaller than theoretical limit i.e. largest eigenvalue of gammak
  max(crit_vec)
  largest_eigen_gammak
  
  #---------------------------
  #2. |nu|<2
  # For |nu|<2 lambda is a frequency in [0,pi]
  lambda_vec<-pi*(0:resolution)/resolution
  
  nu_vec<-crit_vec<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec[i]<-optobj$crit
    nu_vec[i]<-optobj$nu
    
  }  
  
  # Minimum lag-one acf is also minimal eigenvalue of gammak (exact if resolution large)
  min(crit_vec)
  smallest_eigen_gammak
  # Maximal lag-one acf is also theoretical limit i.e. largest eigenvalue of gammak
  max(crit_vec)
  largest_eigen_gammak
  
  #------------------------------
  # 3. Seek optimal nu for rho1 large
  
  rho1<-largest_eigen_gammak-0.01
  # Compare with lag-one acf of (normalized) target
  t(gammak)%*%M%*%gammak
  
  resolution<-10000
  lambda_vec<-pi*(0:resolution)/resolution
  
  nu_vec<-crit_vec<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec[i]<-optobj$crit
    nu_vec[i]<-optobj$nu
    
  } 
  
  ts.plot(crit_vec)
  which_best<-which(abs(crit_vec-rho1)==min(abs(crit_vec-rho1)))
  nu_opt<-nu_vec[which_best]
  
  Nu_opt<-2*M-nu_opt*diag(rep(1,L))
  b_opt<-solve(Nu_opt)%*%gammak#ts.plot(b)  eigen(Nu)$values
  b_opt<-b_opt/as.double(sqrt((t(b_opt)%*%b_opt)))
  rho_opt<-t(b_opt)%*%M%*%b_opt
  if (t(gammak)%*%b_opt<0)
  {
    b_opt<--b_opt
  }
  # Should be nearly vanishing if resolution large
  rho1-rho_opt
  # Criterion value: gammak and b_opt are normalized
  t(gammak)%*%b_opt
  ts.plot(cbind(gammak,b_opt),col=c("black","blue"))
  
  #----------------------------------
  # 4. Does there exist a better b if we include the missing first eigenvector of M (corresponding to the largest eigenvalue) which is missing in gammak?
  # The answer is YES: SEE COMPARISON OF CRITERION VALUES AT END OF CODE
  
  # Specify weights wb in spectral decomposition of b_opt
  V<-eig$vectors
  wb<-solve(V)%*%b_opt
  # Check: should vanish
  # a. Check decomposition
  V%*%wb-b_opt
  # Check normalization
  sum(wb^2)-1
  # Check lag-one acf (formula in paper)
  t(wb^2)%*%eig$values-rho_opt
  
  # add vm to b_opt such that lag-one is still rho1
  # Select weight assigned to random-noise: 
  # This is close to weight assigned to v_m, see below
  alphahh<-0.2
  # Divide by L since random noise is assigned to all L coefficients
  alphah<-alphahh/L
  set.seed(1)
  # Specify new weights: contaminated by noise for 2:L
  wb_newhh<-wb+c(0,alphah*rnorm(length(wb)-1))
  # Normalize
  wb_newhh<-wb_newhh/sqrt(as.double(t(wb_newhh)%*%wb_newhh))
  
  # If term under square-root positive: calculate new alpha such that weights alpha+wb_newhh[1] for v_m and wb_newhh[2:length(wb_newhh)]) for v_{m-1},...,v_1 have lag-one acf rho_opt i.e. the same as b_opt
  if ((rho_opt*sum(as.vector(wb_newhh^2)[2:length(wb_newhh)])-t(as.vector(wb_newhh^2)[2:length(wb_newhh)])%*%eig$values[2:L])/(eig$values[1]-rho_opt)>0)
  {
    print("root positive: OK")
  # Formula for alpha such that lag-one acf will be rho_opt  
    alpha<-sqrt((rho_opt*sum(as.vector(wb_newhh^2)[2:length(wb_newhh)])-t(as.vector(wb_newhh^2)[2:length(wb_newhh)])%*%eig$values[2:L])/(eig$values[1]-rho_opt))-wb_newhh[1]
  # Check: should vanish  
    ((alpha+wb_newhh[1])^2*eig$values[1]+t(as.vector(wb_newhh)[2:length(as.vector(wb_newhh))]^2)%*%eig$values[2:L])/((alpha+wb_newhh[1])^2+sum(as.vector(wb_newhh^2)[2:length(wb_newhh)]))-rho_opt
  }
  # Compute new normalized weights
  wb_newh<-c(alpha+wb_newhh[1],as.vector(wb_newhh)[2:length(as.vector(wb_newhh))])
  wb_new<-wb_newh/as.double(sqrt(t(wb_newh)%*%wb_newh))
  print(c(" Weight assigned to v_m: ",wb_new[1]))
  
  # Compute new b: b_new
  # b_new now depends on v_m and its lag-one acf is rho_opt
  b_new<-V%*%wb_new
  # Check: should vanish
  t(b_new)%*%M%*%b_new-rho_opt
  # Compare b_opt (without v_m) and b_new (with v_m)
  ts.plot(cbind(b_opt,b_new),col=c("blue","red"))
  
  # criterion value: b_new with v_m is better!!!!
  t(gammak)%*%b_opt-t(gammak)%*%b_new
  # Lag-one acfs: are identical
  t(b_new)%*%M%*%b_new-t(b_opt)%*%M%*%b_opt
}





# Solutions when gammak full spectrum converges to gammak reduced spectrum i.e. w_i=epsilon small (here: 0.001)
# This is used for generating example in paper (fig.4)
if (recompute_calculations==T)
{
  ##############################################################################
  ##############################################################################
  opt_b<-function(lambda)
  {  
    nu<-Re(lambda+1/lambda)+0.000001#nu<--2  lambda<-0.5
    Nu<-2*M-nu*diag(rep(1,L))
    b<-solve(Nu)%*%gammak#ts.plot(b)  eigen(Nu)$values
    rho<-t(b)%*%M%*%b/(t(b)%*%b)
    crit<-as.double(rho)
    return(list(crit=crit,nu=nu))
  }

  # Solutions when gammak full spectrum \to gammak reduced spectrum i.e. w_i\to 0
  len<-L<-10
  set.seed(1)
  
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  
# 1. Band-limited target
# 1.1 We skip the first/largest eigenvalue for band-limited target
  larg<-3
  # Epsilon=0 for band-limited target
  epsilon<-0.00
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  
  gammak<-gammak_bandlimited<-eigen(M)$vector%*%w
  ts.plot(gammak_bandlimited)
  
  eig<-eigen(M)
  smallest_eigen_gammak<-eig$values[L]
  largest_eigen_gammak<-eig$values[larg+1]
  
#---------------
# 1.2 compute lag-one acf of band-limited target

  resolution<-1000000
  lambda_vec<-pi*(0:resolution)/resolution
  
  crit_vec_bandlimited<-crit_vec<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
  # Unit roots i.e. |nu|<2   
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec_bandlimited[i]<-optobj$crit
  } 
  
# 2. Augmented full-bandwith target
# 2.1 Epsilon 10^{-3} leads to very good approximation of band-limited gammak and steep/narrow peaks at singularities, see plot below
  epsilon<-0.001
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  
  gammak<-gammak_full<-eigen(M)$vector%*%w
  ts.plot(gammak_full)
  
  eig<-eigen(M)
  smallest_eigen_gammak<-eig$values[L]
  largest_eigen_gammak<-eig$values[larg+1]
  
#---------------
# 2.2 Seek optimal nu for rho1 large
# Select rho1 slightly below largest possible eigenvalue
  max(crit_vec_bandlimited)
# This one is slightly below the highest attainable rho of the band-limited design  
  rho1<-eig$values[larg+1]-0.31
  rho1<-eig$values[larg+1]-0.11
# This one is above the highest attainable rho and requires band-extension by point-mass at longer rhos  
  rho1<-eig$values[larg]-0.05
  largest_eigen_gammak
  # Compare with lag-one acf of (normalized) target
  t(gammak_full)%*%M%*%gammak_full
  
  resolution<-1000000
  lambda_vec<-pi*(0:resolution)/resolution
  
  nu_vec<-crit_vec_full<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
  # Unit roots i.e. |nu|<2   
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec_full[i]<-optobj$crit
    nu_vec[i]<-optobj$nu
  } 

    
# 3. Here we search for nu in vicinity of the large singular peaks as well as to the right (down-swing to the right of peak) of the singular peaks: 
  #   -In each of the possible peaks (on the down-swing) we look at nu (or lambda) such that lag-one acf is closest to ht rho1
  #   -We look at the right half of the peaks because they provide minimally flatter (read: better) AR(2)-filter and therefore minimally better criterion value
  which_best<-rep(NA,larg+1)
  for (i in 1:larg)#i<-2
  {
    if (F)
    {
  # Vicinity of i-th singular peak (see plot below): left half and right-halves (less good/optimal)  
      scan_vec<-i*resolution/(L+1)+((-resolution/(2*(L+1))):(resolution/(2*(L+1))))
    }
  # Vicinity of i-th singular peak (see plot below): only left half (right-half is ignored): note that this distinction (left/half) is irrelevant asymptotically... 
    scan_vec<-i*resolution/(L+1)+((-resolution/(2*(L+1))):0)
# Select nu so that 1) holding-time is met and 2) acf is closest to rho1 
    if (min(abs(crit_vec_full[scan_vec]-rho1))<1/1000)
    {
      which_best[i]<-scan_vec[which(abs(crit_vec_full[scan_vec]-rho1)==min(abs(crit_vec_full[scan_vec]-rho1)))]
    } 
  }
  # Same as above but to the right of the singular peaks
  i<-larg+1
  scan_vec_l<-((scan_vec[length(scan_vec)]+1)+resolution/(2*(L+1))):resolution
  which_best[i]<-scan_vec_l[which(abs(crit_vec_full[scan_vec_l]-rho1)==min(abs(crit_vec_full[scan_vec_l]-rho1)))]
# Remove NAs
  which_best<-which_best[!is.na(which_best)]
  # Plot
  plot(x=nu_vec,y=crit_vec_full,main="Lag-one acf as a function of nu",ylab="rho",xlab="nu",type="l",lwd=1)
  abline(v=nu_vec[which_best],col="red",lty=3)
  abline(h=rho1,col="green",lty=3)
  lines(x=nu_vec,y=crit_vec_bandlimited,col="blue",lty=2)
  nu_opt_vec<-nu_vec[which_best]
  
  
  # For each of the above optima: compute b, rho and citerion value
  crit_val<-1:length(nu_opt_vec)
  b_mat<-NULL
  for (i in 1:length(nu_opt_vec))
  {
    
    Nu_opt<-2*M-nu_opt_vec[i]*diag(rep(1,L))
    b_opt<-solve(Nu_opt)%*%gammak#ts.plot(b)  eigen(Nu)$values
    b_opt<-b_opt/as.double(sqrt((t(b_opt)%*%b_opt)))
    rho_opt<-t(b_opt)%*%M%*%b_opt
    if (t(gammak)%*%b_opt<0)
    {
      b_opt<--b_opt
    }
    b_mat<-cbind(b_mat,b_opt)
    # Should be nearly vanishing if resolution large
    rho1-rho_opt
    crit_val[i]<-round(t(gammak)%*%b_opt,3)
    # Criterion value: gammak and b_opt are normalized
    print(paste("Nu: ",round(nu_opt_vec[i],3),", criterion: ",round(t(gammak)%*%b_opt,3),", rho:", round(rho_opt,3),sep=""))
    ts.plot(cbind(gammak,b_opt),col=c("black","blue"))
  }
  
  # Generate pdf for latex file  
  if (F)
  {
    file<-"rho_nu_bandlimited.pdf"
    pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
    par(mfrow=c(2,2))
    colo<-c("black","brown","red","violet","orange")
    # Plot: selected local optima correspond to red vertical lines
    plot(x=0:(L-1),y=gammak_bandlimited+0.05,main="Targets",ylab="",xlab="Lag",type="l",lwd=1,col="blue")
    lines(x=0:(L-1),y=gammak_full)
    lines(x=0:(L-1),y=gammak_full-gammak_bandlimited,col=colo[5])
    mtext(at=L/2,"Original target",line=-1,col="blue")
    mtext(at=L/2,"Completed target",line=-2,col=colo[1])
    mtext(at=L/2,"Difference",line=-3,col=colo[5])

    plot(x=nu_vec,y=crit_vec_full,main="Lag-one acf",ylab="",xlab="nu",type="l",lwd=1)
    abline(v=nu_vec[which_best[1:(length(which_best)-1)]],col=c(colo[2:length(colo)],"black"),lty=3,lwd=1)
    abline(h=rho1,col="green",lty=2,lwd=1)
    lines(x=nu_vec,y=crit_vec_bandlimited,col="blue")
    for (i in 1:(length(which_best)-1))
      mtext(at=nu_vec[which_best[i]+resolution/10],paste("Criterion: ",crit_val[i],sep=""),line=-i,side=1,col=c(colo[2:length(colo)],"black")[i])
#      mtext(at=nu_vec[which_best[i]],paste("Criterion: ",crit_val[i],sep=""),line=-i,side=1,col=c(colo[2:length(colo)],"black")[i])
    plot(x=0:(L-1),y=gammak_bandlimited,type="l",xlab="Lag",ylab="",main="SSA-solution")
    for (i in 1:1)
    {  
      lines(x=0:(L-1),y=b_mat[,i],col=colo[i+1])
      mtext(at=5,paste("Criterion: ",crit_val[i],sep=""),line=-1,col=colo[i+1])
      print(b_mat[,i]%*%b_mat[,i])
    }
    plot(x=0:(L-1),y=gammak_bandlimited,type="l",xlab="Lag",ylab="",main="Second and third best")
    for (i in 2:(ncol(b_mat)-1))
    {  
      lines(x=0:(L-1),y=b_mat[,i],col=colo[i+1])
      mtext(at=5,paste("Criterion: ",crit_val[i],sep=""),col=colo[i+1],line=-i+1)
      print(b_mat[,i]%*%b_mat[,i])
    }

    dev.off()
  }
} else
{
  L<-10
  larg<-3
# Epsilonh=0 for band-limited target
  epsilonh<-0.00
  w_bandlimited<-c(rep(epsilonh,larg),rep(1,L-larg))
  w_bandlimited<-w_bandlimited/sqrt(as.double(t(w_bandlimited)%*%w_bandlimited))
# Epsilon 10^{-3} for completed target: leads to very good approximation of band-limited gammak and steep/narrow peaks at singularities, see plot below
  epsilon<-0.001
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  rho1<-eigen(M)$values[larg]-0.05
  

}


@

<<label=init,results=hide>>=
# Second example: as above but now rho1 is attainable by bandlimited target but completed b outperforms original b
if (recompute_calculations==T)
{
  ##############################################################################
  ##############################################################################
  opt_b<-function(lambda)
  {  
    nu<-Re(lambda+1/lambda)+0.000001#nu<--2  lambda<-0.5
    Nu<-2*M-nu*diag(rep(1,L))
    b<-solve(Nu)%*%gammak#ts.plot(b)  eigen(Nu)$values
    rho<-t(b)%*%M%*%b/(t(b)%*%b)
    crit<-as.double(rho)
    return(list(crit=crit,nu=nu))
  }

  # Solutions when gammak full spectrum \to gammak reduced spectrum i.e. w_i\to 0
  len<-L<-10
  set.seed(1)
  
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  
# 1. Band-limited target
# 1.1 We skip the first/largest eigenvalue for band-limited target
  larg<-3
  # Epsilon=0 for band-limited target
  epsilon<-0.00
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  
  gammak<-gammak_bandlimited<-eigen(M)$vector%*%w
  ts.plot(gammak_bandlimited)
  
  ts.plot(eigen(M)$vector[,4])
  
  eig<-eigen(M)
  smallest_eigen_gammak<-eig$values[L]
  largest_eigen_gammak<-eig$values[larg+1]
  
#---------------
# 1.2 compute lag-one acf of band-limited target

  resolution<-1000000
  lambda_vec<-pi*(0:resolution)/resolution
  
  crit_vec_bandlimited<-crit_vec<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
  # Unit roots i.e. |nu|<2   
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec_bandlimited[i]<-optobj$crit
  } 
  
# 2. Augmented full-bandwith target
# 2.1 Epsilon 10^{-3} leads to very good approximation of band-limited gammak and steep/narrow peaks at singularities, see plot below
  epsilon<-0.001
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  
  gammak<-gammak_full<-eigen(M)$vector%*%w
  ts.plot(gammak_full)
  
  eig<-eigen(M)
  smallest_eigen_gammak<-eig$values[L]
  largest_eigen_gammak<-eig$values[larg+1]
  
#---------------
# 2.2 Seek optimal nu for rho1 large
# Select rho1 slightly below largest possible eigenvalue
  max(crit_vec_bandlimited)
# This one is slightly below the highest attainable rho of the band-limited design  
  rho1<-eig$values[larg+1]-0.31
  rho1<-eig$values[larg+1]-0.11
# This one is above the highest attainable rho and requires band-extension by point-mass at longer rhos  
  rho1<-eig$values[larg+1]-0.05
  largest_eigen_gammak
  # Compare with lag-one acf of (normalized) target
  t(gammak_full)%*%M%*%gammak_full
  
  resolution<-1000000
  lambda_vec<-pi*(0:resolution)/resolution
  
  nu_vec<-crit_vec_full<-rep(NA,length(lambda_vec))
  for (i in 1:length(lambda_vec))#i<-10000
  {
  # Unit roots i.e. |nu|<2   
    lambda<-exp(1.i*lambda_vec[i])
    optobj<-opt_b(lambda)
    crit_vec_full[i]<-optobj$crit
    nu_vec[i]<-optobj$nu
  } 

    
# 3. Here we search for nu in vicinity of the large singular peaks as well as to the right (down-swing to the right of peak) of the singular peaks: 
  #   -In each of the possible peaks (on the down-swing) we look at nu (or lambda) such that lag-one acf is closest to ht rho1
  #   -We look at the right half of the peaks because they provide minimally flatter (read: better) AR(2)-filter and therefore minimally better criterion value
  which_best<-rep(NA,larg+2)
  for (i in 1:larg)#i<-2
  {
    if (F)
    {
  # Vicinity of i-th singular peak (see plot below): left half and right-halves (less good/optimal)  
      scan_vec<-i*resolution/(L+1)+((-resolution/(2*(L+1))):(resolution/(2*(L+1))))
    }
  # Vicinity of i-th singular peak (see plot below): only left half (right-half is ignored): note that this distinction (left/half) is irrelevant asymptotically... 
    scan_vec<-i*resolution/(L+1)+((-resolution/(2*(L+1))):0)
# Select nu so that 1) holding-time is met and 2) acf is closest to rho1 
    if (min(abs(crit_vec_full[scan_vec]-rho1))<1/1000)
    {
      which_best[i]<-scan_vec[which(abs(crit_vec_full[scan_vec]-rho1)==min(abs(crit_vec_full[scan_vec]-rho1)))]
    } 
  }
  # Same as above but to the left of the singular peaks: take the two intersections of incompleted with holding-time line
  i<-larg+1
  scan_vec_l<-(((scan_vec[length(scan_vec)]+1)+resolution/(2*(L+1)))/2):resolution
  ret<-abs(crit_vec_full[scan_vec_l]-rho1)
# Second smallest: two intersections  
  min_ret<-ret[order(ret)][2]
  which_best[i:(i+1)]<-scan_vec_l[which(ret<=min_ret)]

  
# Remove NAs
  which_best<-which_best[!is.na(which_best)]
  # Plot
  plot(x=nu_vec,y=crit_vec_full,main="Lag-one acf as a function of nu",ylab="rho",xlab="nu",type="l",lwd=1)
  abline(v=nu_vec[which_best],col="red",lty=3)
  abline(h=rho1,col="green",lty=3)
  lines(x=nu_vec,y=crit_vec_bandlimited,col="blue",lty=2)
  nu_opt_vec<-nu_vec[which_best]
  
  
  # For each of the above optima: compute b, rho and citerion value
  crit_val<-1:length(nu_opt_vec)
  b_mat<-NULL
  for (i in 1:length(nu_opt_vec))
  {
    
    Nu_opt<-2*M-nu_opt_vec[i]*diag(rep(1,L))
    b_opt<-solve(Nu_opt)%*%gammak#ts.plot(b)  eigen(Nu)$values
    b_opt<-b_opt/as.double(sqrt((t(b_opt)%*%b_opt)))
    rho_opt<-t(b_opt)%*%M%*%b_opt
    if (t(gammak)%*%b_opt<0)
    {
      b_opt<--b_opt
    }
    b_mat<-cbind(b_mat,b_opt)
    # Should be nearly vanishing if resolution large
    rho1-rho_opt
    crit_val[i]<-round(t(gammak)%*%b_opt,3)
    # Criterion value: gammak and b_opt are normalized
    print(paste("Nu: ",round(nu_opt_vec[i],3),", criterion: ",round(t(gammak)%*%b_opt,3),", rho:", round(rho_opt,3),sep=""))
    ts.plot(cbind(gammak,b_opt),col=c("black","blue"))
  }
  
  # Generate pdf for latex file
  if (F)
  {
    file<-"rho_nu_bandlimited_ex2.pdf"
    pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
    par(mfrow=c(1,2))
    colo<-c("black","brown","red","violet","orange")
    # Plot: selected local optima correspond to red vertical lines
    plot(x=nu_vec,y=crit_vec_bandlimited,main="Incomplete",ylab="",xlab="nu",type="l",lwd=1,col="blue",ylim=c(-1,1))
    abline(v=nu_vec[which_best[4:(length(which_best))]],col=c(colo[5:length(colo)],"darkgreen"),lty=3,lwd=1)
    abline(h=rho1,col="green",lty=2,lwd=1)
    abline(h=0.6,col="green",lty=1,lwd=1)
    for (i in 4:(length(which_best)))
      mtext(at=nu_vec[which_best[i]],crit_val[i],line=-i,side=1,col=c(colo[2:length(colo)],"darkgreen")[i])

    # Plot: selected local optima correspond to red vertical lines
    plot(x=nu_vec,y=crit_vec_full,main="Completed",ylab="",xlab="nu",type="l",lwd=1)
    abline(v=nu_vec[which_best[1:(length(which_best))]],col=c(colo[2:length(colo)],"darkgreen"),lty=3,lwd=1)
    abline(h=rho1,col="green",lty=2,lwd=1)
    abline(h=0.6,col="green",lty=1,lwd=1)
    for (i in 1:(length(which_best)))
      mtext(at=nu_vec[which_best[i]],crit_val[i],line=-i,side=1,col=c(colo[2:length(colo)],"darkgreen")[i])
    dev.off()

  }
} else
{
  L<-10
  larg<-3
# Epsilonh=0 for band-limited target
  epsilonh<-0.00
  w_bandlimited<-c(rep(epsilonh,larg),rep(1,L-larg))
  w_bandlimited<-w_bandlimited/sqrt(as.double(t(w_bandlimited)%*%w_bandlimited))
# Epsilon 10^{-3} for completed target: leads to very good approximation of band-limited gammak and steep/narrow peaks at singularities, see plot below
  epsilon<-0.001
  w<-c(rep(epsilon,larg),rep(1,L-larg))
  w<-w/sqrt(as.double(t(w)%*%w))
  M<-matrix(nrow=L,ncol=L)
  M[L,]<-rep(0,L)
  M[L-1,]<-c(rep(0,L-1),0.5)
  for (i in 1:(L-2))
    M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
  M<-M+t(M)
  
  eigen(M)$values
  rho1<-eigen(M)$values[larg+1]-0.05
# Compute N for nu_i0=lambda_10, see theorem in paper for derivations  
  nu_i0<-2*eigen(M)$values[1]
  M1<-sum((w[2:L]*eigen(M)$values[2:L])^2/(2*eigen(M)$values[2:L]-nu_i0)^2)
  M2<-sum((w[2:L])^2/(2*eigen(M)$values[2:L]-nu_i0)^2)
  N<-(rho1*M2-M1)/(eigen(M)$values[1]-rho1)
# Check: should vanish
  rho1-(M1+eigen(M)$values[1]*N)/(M2+N)
}


@
In order to illustrate the case of incomplete spectral support addressed by corollary \ref{incomplete_spec_sup}  we here consider a simple nowcast example (forecast horizon $\delta=0$) based on a band-limited target $\boldsymbol{\gamma}_{0}$ of length $L=\Sexpr{L}$ 
\[
\boldsymbol{\gamma}_{0}=\sum_{i=1}^{10}w_i\mathbf{v}_i
\]
where $\mathbf{v}_i$ are the eigenvectors of the $10*10$-dimensional $\mathbf{M}$ and where the last three  weights in the spectral decomposition vanish, $w_{8}=w_9=w_{10}=0$ ($m=7$ in \ref{specdec}), and the first seven weights are constant $w_i=\Sexpr{round(w_bandlimited[10],3)}$, $i=1,...,7$
\[
\boldsymbol{\gamma}_{0}=\sum_{i=1}^{7}\Sexpr{round(w_bandlimited[10],3)}\mathbf{v}_i
\]
%The lag-one autocorrelation of the potential solution $\mathbf{b}$ given by \ref{diff_non_home} is then bounded by the largest eigenvalue $\lambda_i$ of $\mathbf{M}$ whose weight $w_i$ does not vanish i.e. $\lambda_7=\Sexpr{round(eigen(M)$values[4],3)}$, which would be obtained by assigning point-mass to $\lambda_7$ by selecting $\nu\approx 2\lambda_7=\Sexpr{2*round(eigen(M)$values[4],3)}$. We now impose a larger $\rho_1=\Sexpr{round(rho1,3)}$ in the holding-time constraint and complete 'almost imperceptibly' $\boldsymbol{\gamma}_{\delta}$ with the missing eigenvectors of the roots $\lambda_8,\lambda_9,\lambda_{10}$ by selecting a small $\epsilon=\Sexpr{epsilon}$ and setting $\tilde{w}_i=\epsilon$, for $i=8,9,10$ to obtain the full-band normalized target $\boldsymbol{\gamma}_{\delta}(\epsilon):=\displaystyle{\frac{\boldsymbol{\gamma}_{\delta}+0.001\sum_{i=8}^{10}\mathbf{v}_i}{\sqrt{1+3\cdot\Sexpr{epsilon}^2}}}$: for $|\epsilon|$ sufficiently small, band-limited and augmented full-band targets cannot be distinguished by (nearly) all practical means, see 
The left panel in fig. \ref{rho_nu_bandlimited_ex2} displays the lag-one acf \ref{sefrhobnotcomp} %\ref{sefrhobnotcomp} 
of $\mathbf{b}(\nu)$ %$\mathbf{b}_{\nu_{i_0}}$ 
given by \ref{diff_non_home_singular} %\ref{bnotcomp} 
as a function of $\nu\in [-2,2]-\{2\lambda_i, i=1,...,L\}$, thus omitting all potential singularities at $\nu=2\lambda_i$, $i=1,...,L$; the right panel displays additionally the lag-one acf \ref{sefrhobcomp} of the extension $\mathbf{b}_{i_0}(\tilde{N}_{i_0})$ in  \ref{b_new_comp}, when $\nu=\nu_{i_0}=2\lambda_{i_0}$ for $i_0=8,9,10$, where the three additional (vertical black) spectral lines, corresponding to $\mathbf{v}_{8},\mathbf{v}_{9},\mathbf{v}_{10}$, show the range of acf-values as a function of $\tilde{N}_{i_0}\in\mathbb{R}$: lower and upper bounds of each spectral line correspond to $\rho_{i_0}(0)=\rho_{\nu_{i_0}}=\frac{M_{i_01}}{M_{i_02}}$, when $\tilde{N}_{i_0}=0$ in \ref{sefrhobcomp}, and $\rho_{i_0}(\pm\infty)=\lambda_{i_0}$, when $\tilde{N}_{i_0}=\pm\infty$. The green horizontal lines in both graphs correspond to two different arbitrary holding-times $\rho_1=0.6$ and $\rho_1=\Sexpr{round(rho1,3)}$: the intersections of the latter with the acfs, marked by colored vertical lines in each panel, indicate potential solutions of the SSA-problem for the thusly specified  holding-time constraint. The corresponding criterion values are reported at the bottom of the colored vertical lines: the SSA-solution is determined by the intersection which leads to the highest criterion value (rightmost in this example). %Note also that the acf in the left panel can be replicated in the right panel by setting $\tilde{N}_{i_0}=0$ for any of $i_0=8,9,10$.  
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file<-"rho_nu_bandlimited_ex2.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=2in, width=5in]{", file, "}\n",sep = "")
cat("\\caption{Lag-one autocorrelation  as a function of $\\nu$. Original (incomplete) solutions (left panel) vs. completed solutions (right-panel). Intersections of the acf with the two green lines are potential solutions of the SSA-problem for the corresponding holding-times: criterion values are reported for each intersection ( bottom right).", sep = "")
cat("\\label{rho_nu_bandlimited_ex2}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The right panel in the figure illustrates that the completion with the extensions $\mathbf{b}_{i_0}(\tilde{N}_{i_0})$ at the singular points $\nu=\nu_{i_0}=2\lambda_{i_0}$ for $i_0=8,9,10$ can accommodate for a wider range of holding-time constraints, such that $|\rho_1|<\rho_{max}(L)=\lambda_{10}=\Sexpr{round(eigen(M)$values[1],3)}$; in contrast, $\mathbf{b}(\nu)$ in the left panel is limited to $\Sexpr{round(eigen(M)$values[L],3)}=\lambda_1<\rho_1<\lambda_7=\Sexpr{round(eigen(M)$values[larg+1],3)}$ so that there does not exist a solution for $\rho_1=0.6$ (no intersection with upper green line in left panel). Moreover, for a given holding-time constraint, the additional stationary points corresponding to intersections at the spectral lines of the (completed) acf might lead to improved performances, as shown in the right panel, where the maximal criterion value \[
\Big(\mathbf{b}_{i_0}(\tilde{N}_{i_0})\Big)'\boldsymbol{\gamma}_{\delta}=\Big(\mathbf{b}_{10}(\Sexpr{round(N,3)})\Big)'\boldsymbol{\gamma}_{0}=0.737
\] 
is attained at the right-most spectral line, for $i_0=10$, and where $\tilde{N}_{10}=\Sexpr{round(N,3)}$ has been obtained from \ref{N_comp}, with the correct signs of $D$ and $\tilde{N}_{10}$ in place. 




\section{Application to Business-Cycle Analysis}\label{zcc-criter}\label{busi_cyc}


We here apply the SSA-design to monthly industrial production indices of various countries with long business-cycle histories  and benchmark performances against the HP-filter, see Hodrick and Prescott (1997). We refer to the two-sided symmetric HP-filter as the 'target' $z_t$ which must be nowcasted at the current sample-end i.e. $\delta_0=0$. For that purpose, we consider five concurrent designs, namely two classic HP-filters, HP-gap and HP-trend, as well as three SSA-designs based on distinct hyper-parameter settings for $\rho_1,\delta$, see section \ref{hp_f} for reference. %which emphasize various research priorities %The latter is subject to a stronger holding-time constraint, which limits the occurrence of (noisy) alarms, 
%and we then compare frequencies and timings of zero-crossings. % of these concurrent filters. 
Our implementation of SSA in this application emphasizes simplicity and robustness: in particular, we do not fit models to the data, assuming log-returns to be white noise and accepting the deliberate misspecification as a tradeoff for simplicity; the same filters are applied to all countries; moreover, SSA must at least equal the benchmarks in terms of smoothness or noise-rejection i.e. we emphasize  reliability over timeliness. Summing-up, our implementation illustrates the possibility of modifying an existing benchmark in view of emphasizing alternative research- or user-priorities.%\footnote{One just needs the MA-weights of the corresponding MSE-predictor and suitable settings of the hyper-parameters.}.   
 

\subsection{SSA- and Hodrick Prescott Filters}\label{hp_f}


The HP filter is widely used to estimate trends and cycles of economic time series. It 
can be interpreted as an optimal MSE-signal extraction filter for the trend in the smooth trend model, see Harvey (1989). Conceptually, this results in 'implied' models for the cycle and the trend, such that applying the HP filter results in MSE optimal estimates. In this framework, the bi-infinite symmetric expansion of the filter is obtained as 
\begin{equation}\label{hp_eq_tc}
(\gamma_{|k|}B^k)_{|k|<\infty}=\frac{1}{1+\lambda(1-B)^2(1-B^{-1})^2}
\end{equation}
where $\lambda$ is a 'smoothing' hyperparameter and where $B,B^{-1}$ are backward and forward operators. The implicit data-generating process is an ARIMA(0,2,2) whose MA-coefficients are determined by $\lambda$, see  McElroy (2006).  In finite samples, the filter behaves differently in the middle or toward the  boundaries of the data, where the symmetry is lost: an exact finite sample representation of the concurrent HP-trend filter, denoted as $b_k^{HP-trend}$, is derived in McElroy (2006). %We also analyze a conventional MSE-design $\boldsymbol{\gamma}_{\delta}$, based on a finite one-sided extract of \ref{hp_eq_tc}, whereby $\delta=0$ (nowcast). MSE and $b_k^{HP-trend}$ differ in their assumptions about the data-generating process: white noise for MSE vs. ARIMA(0,2,2) for HP (the integration order two of the HP-design sets constraints on amplitude and phase-lag functions at frequency zero, see e.g. McElroy and Wildi (2020)). 
In addition, we also consider the classic HP-{gap} filter 
\begin{eqnarray}\label{hpgap}
b_k^{gap}:=\left\{\begin{array}{ccc}1-b_0^{HP-trend}&~& k=0\\-b_k^{HP-trend}&,&k>0
\end{array}\right.
\end{eqnarray}
%Since all filter coefficients decay towards zero rapidly pace, we can select truncated finite length designs, $b_k^{HP-trend~ truncated}:=\left\{\begin{array}{cc}b_k^{HP-trend}&0\leq k<L\\ 0&k>L\end{array}\right.$, and similarly for the symmetric (target) filter, with $L$ sufficiently large, see fig.\ref{filters_hp}.  
While SSA- and HP-trend will be applied to \emph{differenced} data, the HP-gap is typically applied to data in levels. Therefore, % in order to proceed to meaningful comparisons of concurrent designs 
we here propose a modified gap-design  $b_k^{\Delta gap}$ such that
\[\sum_{k=0}^{L-1}b_k^{\Delta gap}\Delta x_{t-k}=\sum_{k=0}^{L-1}b_k^{gap}x_{t-k}\]
where $\Delta x_t$ are first differences of a time series $x_t$. Note that filter outputs of original and modified gap-filters are strictly identical but their input series differ.  
One can  verify that $b_k^{\Delta gap}=\sum_{i=1}^kb_i^{gap}$ and that the coefficients decay towards zero for increasing lag, see for example McElroy and Wildi (2020)\footnote{$b_k^{gap}$ is a bandpass design with the property that $\sum b_k^{gap}= 0$ which follows from the definition \ref{hpgap}. Therefore $b_k^{\Delta gap}=\sum_{i=1}^kb_i^{gap}\to 0$ for increasing $k$.}. \\%In the following, we will invariably refer to this modification of the classic HP-gap filter so that cross-comparisons of filter characteristics will be consistent and meaningful.\\

<<label=init,echo=FALSE,results=hide>>=
# The following file runs the newest exact SSA filter design and computes HP-target, HP-trend concurrent (ARIMA(0,2,2) model) and HP-gap.
#   -It reads data (provided by Simon)
#   -It computes all filters and applies to data
#   -It could account for MA(1) structure in data (though this feature is ignored)

# 1. Load and select data
data_obj<-data_load_func(path.data)
# Data sent by Simon: log-returns, data from FRED
# Remove fourth series (INDPRO not seasonally adjusted: all other series are adjusted)
indpro<-data_obj$indpro[,-4]
indpro_level<-data_obj$indpro_level
# Second data sent by Simon: data from OECD, seasonally adjusted 
# Data is shorter than FRED: for countries appearing in both data sets the longer series in FRED are selected 
indpro_euh<-data_obj$indpro_eu
# Remove Spain and Japan which are contained in indpro (longer series there)
remove_series<-which(colnames(indpro_euh)%in%c("Japan","Spain"))
indpro_eu<-indpro_euh[,-remove_series]

# Select INDPRO and make xts object
select_series<-"US"
series_level<-indpro_level[,select_series]
series<-indpro[,select_series]
plot(series)

#----------------------
# 2. HP and hyperparameter
L<-200
lambda_monthly<-14400

HP_obj<-HP_target_mse_modified_gap(L,lambda_monthly)

target=HP_obj$target
hp_gap=HP_obj$hp_gap
modified_hp_gap=HP_obj$modified_hp_gap
hp_trend=HP_obj$hp_trend
hp_mse=HP_obj$hp_mse
#---------------------------
# 3. SSA and hyperparameters
# Holding time
ht<-12
forecast_horizon_vec<-c(0,18)
# White noise assumption: MA1_adjustment<-F (MA1_adjustment<-T is not used and should be checked) 
MA1_adjustment<-F
# Size of discret grid for computing nu
grid_size<-200

# Computations are done if recompute_calculations==T (takes a couple seconds). Otherwise saved coefficients are loaded from path.result
SSA_obj<-SSA_compute(ht,L,hp_mse,forecast_horizon_vec,MA1_adjustment,grid_size)

bk_mat<-SSA_obj$bk_mat

#mse_forecast<-c(hp_mse[(1+forecast_horizon_vec[2]):L],rep(0,forecast_horizon_vec[2]))
ht_short<-compute_holding_time_func(hp_trend)$ht

# Compute fast SSA-filter with same holding-time as HP-trend
SSA_obj<-SSA_compute(ht_short,L,hp_mse,forecast_horizon_vec[2],MA1_adjustment,grid_size)


bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)


#----------------------------
# 4. Filter and plot series
# 4.1 Filter
# Start date for plots
start_date<-"1970-01-01"
end_date<-NULL
colo_hp_all<-c("brown","red")
colo_SSA<-c("orange","blue","violet")
colo_all<-c(colo_hp_all,colo_SSA)


# 3.2 Specify filters: HP concurrent and SSA filters
if (F)
{  
  mse_forecast<-c(hp_mse[(1+forecast_horizon_vec[2]):L],rep(0,forecast_horizon_vec[2]))
  filter_mat<-cbind(hp_trend,hp_mse,modified_hp_gap,mse_forecast,bk_mat)
  colnames(filter_mat)<-c("HP trend","MSE","Modified gap","MSE-forecast",colnames(bk_mat))
}
filter_mat<-cbind(hp_trend,modified_hp_gap,bk_mat)
colnames(filter_mat)<-c("HP trend","Modified gap",colnames(bk_mat))
ts.plot(scale(filter_mat,center=F,scale=T),col=colo_all)

#   Filter data
filter_obj<-SSA_filter_func(filter_mat,L,series)

y_mat=filter_obj$y_mat
ts.plot(scale(y_mat,center=F,scale=T))

# number of crossings
number_cross_all_filters<-rep(NA,ncol(filter_mat))
names(number_cross_all_filters)<-colnames(filter_mat)
for (i in 1:ncol(y_mat))
{
  if (is.xts(y_mat))
  {  
    number_cross_all_filters[i]<-length(which(sign(y_mat[,i])!=sign(lag(y_mat[,i]))))
  } else
  {
    number_cross_all_filters[i]<-length(which(sign(y_mat[1:(nrow(y_mat)-1),i])!=sign(lag(y_mat[2:nrow(y_mat),i]))))
  }
}
number_cross_all_filters
#-------------
# 4.2 Plot 

plot_obj<-plot_paper(y_mat,start_date,end_date,colo_all)
  
q_gap=plot_obj$q_gap
q_trend=plot_obj$q_trend
x_trend=plot_obj$x_trend
y_trend=plot_obj$y_trend
x_gap=plot_obj$x_gap
y_gap=plot_obj$y_gap
# Plot great-lockdown (Pandemy)
start_date_covid<-"2019-01-01"
end_date_covid<-"2021-06-01"
plot_obj<-plot_paper(y_mat,start_date_covid,end_date_covid,colo_all)
  
q_trend_covid=plot_obj$q_trend
x_trend_covid=plot_obj$x_trend
y_trend_covid=plot_obj$y_trend
q_trend_covid
polygon(x_trend_covid, y_trend_covid, xpd = T, col = "grey",density=10)#
q_SSA_covid=plot_obj$q_SSA
x_SSA_covid=plot_obj$x_trend
y_SSA_covid=plot_obj$y_trend
q_SSA_covid
polygon(x_SSA_covid, y_SSA_covid, xpd = T, col = "grey",density=10)#
  
 
start_date_moderation_financial_1<-"1990-01-01"
end_date_moderation_financial_1<-"2002-01-01"
plot_obj<-plot_paper(y_mat,start_date_moderation_financial_1,end_date_moderation_financial_1,colo_all)
  
q_gap_great_moderation_1=plot_obj$q_gap
x_gap_great_moderation_1=plot_obj$x_trend
y_gap_great_moderation_1=plot_obj$y_trend

par(mfrow=c(1,1))
q_gap_great_moderation_1
polygon(x_gap_great_moderation_1, y_gap_great_moderation_1, xpd = T, col = "grey",density=10)#
  
start_date_moderation_financial_2<-"2001-01-01"
end_date_moderation_financial_2<-"2010-01-01"
plot_obj<-plot_paper(y_mat,start_date_moderation_financial_2,end_date_moderation_financial_2,colo_all)
  
q_gap_great_moderation_2=plot_obj$q_gap
x_gap_great_moderation_2=plot_obj$x_trend
y_gap_great_moderation_2=plot_obj$y_trend
par(mfrow=c(1,1))
q_gap_great_moderation_2
polygon(x_gap_great_moderation_2, y_gap_great_moderation_2, xpd = T, col = "grey",density=10)#


# Number recessions occurring after 1935
number_recessions_1935<-length(which(nberDates()[,"Start"]>as.double(substr("1935-01-01",1,4))*10000))
# length of INDPRO afer 1935
nrow(indpro["1935-01-01/"])
head(indpro["1935-01-01/"])
# Duration of complete recession/expansion cycles
cycle_length_NBER_1935<-nrow(indpro["1935-01-01/"])/number_recessions_1935
# Duration of complete recession/expansion cycles
cycle_length_NBER_start_date<-nrow(indpro[paste(start_date,"/",sep="")])/length(which(nberDates()[,"Start"]>as.double(substr(start_date,1,4))*10000))

#------------------------------------
# 5 Diagnostics real data: number of Crossings, peak correlation and shift (tau statistic)

# 5.1 SSA(12,18) vs HP trend
# Select competing series: reference filter/series first
#   Reference series determines crossings, see description of tau-statistic in paper (should be smoother)
#   Reference series determines sign of lead/lags of peak-correlation
# Note: should not be an xts-object!!!!
mplot<-as.matrix(y_mat[,c(4,1)])
length_series<-nrow(mplot)

# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F
# Skip shifts larger than outlier_limit in absolute value: useful when reference filter has additional crossings which do not correspond to contender
outlier_limit<-10

timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

cor_peak=timeliness_obj$cor_peak
tau_vec=timeliness_obj$tau_vec
tau_vec_adjusted=timeliness_obj$tau_vec_adjusted
tau=timeliness_obj$tau
tau_adjusted=timeliness_obj$tau_adjusted
t_test=timeliness_obj$t_test
t_test_adjusted=timeliness_obj$t_test_adjusted
number_cross=timeliness_obj$number_cross

tau
tau_adjusted
t_test
number_cross

# 5.2 SSA(12,18) vs HP gap
mplot<-as.matrix(y_mat[,c(4,2)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_gap=timeliness_obj$tau
tau_gap

# 5.3 SSA(12,18) vs SSA(12,0)
mplot<-as.matrix(y_mat[,c(4,3)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_slow=timeliness_obj$tau
tau_slow


# 5.4 SSA(12,18) vs SSA(7.66,18)
mplot<-as.matrix(y_mat[,c(4,5)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_fast=timeliness_obj$tau
tau_fast



#------------------------------------
# 6. Diagnostics Gaussian noise: number of Crossings, peak correlation and shift (tau statistic)
len<-100000
set.seed<-(46)
series_Gauss<-rnorm(len)

filter_obj<-SSA_filter_func(filter_mat,L,series_Gauss)

y_mat_Gauss=filter_obj$y_mat

# 6.1 SSA(12,0) vs HP trend
mplot_Gauss<-as.matrix(y_mat_Gauss[,c(3,1)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot_Gauss,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_Gauss_slow=timeliness_obj$tau

tau_Gauss_slow

# 6.2 SSA(12,18) vs HP trend
mplot_Gauss<-as.matrix(y_mat_Gauss[,c(4,1)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot_Gauss,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_Gauss_middle=timeliness_obj$tau

tau_Gauss_middle

# 6.3 SSA(7.66,18) vs HP trend
mplot_Gauss<-as.matrix(y_mat_Gauss[,c(5,1)])
# Max lead for peak-correlation
max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
last_crossing_or_closest_crossing<-F

timeliness_obj<-compute_timeliness_func(mplot_Gauss,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)

tau_Gauss_fast=timeliness_obj$tau

tau_Gauss_fast


ht_trend<-round(compute_holding_time_func(hp_trend)$ht,2)
@



According to Morten and Uhlig (2002), we select $\lambda=\Sexpr{as.integer(lambda_monthly)}$ (monthly data). For the holding-time, we select either $ht_1=\Sexpr{round(ht,0)}$, which matches roughly the mean-duration of recessions (see also the closing discussion in section \ref{app_indpro}) or $ht_1=$\Sexpr{ht_trend} which is the expected holding-time of the benchmark HP(trend)-filter, as based on \ref{ht}. %: in the first case, SSA has improved smoothing capability: in the second case, SSA replicates HP-trend in terms of smoothness but since the underlying model assumptions are different e.g. white noise vs. ARIMA(0,2,2), timeliness will differ, see table \ref{perf_zcc_gap_trend_mean} further down. 
Timeliness is addressed by selecting either $\delta=\Sexpr{forecast_horizon_vec[1]}$ (nowcast) or $\delta=\Sexpr{forecast_horizon_vec[2]}$ (forecast), see fig.\ref{lead_zcc_hp_trend} for further analysis and keep in mind that the proper target is a nowcast i.e. $\delta_0=0$ is fixed.  % and we can rely on corollary \ref{lambda_cor} for deriving the SSA-design (the HP-filter belongs to the class of eigenfunctions of the difference equation). 
<<label=init,echo=FALSE,results=hide>>=

file = "filters_hp_short.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
mplot<-scale(cbind(hp_trend,target,hp_gap,modified_hp_gap),center=F,scale=T)
colnames(mplot)<-c("HP trend","Target symmetric","HP-gap (original)","HP-gap (modified)")
colo<-c(colo_hp_all[1],"black","darkgreen",colo_hp_all[2])
plot(mplot[,1],main="HP-Filters",axes=F,type="l",xlab="Lag-structure",ylab="filter-coefficients",ylim=c(min(mplot),max(mplot)),col=colo[1])
for (i in 1:ncol(mplot))
{
  lines(mplot[,i],col=colo[i])
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}  
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()
# Select forecast horizons 0 and 18
select_vec<-1:3
mplot<-scale(bk_mat[,select_vec],center=F,scale=T)
plot(mplot[,1],main="SSA-Filters",axes=F,type="l",xlab="Lag-structure",ylab="filter-coefficients",ylim=c(min(mplot),max(mplot)),col=colo_SSA[1])
for (i in 1:ncol(mplot))
{
  lines(mplot[,i],col=colo_SSA[i])
  mtext(colnames(mplot)[i],col=colo_SSA[i],line=-i)
}  
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filters_hp_short.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{HP concurrent (left panel) and SSA concurrent filters (right panel). All filters are arbitrarily scaled to unit-variance.", sep = "")
cat("\\label{filters_hp}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The coefficients of the specified concurrent and target filters are displayed in fig.\ref{filters_hp}: all filters have length $L=\Sexpr{L}$ and are normalized to unit-variance ($\frac{1}{L}\sum_{k=0}^{L-1}(b_k-\overline{b})^2=1$). %The tails of the symmetric HP target (black) suggests evidence of a challenging prediction problem because the left tail reaches far into the future of a series. 
%The one-sided trend-MSE design (green) corresponds to the right half of the target, up to the arbitrary scaling. The difference between HP-trend and Trend-MSE are due to different model-assumptions: while the former assumes an ARIMA(0,2,2) data generating process, the latter relies on the white noise assumption: both are misspecified since log-returns of INDPRO are close to an MA(1) specification (in the following we ignore this topic which could be addressed formally by the extension proposed in section \ref{ext_stat}). The HP-gap corresponds to  the modification introduced in the previous section. 
The characteristic tips of the SSA-filters are indicative of the boundary constraint $b_{-1}=0$, see theorem \ref{lambda}. The first two (orange, blue) have identical holding-times (smoothness) but %one of them (blue) is a forecast while the other (orange) is a nowcast and we therefore 
we expect different lead-lag properties (timeliness); the third (violet) has a shorter holding-time matching HP-trend; also, \Sexpr{colnames(y_mat)[3]} is virtually indistinguishable from the MSE-estimate of the symmetric HP-target, not shown here, so that it may be considered as a third benchmark in our analysis. Fig.\ref{amp_shift_hp} compares amplitudes and phase-lags\footnote{The phase-lag at a given frequency $\omega$ measures the shift, in time-units, between output and input of the filter when fed with a sinusoidal of that frequency.} of all concurrent filters, except the original HP-gap which is discarded from further consideration when working with differenced data.
<<label=init,echo=FALSE,results=hide>>=


mat_coef_hp<-scale(cbind(hp_trend,modified_hp_gap),center=F,scale=T)
mat_coef_SSA<-scale(bk_mat[,select_vec],center=F,scale=T)

mat_amp_hp<-mat_shift_hp<-mat_amp_SSA<-mat_shift_SSA<-NULL
K<-600
for (i in 1:ncol(mat_coef_hp))
{
  tr_obj_hp<-amp_shift_func(K,mat_coef_hp[,i],F)
  mat_amp_hp<-cbind(mat_amp_hp,tr_obj_hp$amp)  
  mat_shift_hp<-cbind(mat_shift_hp,tr_obj_hp$shift)  
} 
colnames(mat_amp_hp)<-colnames(mat_shift_hp)<-colnames(mat_coef_hp)
for (i in 1:ncol(mat_coef_SSA))
{
  tr_obj_SSA<-amp_shift_func(K,mat_coef_SSA[,i],F)
  mat_amp_SSA<-cbind(mat_amp_SSA,tr_obj_SSA$amp)  
  mat_shift_SSA<-cbind(mat_shift_SSA,tr_obj_SSA$shift)  
} 
colnames(mat_amp_SSA)<-colnames(mat_shift_SSA)<-colnames(mat_coef_SSA)
colo_hp<-colo_hp_all[c(1,4)]

file = "amp_shift_hp_short.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
mplot<-scale(mat_amp_hp,center=F,scale=T)
#colnames(mplot)<-c("HP trend","HP-gap (modified)","MSE-forecast")

plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="",main=paste("Amplitude HP",sep=""),ylim=c(min(mplot),max(mplot)),col=colo_hp[1])
mtext(colnames(mplot)[1],col=colo_hp_all[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo_hp_all[i])
  mtext(colnames(mplot)[i],col=colo_hp_all[i],line=-i)
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()

mplot<-scale(mat_amp_SSA,center=F,scale=T)

plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="",main=paste("Amplitude SSA",sep=""),ylim=c(min(mplot),max(mplot)),col=colo_SSA[1])
mtext(colnames(mplot)[1],col=colo_SSA[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo_SSA[i])
  mtext(colnames(mplot)[i],col=colo_SSA[i],line=-i)
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()


mplot<-(mat_shift_hp)
#colnames(mplot)<-c("HP trend","Trend MSE","HP-gap (modified)","MSE-forecast")

mplot[1,]<-NA
# skip extreme values (larger than max of other shifts)
#ex_val<-max(na.exclude(mplot[,1:ncol(mplot)]))
#mplot[which(abs(mplot[,1])>ex_val),1]<-NA
plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="",main=paste("Phase-lag HP",sep=""),
     ylim=c(-1,max(na.exclude(mplot))),col=colo_hp_all[1])
mtext(colnames(mplot)[1],col=colo_hp_all[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo_hp_all[i])
  mtext(colnames(mplot)[i],col=colo_hp_all[i],line=-i)
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()

mplot<-(mat_shift_SSA)

mplot[1,]<-NA
# skip extreme values (larger than max of other shifts)
#ex_val<-max(na.exclude(mplot[,2:ncol(mplot)]))
#mplot[which(abs(mplot[,1])>ex_val),1]<-NA
plot(mplot[,1],type="l",axes=F,xlab="Frequency",ylab="",main=paste("Phase-lag SSA",sep=""),
     ylim=c(-1,max(na.exclude(mplot))),col=colo_SSA[1])
mtext(colnames(mplot)[1],col=colo_SSA[1],line=-1)
for (i in 2:ncol(mplot))
{
  lines(mplot[,i],col=colo_SSA[i])
  mtext(colnames(mplot)[i],col=colo_SSA[i],line=-i)
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()

invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "amp_shift_hp_short.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Amplitude and phase-lag functions of HP-gap (red), HP-trend (brown), SSA(12,0) (orange), SSA(12,18) (blue) and SSA(7.66,18) violet)", sep = "")
cat("\\label{amp_shift_hp}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The amplitude functions suggest that all filters, except HP-gap, are  lowpass designs; $b_k^{\Delta gap}$ is a bandpass %with vanishing amplitude at frequency zero\footnote{The original $b_k^{gap}$ has a zero of order two at frequency zero such that a zero of order one is left in $b_k^{\Delta gap}$.} 
which suppresses low-frequency content: this specific characteristic of HP-gap can be related to the phenomenon of so-called 'spurious cycles', see for example fig.\ref{business_cycle_trend_covid}, left panels. %SSA-nowcast (orange) and HP-MSE are similar because their holding-times are similar:   $ht^{MSE}=$\Sexpr{round(compute_holding_time_func(hp_mse)$ht,0)} vs. $ht_1=$\Sexpr{ht}. Therefore, the SSA-nowcast (orange) is redundant and will be skipped from further analysis. 
Amplitude functions of SSA with larger holding-times (orange and blue) are smallest at higher frequencies, due to stronger smoothing. %: a stronger suppression of high-frequency components warrants less frequent zero-crossings. 
The phase-lag of HP-gap is smallest overall; \Sexpr{colnames(filter_mat)[5]} (orange) outperforms HP-trend uniformly at business-cycle frequencies; \Sexpr{colnames(filter_mat)[4]}  outperforms HP-trend only at lower cycle-frequencies, see fig.\ref{lead_zcc_hp_trend}.  
<<label=init,echo=FALSE,results=hide>>=
file = paste("lead_zcc_hp_trend.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,1))
mplot<-cbind(mat_shift_hp[,1],mat_shift_SSA)
colnames(mplot)[1]<-colnames(mat_shift_hp)[1]
mplot[1,]<-NA
# skip extreme values (larger than max of other shifts)
ex_val<-max(na.exclude(mplot[,2:ncol(mplot)]))
mplot[which(abs(mplot[,1])>ex_val),1]<-NA
plot(mplot[,1]-mplot[,2],type="l",axes=F,xlab="Frequency",ylab="Lag                              Lead            ",main=paste("Lead/lag of SSA over HP-trend",sep=""),col=colo_SSA[1],ylim=c(-2,6))
for (i in 2:ncol(mplot))
{  
lines(mplot[,1]-mplot[,i],col=colo_SSA[i-1])
mtext(paste("Relative lead/lag of ",colnames(mplot)[1], " over ",colnames(mplot)[i],sep=""),col=colo_SSA[i-1],line=-i)
}
x<-c(as.integer(nrow(mplot)/(10*6)),as.integer(nrow(mplot)/(10*6)),as.integer(nrow(mplot)/(2*6)),as.integer(nrow(mplot)/(2*6)))
#y<-c(min(na.exclude(mplot[,2]-mplot[,3])),max(na.exclude(mplot[,2]-mplot[,3])),max(na.exclude(mplot[,2]-mplot[,3])),min(na.exclude(mplot[,2]-mplot[,3])))
y<-c(-2,6,6,-2)
polygon(x, y, xpd = T, col = "grey",density=10)#, lty = 2, lwd = 2, border = "red")
#mtext("Trend-cycle frequencies",col="grey",line=-3)
abline(h=0,lty=2)
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()



invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "lead_zcc_hp_trend.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Difference of phase-lags of SSA vs. HP-trend. Positive values signify a lead of SSA at the corresponding frequency. Business-cycle frequencies i.e. periodicities between two and ten years are highlighted in the shaded area.", sep = "")
cat("\\label{lead_zcc_hp_trend}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
%Although the figure suggest that HP-trend is not outperformed uniformly, in terms of phase-lag at business-cycle frequencies, 
An application of SSA and HP-trend to simulated Gaussian noise leads to mean-shift or $\tau$-statistics summarized in table \ref{perf_zcc_gap_trend_sh} (the mean-shift or $\tau$-statistic is discussed in the appendix: it measures the shift of two competing filter-outputs at zero-crossings and a positive mean-shift implies a corresponding lead or left-shift of the reference-filter, averaged over all crossings).  
<<label=init,echo=FALSE,results=hide>>=
# Compute empirical mean shift: lead/lag
mat_re<-matrix(rbind(as.integer(c(0,round(tau,0),round(tau_gap,0),round(tau_slow,0),round(tau_fast,0))),as.integer(c(number_cross_all_filters[c("SSA(12,18)","HP trend","Modified gap","SSA(12,0)","SSA(7.66,18)")]))),ncol=5)
colnames(mat_re)<-c("SSA(12,18)","HP trend","Modified gap","SSA(12,0)","SSA(7.66,18)")
rownames(mat_re)<-c("Mean-shift (Tau-statistic)","Number of crossings")
#save(mat_re_gap_trend_zcc,file=paste(path.result,"mat_re_gap_trend_zcc"))
index(indpro)[1]
index(indpro)[nrow(indpro)]
mat_sh<-matrix(c(round(tau_Gauss_slow,0),round(tau_Gauss_middle,0),round(tau_Gauss_fast,0)),nrow=1)
colnames(mat_sh)<-colnames(y_mat_Gauss)[c(3,4,5)]
rownames(mat_sh)<-"Mean-shift Tau-statistic"
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
xtable(mat_sh, dec = 1,dig=1,
paste("Mean-shift (tau-statistic) of HP-trend as referenced against SSA based on an application to Gaussian white noise: positive numbers suggest a lead or left-shift by the corresponding SSA-design"),
label=paste("perf_zcc_gap_trend_sh",sep=""),
center = "centering", file = "", floating = FALSE)
@
%$\tau$(\Sexpr{colnames(y_mat_Gauss)[3]},HP-trend)=\Sexpr{round(tau_Gauss_slow,0)}, $\tau$(\Sexpr{colnames(y_mat_Gauss)[4]},HP-trend)=\Sexpr{round(tau_Gauss_middle,0)} and $\tau$(\Sexpr{colnames(y_mat_Gauss)[5]},HP-trend)=\Sexpr{round(tau_Gauss_fast,0)}. %; but an application to the US-INDPRO series reveals a slight lead $\tau$(SSA,HP-trend)=\Sexpr{round(tau,0)} which might be due to the fact that log-returns of the series are positively autocorrelated (stronger low-frequency content tends to favor the SSA-filter, recall fig.\ref{lead_zcc_hp_trend}). 
In summary, HP-gap is expected to lead systematically all lowpass designs, but the eventuality of spurious cycles and zero-crossings could affect the analysis; SSA emphasizes three different research priorities whereby HP-trend is at least equaled in terms of holding-time. We now verify the established diagnostics based on empirical data and effective filter-outputs. A detailed analysis of the US business-cycle  is proposed in section \ref{app_indpro} and summary-statistics for a selection of additional countries are provided in section \ref{multi_nat}.%The following section verifies these assumptions.\\













%, as  displayed in fig.\ref{amp_shift_hp}, which will be thoroughly analyzed further down. %: the latter  suggest that the concurrent {trend} estimates are lowpass designs whereas the HP-gap is a bandpass (vanishing amplitude at frequency zero). By altering low-frequency content, the gap filter is likely to introduce spurious cycles and zero-crossings which potentially disturb a coherent analysis of the underlying time series dynamics. 

\subsection{Application to the US-Industrial Production Index}\label{app_indpro}


We consider an application of the proposed concurrent filters to first differences $\Delta I_t$ of the (log-transformed) monthly US industrial production index $I_t$ plotted in fig.\ref{z_us_log_indpro}: the series effectively starts in \Sexpr{index(indpro)[1]} (FRED database) but we display shorter samples for ease of visual inspection. %Applications to a selection of other countries are summarized in section \ref{multi_nat}. % Since the HP-gap filter, with coefficients $b_k^{gap}$ defined above, is typically applied to (un-differenced) $I_t$, we here propose an alternative representation suitable for first differences $\Delta I_t$ with coefficients $b_k^{\Delta gap}$ such that
<<label=init,echo=FALSE,results=hide>>=
file = paste("z_us_log_indpro.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
x<-nber_dates_polygon(start_date,series_level)$x
y<-nber_dates_polygon(start_date,series_level)$y
# Adjust rectangles of shaded recession episodes to minimal series value
y[which(y==min(y))]<-min(series_level[paste(start_date,"/",sep="")])
#x<-c(start_date[33],start_date[33],end_date[33],end_date[33])
#y<-c(min(level),max(level),max(level),min(level))
plot((series_level[paste(start_date,"/",sep="")]),#ylim=c(min(series_level[paste(start_date,"/",sep="")]),max(series_level[paste(start_date,"/",sep="")])),
     plot.type='s',col="black",ylab="",main="US industrial production index ")
polygon(x, y, xpd = T, col = "grey",density=10)#, lty = 2, lwd = 2, border = "red")
invisible(dev.off())
@
<<label=z_us_real_log_gdp.pdf,echo=FALSE,results=tex>>=
  file = paste("z_us_log_indpro.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Monthly US industrial production index (INDPRO) and recession episodes as dated by the National Bureau of Economic Research, NBER (shaded)", sep = "")
  cat("\\label{z_us_log_indpro}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
<<label=init,echo=FALSE,results=hide>>=
# Load full data-set
file = paste("business_cycle_gap_trend_short.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
q_gap
polygon(x_gap, y_gap, xpd = T, col = "grey",density=10)#
q_trend
polygon(x_trend, y_trend, xpd = T, col = "grey",density=10)#
invisible(dev.off())


@
<<label=z_us_real_log_gdp.pdf,echo=FALSE,results=tex>>=
file = paste("business_cycle_gap_trend_short.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter-outputs: HP-gap (red) and SSA(12,18) (blue) in the top panel; HP-trend (brown) and SSA(12,18) (blue) in the bottom panel with shaded recession episodes.", sep = "")
  cat("\\label{business_cycle_trend}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
%As seen in the previous fig.\ref{z_us_log_indpro}, this episode corresponds to a period of weaker mean-growth of the Index, which is likely representative of the foreseeable future, and with correspondingly reduced signal to noise ratio, complicating additionally the real-time forecast exercise. 
Filter-outputs, scaled to unit-variance, are displayed in fig.\ref{business_cycle_trend}. 
Zero-crossings of the filters are marked by vertical lines with matching colors (overlaps are possible). The figure illustrates that the bandpass HP-gap (top panel) generates excessively many crossings, see table \ref{perf_zcc_gap_trend}, as well as false systematic sign changes, i.e. spurious cycles, during longer up-swings covering the great moderation, from the early nineties up to the financial crisis, see also fig.\ref{business_cycle_trend_covid}, left panels. In contrast, the lowpass designs %do not suppress low-frequency content and 
track the longer cycle-dynamics better. All filters 
%The period extends from the beginning of the so-called dot-com recession, in early 2000, down to the Covid-crisis and refers to a longer period of weakening growth and consequently weaker signal-to-noise ratio which is likely more representative of the foreseeable future. 
indicate a slowdown of industrial production in 2015 and 2016, at a time when the price for crude oil declined sharply, hence affecting petrol extraction as well as collateral industrial activity in the US. A potential advantage of imposing stronger smoothness can be seen in fig.\ref{business_cycle_trend_covid}, right panels, which highlight the pandemic 'great-lockdown' crisis: zero-crossings of the SSA-design are fewer and dynamics are less noisy than the benchmark, which facilitates a real-time assessment of economic conditions, at least in the industrial sector. The bottom-right panel illustrates the potential right-shift or lag of \Sexpr{colnames(y_mat)[3]} at zero-crossings, as measured by $\tau$ in the appendix. Table  \ref{perf_zcc_gap_trend} compares timeliness and smoothness performances in terms of $\tau$ and number of sign changes: HP-gap and \Sexpr{colnames(y_mat)[5]} outperform in terms of timeliness followed by \Sexpr{colnames(y_mat)[4]}, HP-trend and \Sexpr{colnames(y_mat)[3]}; but the latter \Sexpr{colnames(y_mat)[3]} outperforms in terms of smoothness, followed by \Sexpr{colnames(y_mat)[4]}, \Sexpr{colnames(y_mat)[5]}, HP-trend and finally HP-gap. These rankings mostly conform with the diagnostics established in the previous section, based on amplitude and phase-lag functions. Based on theoretical as well as empirical evidences, we now discard HP-gap, subject to spurious cycles, as well as \Sexpr{colnames(filter_mat)[3]}, subject to a relative lag (recall that \Sexpr{colnames(filter_mat)[3]} is virtually indistinguishable from MSE here).
<<label=init,echo=FALSE,results=hide>>=
# Load full data-set
file = paste("business_cycle_trend_covid.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
q_gap_great_moderation_1
polygon(x_gap_great_moderation_1, y_gap_great_moderation_1, xpd = T, col = "grey",density=10)#
q_trend_covid
polygon(x_trend_covid, y_trend_covid, xpd = T, col = "grey",density=10)#
q_gap_great_moderation_2
polygon(x_gap_great_moderation_2, y_gap_great_moderation_2, xpd = T, col = "grey",density=10)#
q_SSA_covid
polygon(x_SSA_covid, y_SSA_covid, xpd = T, col = "grey",density=10)#
invisible(dev.off())

@
<<label=z_us_real_log_gdp.pdf,echo=FALSE,results=tex>>=
file = paste("business_cycle_trend_covid.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter-outputs: Modified HP-gap (red), HP-trend (brown), SSA(12,18) (blue) and SSA(12,0) (orange). From 1990-2002 (top left panel, great moderation), 2000-2010 (bottom left panel) and 2019-2021 (right panel: great lockdown). Zero-crossings are marked by corresponding vertical lines (overlaps are possible). ", sep = "")
  cat("\\label{business_cycle_trend_covid}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@



%The excess zero-crossings of HP-gap as well as a couple of connected intervals with wrong growth-signs in fig.\ref{business_cycle_trend} hamper a straightforward determination of sign-changes of the cycle, as anticipated by an analysis of amplitude functions in fig.\ref{amp_shift_hp} (bandpass vs. lowpass). Both lowpass designs in the bottom panel track upturns and downturns more reliably and the SSA-crossings are more timely (left-shifted) with fewer false alarms (less crossings). Note, however, that the modified gap-filter performs superiorly during the Covid-breakout, with faster signal-tracking than SSA-forecast and fewer false crossings than HP-trend. The key-statistics are summarized in table \ref{perf_zcc_gap_trend} and the reported mean-lead of the SSA-design at zero-crossings is consistent with the positive phase-lag differences at business-cycle frequencies in fig.\ref{lead_zcc_hp_trend} (shaded area). \\

%Our results suggest that improved smoothing-capability of SSA against HP(trend), in terms of fewer zero-crossings or 'false alarms', does not necessarily impair  relative timing-ability in this context, quite the contrary: alas, a formal treatment of the underlying tradeoff(s) would conflict with actual space limitations. Note also that our simplified SSA-design relies on the white noise assumption which is a misspecification in the presence of 'cycles'. Nonetheless, we here deliberately refrain from fine-tuning (and possibly overfitting) the design, based on the extension to stationary processes presented in section \ref{ext_stat}, in particular since the implicit ARIMA(0,2,2)-model of the competing HP-design is likely subject to misspecification, too.
<<label=init,echo=FALSE,results=hide>>=
# Compute empirical mean shift: lead/lag
mat_re<-matrix(rbind(as.integer(c(0,round(tau,0),round(tau_gap,0),round(tau_slow,0),round(tau_fast,0))),as.integer(c(number_cross_all_filters[c("SSA(12,18)","HP trend","Modified gap","SSA(12,0)","SSA(7.66,18)")]))),ncol=5)
colnames(mat_re)<-c("SSA(12,18)","HP trend","Modified gap","SSA(12,0)","SSA(7.66,18)")
rownames(mat_re)<-c("Mean-shift (Tau-statistic)","Number of crossings")
#save(mat_re_gap_trend_zcc,file=paste(path.result,"mat_re_gap_trend_zcc"))
index(indpro)[1]
index(indpro)[nrow(indpro)]
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
xtable(mat_re, dec = 1,
paste("Mean-shift (tau-statistic), as referenced against SSA(12,18), and number of crossings for US-INDPRO from 1935-09-01 to 2022-11-01: positive mean-shifts signify a left-shift or lead by SSA(12,18)"),
label=paste("perf_zcc_gap_trend",sep=""),
center = "centering", file = "", floating = FALSE)
@
 
To conclude our analysis of the US business-cycle, we note that the empirical holding-time of \Sexpr{round(length_series/number_cross[1,"SSA(12,18)"],0)} months of \Sexpr{colnames(filter_mat)[4]} exceeds $ht_1=\Sexpr{ht}$. This discrepancy is mainly due to positive autocorrelation (the 'cyclical' growth-rates of the series are smoother than white noise) and the observed effect could be addressed by the extension in section \ref{ext_stat}, fixing the link between expected and empirical holding-times and reestablishing interpretability of the hyper-parameter in its original meaning (but we excluded data-fitting at the outset). Also, the NBER declared  \Sexpr{number_recessions_1935} recessions in a time span from  \Sexpr{index(indpro)[L]}\footnote{The earliest observations are skipped due to filter initialization.} to \Sexpr{index(indpro)[nrow(indpro)]}  which corresponds to a mean-length of expansion-recession cycles of \Sexpr{round(cycle_length_NBER_start_date,0)} months or roughly \Sexpr{round(cycle_length_NBER_start_date/12,0)} years. In comparison, twice the empirical holding-time of the SSA-filter corresponds to $2\cdot\Sexpr{round(length_series/number_cross[1,"SSA(12,18)"],0)}$=\Sexpr{2*round(length_series/number_cross[1,"SSA(12,18)"],0)} months or four years. Therefore, additional fine-tuning of the design, including the selection of the (HP) target or of the economic indicator\footnote{Some of the downturns of the industrial production index, such as in 2015 and 2016, do not classify as economic recessions and therefore a direct comparison of corresponding 'cycles' is subject to caution.} or of $ht_1$ in the SSA-specification might be envisioned to match  cycle-lengths, at least when calibrated against NBER recession datings.  




\subsection{Multi-National Perspective}\label{multi_nat}


We here extend the above framework 'as is' to a selection of countries, restricting attention to 'mature' and large economies with correspondingly long and stable cycle histories, ideally differing from the US. see table \ref{perf_zcc_gap_trend_extended}. Aggregate performances, obtained by concatenating all series, are summarized in table \ref{perf_zcc_gap_trend_mean}: they confirm expectations, as entailed by the selected hyperparameters, and the two SSA-designs are representative of two particular non-exhaustive research priorities. 
<<label=init,echo=FALSE,results=hide>>=
# Compute timeliness smoothness for all time series

# 1. First data set Simon
# We initialize table with results for US above i.e. we skip US from indpro: indpro[,-3]
time_cross_mat<-mat_re[,c(1,2,5)]
series_mat<-indpro[,-3]
time_cross_mat<-NULL
number_cross_mid<-number_cross_fast<-matrix(rep(0,2),nrow=1)
series_mat<-indpro
tau_vec_long_mid<-tau_vec_long_fast<-NULL
for (ijk in 1:ncol(series_mat))#ijk<-1
{  
# Select INDPRO and make xts object
  select_series<-ijk
# Remove NAs and fix correct type  
  series<-as.vector(na.exclude(series_mat[,select_series]))
  ts.plot(series)

#   Filter data

  filter_obj<-SSA_filter_func(filter_mat,L,series)

  y_mat=filter_obj$y_mat
  
  ts.plot(scale(y_mat,center=F,scale=T)[,c("SSA(12,18)","HP trend")],col=c("blue","brown"))
  abline(h=0)

#------------------------------------
# Diagnostics real data: number of Crossings, peak correlation and shift (tau statistic)

# 5.1 SSA-mid vs HP trend
# Select competing series: reference filter/series first
#   Reference series determines crossings, see description of tau-statistic in paper (should be smoother)
#   Reference series determines sign of lead/lags of peak-correlation
# Note: should not be an xts-object!!!!
  mplot<-as.matrix(y_mat[,c("SSA(12,18)","HP trend")])
# Max lead for peak-correlation
  max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
  last_crossing_or_closest_crossing<-F
  
  timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)
  
  cor_peak=timeliness_obj$cor_peak
  tau_vec=timeliness_obj$tau_vec
  tau_vec_adjusted=timeliness_obj$tau_vec_adjusted
# Collect tau summands over all series for statistical test  
  tau_vec_long_mid<-c(tau_vec_long_mid,tau_vec_adjusted)
  tau_trend=timeliness_obj$tau
  tau_adjusted_trend=timeliness_obj$tau_adjusted
  t_test=timeliness_obj$t_test
  t_test_adjusted=timeliness_obj$t_test_adjusted
  number_cross_trend=timeliness_obj$number_cross
  number_cross_mid<-number_cross_mid+number_cross_trend
  tau_trend
  tau_adjusted_trend
  number_cross_trend
  
# 5.2 SSA-mid vs. SSA-fast
  mplot<-as.matrix(y_mat[,c(4,5)])

  timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)
  
  tau_fast_trend=timeliness_obj$tau
  tau_adjusted_fast_trend=timeliness_obj$tau_adjusted
  tau_vec_fast_trend_adjusted=timeliness_obj$tau_vec_adjusted
# Collect tau summands over all series for statistical test  
  tau_vec_long_fast<-c(tau_vec_long_fast,tau_vec_fast_trend_adjusted)
  number_cross_fast_trend=timeliness_obj$number_cross
  number_cross_fast<-number_cross_fast+number_cross_fast_trend

  tau_fast_trend
  tau_adjusted_fast_trend
  number_cross_fast_trend

  time_cross_mat<-rbind(time_cross_mat,rbind(c(0,as.integer(round(tau_adjusted_trend,0)),as.integer(round(tau_adjusted_fast_trend,0)))),c(as.integer(number_cross_trend[2:1]),as.integer(number_cross_fast_trend[1])))

}

colnames(time_cross_mat)<-colnames(y_mat)[c(4,1,5)]
#rowname<-c("US shift","US number crossings")
rowname<-NULL#c("US shift","US number crossings")
for (i in 1:ncol(series_mat))
  rowname<-c(rowname,paste(colnames(series_mat)[i],"shift"),paste(colnames(series_mat)[i],"number crossings"))
rownames(time_cross_mat)<-rowname
time_cross_mat

time_cross_mat_first_data_set<-time_cross_mat

#-----------------------------------------------------
# 2. Second data set Simon
colnames(indpro_eu)
series_mat<-indpro_eu

time_cross_mat<-NULL

for (ijk in 1:ncol(indpro_eu))#ijk<-7
{  
# Select INDPRO and make xts object
  select_series<-ijk
# Remove NAs and fix correct type  
  series<-as.vector(na.exclude(indpro_eu[,select_series]))
  ts.plot(series)


  filter_obj<-SSA_filter_func(filter_mat,L,series)

  y_mat=filter_obj$y_mat
  
  ts.plot(scale(y_mat,center=F,scale=T)[,c("SSA(12,18)","HP trend")],col=c("blue","brown"))
  abline(h=0)

#------------------------------------
# Diagnostics real data: number of Crossings, peak correlation and shift (tau statistic)

# 5.1 SSA-mid vs HP trend
# Select competing series: reference filter/series first
#   Reference series determines crossings, see description of tau-statistic in paper (should be smoother)
#   Reference series determines sign of lead/lags of peak-correlation
# Note: should not be an xts-object!!!!
  mplot<-as.matrix(y_mat[,c("SSA(12,18)","HP trend")])
# Max lead for peak-correlation
  max_lead<-41
# Select closest crossings of contender to reference crossings (conservative measure of shift: corresponds to tau-statistic in paper)
  last_crossing_or_closest_crossing<-F
  
  timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)
  
  cor_peak=timeliness_obj$cor_peak
  tau_vec=timeliness_obj$tau_vec
  tau_vec_adjusted=timeliness_obj$tau_vec_adjusted
# Collect tau summands over all series for statistical test  
  tau_vec_long_mid<-c(tau_vec_long_mid,tau_vec_adjusted)
  tau_trend=timeliness_obj$tau
  tau_adjusted_trend=timeliness_obj$tau_adjusted
  t_test=timeliness_obj$t_test
  t_test_adjusted=timeliness_obj$t_test_adjusted
  number_cross_trend=timeliness_obj$number_cross
  number_cross_mid<-number_cross_mid+number_cross_trend

  tau_trend
  tau_adjusted_trend
  number_cross_trend
  
# 5.2 SSA-mid vs. SSA-fast
  mplot<-as.matrix(y_mat[,c(4,5)])

  timeliness_obj<-compute_timeliness_func(mplot,max_lead,ht,last_crossing_or_closest_crossing,outlier_limit)
  
  tau_fast_trend=timeliness_obj$tau
  tau_adjusted_fast_trend=timeliness_obj$tau_adjusted
  tau_vec_fast_trend_adjusted=timeliness_obj$tau_vec_adjusted
# Collect tau summands over all series for statistical test  
  tau_vec_long_fast<-c(tau_vec_long_fast,tau_vec_fast_trend_adjusted)
  number_cross_fast_trend=timeliness_obj$number_cross
  number_cross_fast<-number_cross_fast+number_cross_fast_trend

  tau_fast_trend
  tau_adjusted_fast_trend
  number_cross_fast_trend

  time_cross_mat<-rbind(time_cross_mat,rbind(c(0,as.integer(round(tau_adjusted_trend,0)),as.integer(round(tau_adjusted_fast_trend,0)))),c(as.integer(number_cross_trend[2:1]),as.integer(number_cross_fast_trend[1])))


}



t_test_mid<-t.test(tau_vec_long_mid,  alternative = "two.sided")$statistic
t_test_fast<-t.test(tau_vec_long_fast,  alternative = "two.sided")$statistic
ts.plot(cumsum(tau_vec_long_mid))
mean(tau_vec_long_mid)
ts.plot(cumsum(tau_vec_long_fast))
mean(tau_vec_long_fast)


colnames(time_cross_mat)<-colnames(y_mat)[c(4,1,5)]
#rowname<-c("US shift","US number crossings")
rowname<-NULL
for (i in 1:ncol(series_mat))
  rowname<-c(rowname,paste(colnames(series_mat)[i],"shift"),paste(colnames(series_mat)[i],"number crossings"))
rownames(time_cross_mat)<-rowname
time_cross_mat

# Concatenate both country sets and perform aggregate mean performances over all countries
time_cross_mat_final<-rbind(time_cross_mat_first_data_set,time_cross_mat)
aggregate_time_cross_mat_final<-rbind(round(c(0,mean(tau_vec_long_mid),mean(tau_vec_long_fast)),2),
                                      c(0,t_test_mid,t_test_fast),
                                     c( as.integer(number_cross_mid[2:1]),as.integer(number_cross_fast[1])))
  
colnames(aggregate_time_cross_mat_final)<-colnames(time_cross_mat)
  

rownames(aggregate_time_cross_mat_final)<-c("Mean-shift over countries","t-test for time-shift","Total number of crossings")
colnames(aggregate_time_cross_mat_final)<-colnames(aggregate_time_cross_mat_final)

@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
library(Hmisc)
require(xtable)
xtable(time_cross_mat_final, dec = 1,digits=0,
paste("Number of crossings and mean-shift (tau-statistic), as referenced against SSA(12,18): a positive shift means a corresponding lead."),
label=paste("perf_zcc_gap_trend_extended",sep=""),
center = "centering", file = "", floating = FALSE)
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
xtable(aggregate_time_cross_mat_final, dec = 1,digits=2,
paste("Aggregate mean timeliness and smoothness performances obtained by concatenation into a single long series. Shifts at zero-crossings i.e. tau-statistics are referenced against SSA(12,18): positive shifts indicate a lead or left-shift of the reference; t-statistics for significance of the lead or lag are reported in the middle row"),
label=paste("perf_zcc_gap_trend_mean",sep=""),
center = "centering", file = "", floating = FALSE)
@
While the lag of HP-trend against the reference \Sexpr{colnames(filter_mat)[4]} is statistically insignificant, the latter outperforms the former in terms of smoothness. The lead of \Sexpr{colnames(filter_mat)[5]} over the reference \Sexpr{colnames(filter_mat)[4]}, and thus over the benchmark, is strongly significant: for similar smoothness, SSA outperforms HP-trend in terms of timeliness (the strict equality of the total number of crossings is fortuitous in this case). We may also refer to fig.\ref{tau_vec} in the appendix for a visualization of $\tau$ and the underlying statistical t-test. While our treatment of timeliness, by alterations of the forecast horizon $\delta$, might be felt as 'ad hoc', we  argue that the intended effect can be established, measured and tested for statistical significance; moreover, the upshot of our proceeding consists in interpreting the forecast horizon as an additional free tuning-parameter, affecting properties of the predictor to match alternative research priorities. In this abstract perspective,  a more legitimate formal approach can be developed for addressing prediction-tradeoffs in a fundamental manner and SSA is a first step pointing % remains a key-element for building  is currently under investigation criterion could be derived,  based on the proposed SSA-framework, which would emphasize all three dimensions of the prediction tradeoff, namely accuracy, timeliness and smoothness, in a more elaborate and fundamental manner. Our proceeding here is a first step pointing 
to this direction. 


%. We conclude, by emphasizing that our treatment of timeliness, by alteration of the forecast horizon $\delta$, is to some extent ad hoc: indeed, a more refined criterion could be derived,  based on the proposed SSA-framework, which would emphasize all three dimensions of the prediction tradeoff, namely accuracy, timeliness and smoothness, in a more elaborate and fundamental manner. Our proceeding here is a first step pointing to this direction.




\section{Introduction}




Time series forecasting aims at a coherent analysis of the main systematic dynamics of a phenomenon in view of synthesizing information about future events. Typically, the forecast process is structured by a formal optimality concept, whereby a particular forecast-error measure, such as e.g. the mean-square error (MSE), is minimized. We here argue that multiple and various characteristics of a predictor might draw attention such as the smoothing capability, i.e. the extent by which undesirable 'noisy' components of a time series are suppressed, or timeliness, as measured by relative lead or lag properties of a predictor, or sign accuracy and zero-crossings, as measured by the ability to predict the correct sign of a target. Wildi (2023 b) proposes a generic forecast approach, referred to as simple sign-accuracy (SSA), by merging sign-accuracy and MSE performances subject to a holding-time constraint which  determines the expected number of zero-crossings of the predictor in a given time interval. Zero-crossings (of the growth-rate) of a time series are influential in the decision-making, for e.g. economic actors, by marking transitions between up- and down-turns, expansions and recessions, bull and bear markets, and our forecast approach contributes to such a design of the predictor.  Wildi (2023 a) illustrates an application of  SSA to  business-cycle analysis, but the chosen treatment remains  largely informal.  
Wildi (2023 b) derives SSA-solutions under various assumptions about the structure of the estimation problem in the case of univariate time series and we here extend theses results to a multivariate framework.    
% in terms of an interpretable hyper-parameter. % While a comprehensive and formal treatment of timeliness must be deferred, corresponding issues will be considered indirectly, via an additional and interpretable tuning- or hyper-parameter, and performances in terms of leads or lags will be measured accordingly. %Some of our examples illustrate that classic predictors can be outperformed both in terms of smoothness and timeliness at once.       
%We here combine mean-square error (MSE) performances, sign accuracy, zero-crossings and smoothness characteristics in a common formal framework under suitable assumptions about the data-generating process. % and defer a lengthier theoretical treatment of timeliness which will be considered from a purely descriptive perspective, only. 
McElroy and Wildi (2019) propose an alternative approach for addressing specific facets of the forecast problem but they do not address zero-crossings explicitly which may be viewed as a shortcoming in some applications. \\

    
The analysis of zero-crossings has been pioneered by Rice (1944). Kedem (1986) and Barnett (1996) extend the concept to exploratory and inferential statistics and a theoretical overview is provided by Kratz (2006). Application fields are various in electronics and image processing, process discrimination, pattern detection in speech, music, or radar screening. However, in contrast to the analysis of current or past events, we here emphasize foremost a prospective prediction perspective. \\



The optimization criterion is derived in section \ref{zc}; solutions of the criterion are proposed in section \ref{theorem_SSA} with consideration of regular and boundary cases and with a derivation of the sample distribution of the predictor as well as of a numerical optimization routine; sections \ref{examples} and \ref{sign_het} propose applications to  forecasting and  signal extraction. All empirical examples are reproducible in an open source R-package (include link to Github). Finally, section \ref{conclusion} concludes by summarizing our main findings. 









\section{Simple Sign-Accuracy (SSA-) Criterion} \label{zc}

\subsection{Introduction}

Let $\mathbf{x}_t=(x_{1t},...,x_{nt})'$ be a multivariate stationary process with (purely non-deterministic) Wold-decomposition $\mathbf{x}_t=\sum_{k=0}^\infty \boldsymbol{\Xi}_k\boldsymbol{\epsilon}_{t-k}$ where $\boldsymbol{\Xi}_0=\mathbf{I}_{n*n}$ is the identity and where $\boldsymbol{\epsilon}_{t}=(\epsilon_{1t},...,\epsilon_{nt})'$ is a sequence of multivariate Gaussian noise with variance-covariance matrix $\boldsymbol{\Sigma}$: denote its entries by $\sigma_{ij}$,  assume that its eigenvalues $\tilde{\sigma}_{j}$ are strictly positive (full rank) and denote its eigenvectors by $\mathbf{v}_{\sigma k}$. %\footnote{If $\boldsymbol{\Xi}_0\neq \mathbf{I}_{n*n}$ then one can multiply the system with $\boldsymbol{\Xi}_0^{-1}$, which is assumed to exist because $\boldsymbol{\Sigma}>0$, and obtain a transformed variance covariance matrix $\boldsymbol{\Xi}_0^{-1}\boldsymbol{\Sigma}\boldsymbol{\Xi}_0$.}. 
Let $\mathbf{z}_t=\sum_{|k|<\infty}\boldsymbol{\Gamma}_k\mathbf{x}_{t-k}$ be a stationary Gaussian series, where $\boldsymbol{\Gamma}_k$, with entries $(\gamma_{ijk})$ $i,j=1,...,n$, denotes a sequence of square-summable filter matrices: $\sum_{|k|<\infty}\boldsymbol{\Gamma}_k'\boldsymbol{\Gamma}_k<\infty$ elementwise. We can interpret $\mathbf{\Gamma}_k$ as signal extraction filters and $\mathbf{z}_t$ as 'components' of $\mathbf{x}_t$, e.g. a trend, or a cycle or a seasonally adjusted component,  see  McElroy and Wildi (2020) for background and section \ref{sign_het} for an application. We consider estimation of $\mathbf{z}_{t+\delta}$, where $\delta\in \mathbb{Z}$, based on the predictor $\mathbf{y}_t=\sum_{k=0}^{L-1}\mathbf{B}_k\mathbf{x}_{t-k}$ which must be a stationary process, too: let $b_{ijk}$ denote the entries of $\mathbf{B}_k$ and define
\begin{eqnarray*}
\mathbf{x}_{it}&=&(x_{it},x_{it-1},...,x_{it-(L-1)})'\\
\mathbf{x}_{\cdot t}&=&(\mathbf{x}_{1t}',...,\mathbf{x}_{nt}')'\\
\boldsymbol{\epsilon}_{it}&=&(\epsilon_{it},\epsilon_{it-1},...,\epsilon_{it-(L-1)})'\\
\boldsymbol{\epsilon}_{\cdot t}&=&(\boldsymbol{\epsilon}_{1t}',...,\boldsymbol{\epsilon}_{nt}')'\\
\boldsymbol{\gamma}_{ij\delta}&=&(\gamma_{ij\delta},\gamma_{ij\delta+1},...,\gamma_{ij\delta+L-1})'\\
\boldsymbol{\gamma}_{i\cdot\delta}&=&(\boldsymbol{\gamma}_{i1\delta}',\boldsymbol{\gamma}_{i2\delta}',...,\boldsymbol{\gamma}_{in\delta}')'\\
\boldsymbol{\gamma}_{\delta}&=&\left(\begin{array}{c}\boldsymbol{\gamma}_{1\cdot\delta}'\\
\boldsymbol{\gamma}_{2\cdot\delta}'\\
\cdot\\
\boldsymbol{\gamma}_{n\cdot\delta}'\end{array}\right)\\
\mathbf{b}_{ij}&=&(b_{ij0},b_{ij1},...,b_{ijL-1})'\\
\mathbf{b}_{i}&=&(\mathbf{b}_{i1}',\mathbf{b}_{i2}',...,\mathbf{b}_{in}')'\\
\mathbf{b}&=&\left(\begin{array}{c}\mathbf{b}_{1}'\\\mathbf{b}_{2}'\\\cdot\\\mathbf{b}_{n}'\end{array}\right)
\end{eqnarray*}   
Finally, set $y_{ijt}:=\mathbf{b}_{ij}'\mathbf{x}_{jt}$ so that $y_{it}=\mathbf{b}_i'\mathbf{x}_{\cdot t}=\sum_{j=1}^ny_{ijt}$ and $\mathbf{y}_t=\mathbf{b}'\mathbf{x}_{\cdot t}$. 
In order to introduce the relevant topics we first assume $\mathbf{x}_t=\boldsymbol{\epsilon}_t$ so that   $\boldsymbol{\Xi}_k=\mathbf{0}$ for $k>0$. We refer to this simplifying framework in terms of \emph{basic methodological framework} (BMF). The BFM chiefly intends to clarify exposition and to simplify notation in view of highlighting the relevant facets of the prediction problem in a decluttered formal context. Extensions to  autocorrelated $\mathbf{x}_t$ are proposed in section \ref{ext_stat} and departures from the Gaussian assumption are discussed in Wildi (2023 a), (2023 b) and are addressed in section \ref{non_gaussian}. \\
By orthogonal projection, the (classic) MSE-predictor $\hat{\mathbf{z}}_{t\delta}=(\hat{z}_{1t\delta},...,\hat{z}_{nt\delta})'$ of $\mathbf{z}_{t+\delta}$ is obtained by deleting all $\epsilon_{jk}$, $j=1,...,n$, in $\mathbf{z}_{t+\delta}$ for which  $k\notin\{0,...,L-1\}$: $\hat{\mathbf{z}}_{t\delta}=\boldsymbol{\gamma}_{\delta}'\boldsymbol{\epsilon}_{\cdot t}$. 
<<label=init,results=hide>>=
# Brief empirical check of MSE estimate (intended for a later student-exercise...)
setseed<-1
len<-1000000
eps<-rnorm(len)
L_t<-5
gammak<-rep(1,L_t)
targeth<-eps
for (i in length(gammak):len)
  targeth[i]<-gammak%*%eps[i:(i-length(gammak)+1)]

explanatory<-NULL
# For any L the above MSE-estimate is obtained
L<-1
delta<-1
for (i in 1:L)
  explanatory<-cbind(explanatory,eps[(L+1-i):(len+1-i-delta)])

target<-targeth[(len-nrow(explanatory)+1):len]

summary(lm(target~explanatory-1))
@
Consider next the $L*L$-dimensional autocovariance-generating matrix   
\[
M=\left(\begin{array}{ccccccccc}0&0.5&0&0&0&...&0&0&0\\
0.5&0&0.5&0&0&...&0&0&0\\
...&&&&&&&&\\
0&0&0&0&0&...&0.5&0&0.5\\
0&0&0&0&0&...&0&0.5&0
\end{array}\right)
\]
with eigenvalues $\lambda_k=-\cos(k\pi/(L+1))$, k=1,...L, ordered according to increasing size, and corresponding orthonormal basis of (Fourier) eigenvectors $\mathbf{v}_k=\sin\Big(k\pi/(L+1)\Big)/\sqrt{\sum_{j=1}^L \sin(j\pi/(L+1))^2}$. 
<<label=init,echo=FALSE,results=hide>>=
# Check cosine formula for eigenvalues, see e.g. Anderson
L<-11
M<-matrix(nrow=L,ncol=L)
M[L,]<-rep(0,L)
M[L-1,]<-c(rep(0,L-1),0.5)
for (i in 1:(L-2))
  M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
M<-M+t(M)
  
eigen(M)$values
-cos(pi*(1:L)/(L+1))
# Cancel each other
-cos(pi*(1:L)/(L+1))+eigen(M)$values

V<-eigen(M)$vectors
# Check eigenvectors
k<-4
W<-sin(k*(1:L)*pi/(L+1))/sqrt(sum(sin(k*(1:L)*pi/(L+1))^2))
min(abs(W-V[,k]),abs(W+V[,k]))

@ 
Denote  by $\tilde{\mathbf{I}}:=\boldsymbol{\Sigma}\otimes\mathbf{I}_{L*L}$ and $\tilde{\mathbf{M}}:=\boldsymbol{\Sigma}\otimes\mathbf{M}$  the Kronecker products of the $n*n$ dimensional $\boldsymbol{\Sigma}$, the $L*L$ dimensional identity  $\mathbf{I}_{L*L}$ and the autocovariance generating matrix $\mathbf{M}$. Then 
\begin{eqnarray}
E[z_{it+\delta}y_{it}]&=&E[\hat{z}_{it\delta}y_{it}]=\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}\label{moment1}\\
E[\hat{z}_{it\delta}^2]&=&\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}\label{moment2}\\
E[y_{it}^2]&=&\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}\label{moment3}\\
E[y_{it-1}y_{it}]&=&\mathbf{b}_{i}'\tilde{\mathbf{M}}\mathbf{b}_{i}\label{moment4}
\end{eqnarray}
Finally, the variance of $z_{it}$ is $E[z_{it}^2]=\sum_{|k|<\infty}\boldsymbol{\gamma}_{i k}'\boldsymbol{\Sigma}\boldsymbol{\gamma}_{i k}$ where $\boldsymbol{\gamma}_{i k}:=(\gamma_{i1k},...,\gamma_{ink})'$. 

%\textbf{Remark}\\
%An equivalent re-ordering of the noise-terms according to
%\begin{eqnarray*}
%\boldsymbol{\epsilon}_{t}&=&(\epsilon_{1t},\epsilon_{2t},...,\epsilon_{nt})'\\
%\boldsymbol{\epsilon}_{\cdot t}&=&(\boldsymbol{\epsilon}_{t}',\boldsymbol{\epsilon}_{t-1}'
%,...,\boldsymbol{\epsilon}_{1}')'
%\end{eqnarray*}
%would lead to Kronecker products $\mathbf{M}\otimes\boldsymbol{\Sigma}$ and $\mathbf{I}\otimes\boldsymbol{\Sigma}$. While each ordering has its own advantages and shortcomings, at least in notational terms, we here assign preference to the first one introduced above.   

\subsection{Sign-Accuracy, MSE and Holding-Time}\label{mse_sa_zc}

For each $i=1,...,n$ we  look for an estimate $y_{it}=\mathbf{b}_i'\mathbf{x}_{\cdot t}$ of $z_{it+\delta}$ such that the probability P$\Big(\sign(z_{it+\delta})=\sign(y_{it})\Big)$ is maximized as a function of $\mathbf{b}_i$ and we refer to this criterion in terms of \emph{sign accuracy} (SA). 

\begin{Proposition}\label{sa_crit_mult}
Under the BMF, the sign accuracy  criterion can be stated alternatively as 
\begin{eqnarray}\label{opt_crit}
\max_{\mathbf{b}_i}\rho(y_i,z_i,\delta)~~\textrm{or~~}\max_{\mathbf{b}_i}\rho(y_i,\hat{{z}}_{i\delta},\delta)
\end{eqnarray}
where 
\begin{eqnarray*}
\rho(y_i,z_i,\delta)=\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}\sqrt{\sum_{|k|<\infty}\boldsymbol{\gamma}_{i k}'\boldsymbol{\Sigma}\boldsymbol{\gamma}_{i k}}}~~ \textrm{and}~~\rho(y_i,\hat{{z}}_{i\delta},\delta)=\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}\sqrt{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}}}
\end{eqnarray*}
are the correlations between $y_{it}$ and  the target $z_{it+\delta}$ or between $y_{it}$ and the MSE-estimate $\hat{{z}}_{it\delta}$ of $z_{it+\delta}$. 
\end{Proposition}

Proof\\
In the stipulated case of Gaussian random variables 
\begin{eqnarray}
P\Big(\sign(z_{it+\delta})=\sign(y_{it})\Big)&=&0.5+\frac{\arcsin(\rho(y_i,z_i,\delta))}{\pi}\nonumber\\
&=&0.5+\frac{\arcsin\left(\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}\sqrt{\sum_{|k|<\infty}\boldsymbol{\gamma}_{i k}'\boldsymbol{\Sigma}\boldsymbol{\gamma}_{i k}}}\right)}{\pi}\nonumber\\
&=&0.5+\frac{\arcsin\left(\rho(y_i,\hat{{z}}_{i\delta},\delta)\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}}{\sum_{|k|<\infty}\boldsymbol{\gamma}_{i k}'\boldsymbol{\Sigma}\boldsymbol{\gamma}_{i k}}\right)}{\pi}\label{arcsin}
\end{eqnarray}
The first equality follows from Gaussian assumption. The proof of the proposition is then obtained from strict monotonicity of the non-linear arcsin-transformation in the domain of definition spanned by the correlations. \\



%Discarding the affine transformation, expression \ref{opt_crit} by monotonicity of $\arcsin()$. %Note that signs, zero-crossings or correlations are insensitive to the scalings of $y_t$ or $z_t$. 
%The MSE-estimate  $\mathbf{b}=\boldsymbol{\gamma}_{\delta}:=(\gamma_{\delta},...,\gamma_{\delta+L-1})'$ is a solution of \ref{opt_crit} 
We infer that SA and MSE are equivalent criteria, at least down to an arbitrary scaling of $y_{it}$ and conditional on the Gaussian assumption. The next step will consist in equiping the SA- (or MSE-) criterion with a supplementary smoothing requirement controlling the rate of zero-crossings of $y_{it}$.\\

\textbf{Remarks}\\
We here discard the scaling parameter from further consideration since our approach emphasizes signs, smoothness and timeliness aspects as alternative priorities. In this perspective, predictors that differ by an arbitrary (positive) normalization constant are felt equivalent. Also, computation of the optimal MSE-scaling would be a simple exercise, if needed.  Note also that classification methods such as e.g. logit models are less suitable for the purpose at hand because fitting the signs $\textrm{sign}(z_{it+\delta})=\pm 1$, instead of the actual observations $z_{it+\delta}$, would result in a loss of efficiency under the premises of the  BMF.\\

<<label=init,results=hide>>=
# Purposes
# 0. Use Gauss or student-t (if skewed then one has to shift by mean)
# 1. Check that MSE/correlation has same sign-accuracy as logit, in-sample
# 2. Non-zero crossings can be addressed by simple shift (mu!=0 in code below)
# 3. MSE estimate has much smaller estimation variance (efficiency): should perform better out-of-sample!
len<-10000
L<-10
gamma<-rep(1/L,L)
Gauss_or_t<-F
set.seed(23)
if (Gauss_or_t)
{  
# Gauss
  x<-rnorm(len)
} else
{  
# Student-t
  df<-10
# Keep symmetric design: otherwise target z will be biased (easier to predict)
  skew<-0
  x<-rt(len, df,skew)
}
# Target: 
# Non-zero crossings are obtained by selecting mu!=0
mu<-0.5
x<-x
z<-x
for (i in L:len)
  z[i]<-gamma%*%x[i:(i-L+1)]+mu

ts.plot(cbind(x,z),col=c("black","red"))

delta<-min(5,L-1)

y<-x
for (i in L:len)
  y[i]<-gamma[1:(L-delta)]%*%x[i:(i-L+delta+1)]+mu

# Sign accuracy MSE
length(which(sign(y[1:(len-delta)])==sign(z[(1+delta):len])))/len

ts.plot(cbind(y,z),col=c("blue","red"))

#------------------------------
# Logit
target<-(1+sign(z)[(1+2*delta-1):len])/2
length(target)
explanatory<-x[delta:(len-delta)]
if (delta>1)
{
  for (i in 2:delta)
  {
    explanatory<-cbind(explanatory,x[(delta-i+1):(len-delta-i+1)])
  }
}
dim(explanatory)
# data set
sample<-data.frame(cbind(target,explanatory))


model <- glm(target ~.,family=binomial(link='logit'),data=sample)

summary(model)

# Advantage MSE over logistic model: variance of estimates is much smaller!!!!
summary(lm(z[(1+len-nrow(explanatory)):len]~explanatory))


fitted.results <- predict(model,newdata=subset(sample,select=1+1:delta),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != target)
# Same performance
print(paste('Accuracy',1-misClasificError))
ht_ex<-round((acos(2/3)/pi)^{-1},3)
@
% (zero-crossings or correlations are insensitive to arbitrary scalings.\\ %However, we maintain the above formulation which will prove insightful when generalizing the optimization concept.  \\
In order to introduce the relevant zero-crossing topic, we here rely on  Kedem (1986) and  consider the 'clipped' binary time series $s_{it}=\left\{\begin{array}{cc}1&y_{it}\geq 0\\0&y_{it}<0\end{array}\right.$ together with the number of zero-crossings $D_i(T):=\sum_{t=2}^T (s_{it}-s_{it-1})^2$ of $y_{it}$ in the interval $1\leq t\leq T$. Denote by $ht(y_i|\mathbf{b}_i):=\frac{T-1}{E[D_i(T)]}$ the so-called \emph{holding-time} of $y_{it}$ and let $\rho(y_i,y_i,1):=\displaystyle{\frac{\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i}{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}}$ designate the lag-one autocorrelation (acf) of $y_{it}$, see \ref{moment3} and \ref{moment4}. 


\begin{Proposition}\label{ht_formula}
Under the BMF the holding-time $ht(y_i|\mathbf{b}_i)$ of $y_{it}$ is 
\begin{eqnarray}\label{ht}
ht(y_i|\mathbf{b}_i)=\frac{\pi}{\arccos(\rho(y_i,y_i,1))}
\end{eqnarray}
\end{Proposition}

A proof is provided by Kedem (1986). The holding-time can be interpreted as the mean duration between  consecutive zero-crossings of $y_{it}$ and we can now formalize the concept of 'smoothness' of a predictor $y_{it}$ by constraining $\mathbf{b}_i$ such that
\begin{equation}\label{ht_const}
ht(y_i|\mathbf{b}_i)= ht_i
\end{equation}
or, equivalently,
\begin{equation}\label{ht_const_z}
\rho(y_i,y_i,1)= \rho_i
\end{equation}
Here, $ht_i$ or $\rho_i$, linked through \ref{ht}, are proper hyper-parameters of our design. In the following, we  refer to the 'holding-time' either in terms of $ht(y_i|\mathbf{b}_i)$ or $\rho(y_i,y_i,1)$, clarifying our intent in case of ambiguity. %If $ht_i$ differs from the holding-time of the MSE-predictor $\hat{{z}}_{i\delta}$ then under suitable regularity assumptions, see theorem \ref{lambda_mult}, $y_{it}\not\propto \hat{{z}}_{i\delta}$ where $\not \propto$ means 'not proportional to'. 
We here argue that the hyper-parameter $ht_i$ is interpretable and can be set a priori, at the onset of an analysis, according to structural elements of a prediction problem. As an example, Wildi (2023 a) illustrates the proceeding in a (univariate) business-cycle application, where $ht_1$ matches the length of historical recession episodes. Also, the holding-time could be selected in view of taming the number of unsystematic or noisy crossings (false alarms). Furthermore, if costly adjustments occur at zero-crossings, for example in the context of algorithmic trading or by the implementation of a macro-economic policy rule, then the total cumulated adjustment-costs over a given time period, being inversely proportional to $ht_i$, could be accounted for by an appropriate selection of the hyperparameter. Finally, $ht_i$ could be set according to short-, mid- or long-term i.e tactic, strategic or fundamental outlook perspectives: a corresponding predictor $y_{it}$ could be interpreted as a 'trend'-component  which reflects the outlook horizon by a corresponding match of the rate of its zero-crossings. This novel 'trend' specification provides an alternative to classic signal extraction, see Wildi (2023 a) for an application to business-cycle analysis. In this context, we argue that the holding-time hyperparameter is intuitively appealing and right out interpretable. 
Concerning the proper selection of the holding-time, the following proposition sets limits for admissible constraints in the basic framework.

\begin{Proposition}\label{stationary_eigenvec}
Assume $L\geq 2$ and $\mathbf{b}_i\neq \mathbf{0}$. Then, under the BMF, $\mathbf{b}_i$  is a stationary point of the lag-one autocorrelation $\rho(y_i,y_i,1)$ if and only if $\mathbf{b}_{ij}\propto \mathbf{v}_{k}$ ($\propto$ symbolizes proportionality), for $j=1,...,m$, where $\mathbf{v}_{k}$ is an eigenvector of $\mathbf{{M}}$. In this case $\rho(y_{i},y_{i},1)=\lambda_k$ where $\lambda_k$ is the corresponding eigenvalue. The extremal values $\rho_{min}(L)$ and $\rho_{max}(L)$ of $\rho(y_i,y_i,1)$ are $\rho_{min}(L)=\min_k\lambda_k=\cos(\pi L/(L+1))$ and $\rho_{max}(L)=\max_k\lambda_k=-\cos(\pi L/(L+1))$. 
\end{Proposition}

Proof\\

Assume, for sake of exposition, that $\mathbf{b}_i(\neq\mathbf{0})$ is defined on the 'unit-ellipse'  $\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}=1$ so that  
\begin{eqnarray*}
\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}&=&1\\
\rho(y_i,y_i,1)&=&\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i
\end{eqnarray*}
A stationary point of $\rho(y_i,y_i,1)$ is found by equating the derivative of the Lagrangian $\mathfrak{L}=\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i-\lambda(\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}-1)$ to zero i.e.
\[
\left(\mathbf{\tilde{M}}-\lambda\mathbf{\tilde{I}}\right)\mathbf{b}_i=\mathbf{0}
\]
Under the BMF $\boldsymbol{\Sigma}$ has full-rank so that we can multiply the above expression with $\boldsymbol{\Sigma}^{-1}\otimes\mathbf{I}_{L*L}=\mathbf{\tilde{I}}^{-1}$ to obtain
<<label=init,echo=FALSE,results=hide>>=
# Check claim 
n<-3
L<-5
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
M_obj<-M_func(L,Sigma)
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
M<-M_obj$M

M_inv_obj<-M_func(L,solve(Sigma))
M_inv_tilde<-M_inv_obj$M_tilde
I_inv_tilde<-M_inv_obj$I_tilde

# This is an identity
I_tilde%*%I_inv_tilde
# This is kronecker(M,Id)
M_tilde%*%I_inv_tilde
# Check: vanishes
M_tilde%*%I_inv_tilde-kronecker(diag(rep(1,n)),M)

@
\[
\left(\mathbf{I}_{n*n}\otimes\mathbf{{M}}-\lambda\mathbf{I}_{nL*nL}\right)\mathbf{b}_i=\mathbf{0}
\]
which could be written alternatively as $n$ homogeneous subsystems
\begin{eqnarray*}
\mathbf{M}\mathbf{b}_{ij}-\lambda\mathbf{b}_{ij}=\mathbf{0}
\end{eqnarray*}
for $j=1,...,n$. Therefore, $\lambda=\lambda_k$ must be an eigenvalue of $\mathbf{M}$, for some $k\in\{1,...,L\}$. Moreover, since the eigenvalues of $\mathbf{M}$ are pairwise different, we infer that the corresponding eigenvector $\mathbf{v}_k$ is determined uniquely and that $\mathbf{b}_{ij}\propto\mathbf{v}_k$ for $j=1,...,n$. Also 
\[
\rho(y_{ij},y_{ij},1)=\frac{\mathbf{b}_{ij}'\mathbf{Mb}_{ij}}{\mathbf{b}_{ij}'\mathbf{b}_{ij}}=\lambda\frac{\mathbf{b}_{ij}'\mathbf{b}_{ij}}{\mathbf{b}_{ij}'\mathbf{b}_{ij}}=\lambda_k
\]
Since $\mathbf{b}_{ij}\propto\mathbf{v}_{k}$ we conclude that 
\begin{eqnarray*}
y_{it}=\mathbf{b}_i'\boldsymbol{\epsilon}_{\cdot t}\propto\mathbf{v}_{k}'\boldsymbol{\tilde{\epsilon}}_{t}
\end{eqnarray*} 
where $\boldsymbol{\tilde{\epsilon}}_{t}=\sum_{k=1}^n a_j\boldsymbol{{\epsilon}}_{jt}$ is some linear combination of $\boldsymbol{{\epsilon}}_{jt}$. Therefore, $\rho(y_i,y_i,\delta)=\rho(y_{ij},y_{ij},\delta)=\lambda_k$, as claimed. Finally, since the  unit-ellipse is free of boundary-points, we conclude that the extremal values $\rho_{min}(L)$, $\rho_{max}(L)$ of the lag-one acf of $y_{it}$ must be stationary points i.e. $\rho_{min}(L)=\min_k\lambda_k$ and $\rho_{max}(L)=\max_k\lambda_k$ which completes the proof of the proposition.\\
<<label=init,echo=FALSE,results=hide>>=
# Check cosine formula for eigenvalues, see e.g. Anderson
L<-11
M<-matrix(nrow=L,ncol=L)
M[L,]<-rep(0,L)
M[L-1,]<-c(rep(0,L-1),0.5)
for (i in 1:(L-2))
  M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
M<-M+t(M)
  
eigen(M)$values
-cos(pi*(1:L)/(L+1))
# Cancel each other
-cos(pi*(1:L)/(L+1))+eigen(M)$values

V<-eigen(M)$vectors
# Check eigenvectors
k<-4
W<-sin(k*(1:L)*pi/(L+1))/sqrt(sum(sin(k*(1:L)*pi/(L+1))^2))
min(abs(W-V[,k]),abs(W+V[,k]))

@ 
<<label=init,echo=FALSE,results=hide>>=
# Check claim 
L<-5
n<-3
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
M_obj<-M_func(L,Sigma)
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
M<-M_obj$M
max(eigen(M)$values)
cos(pi/(L+1))
@
%Proof\\
%Re-group $y_{it}=\mathbf{b}_i'\boldsymbol{\epsilon}_{\cdot t}$ as in $y_{it}=\sum_{k=0}^{L-1}\left(\sum_{j=1}^nb_{ijk}\epsilon_{jt-k}\right)=\sum_{k=0}^{L-1}\tilde{b}_k\nu_{t-k}$ where $\nu_t$ is iid Gaussian noise. Then the proof of the proposition follows from the maximal lag-one autocorrelation of an MA($L-1$)-process as derived in N. Davies, M. B. Pate and M. G. Frost (1974), see also proposition \ref{stationary_eigenvec} further down.\\ 
%is a hyper-parameter that controls for the \emph{smoothing}-capability of the filter $\mathbf{b}$. %y_t$ by imposing a mean-length between consecutive zero-crossings. 

Consider now the sign accuracy criterion  \ref{opt_crit} endowed with the holding-time constraint \ref{ht_const_z}:
\begin{eqnarray}\label{crit1_mult}
\left.\begin{array}{cc}
&\max_{\mathbf{b}_i}\displaystyle{\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}\sqrt{\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}}}}\\
&\displaystyle{\frac{\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i}{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}=\rho_i}
\end{array}
\right\}
\end{eqnarray}
This optimization problem is called \emph{simple sign-accuracy} or SSA-criterion: simplicity here refers to the elementary structure of the predictor, as derived in theorem \ref{lambda_mult}. We allude  to solutions of this criterion by the acronym SSA or SSA($ht_i,\delta$) or SSA($\rho_i,\delta$). Under the BMF, the selected objective function $\rho(y_i,\hat{z}_{i\delta},\delta)$ could be replaced by $\rho(y_i,z_{i},\delta)$ or by P$\Big(\sign(z_{it+\delta})=\sign(y_{it})\Big)$ without altering the solution, see proposition \ref{sa_crit_mult}. Our selection of $\rho(y_i,\hat{z}_{i\delta},\delta)$ in \ref{crit1_mult} emphasizes relative performances of SSA, as benchmarked against MSE: if $\rho(y_i,\hat{z}_{i\delta},\delta)=1$ then $\mathbf{b}_i\propto\boldsymbol{\gamma}_{i\cdot\delta}$ and the MSE predictor is already compliant with the holding-time constraint which could be skipped, as well (so called 'degenerate case'). The SSA-criterion merges MSE, sign accuracy and smoothing requirements in a flexible and consistent way. Departures from the Gaussian assumption can be accommodated in the sense that $y_{it}$ or $z_{it}$ can be 'nearly Gaussian' even if $x_{it}=\epsilon_{it}$ is not, due to the central limit theorem, see Wildi (2023 a) for an application to economic data, Wildi (2023 b) for an application to heavy-tailed data and section \ref{non_gaussian}.  Finally, the  criterion remains appealing outside of a strict holding-time or zero-crossing perspective by complementing the classic predictor with a generic smoothing constraint addressing its lag-one acf.        
%, and the criterion aims at matching 'directly' signs of forecast and of target. In contrast, the sign inference for a logit-model is obtained 'indirectly', via the determination of an additional discrete  decision rule determined typically by the logit-output being above or below a $50\%$ score.}. 


\subsection{Extension to Stationary Processes}\label{ext_stat}

Let 
\begin{eqnarray*}
\mathbf{x}_t&=&\sum_{m=0}^{\infty}\boldsymbol{\Xi}_m\boldsymbol{\epsilon}_{t-m}\\
\mathbf{z}_t&=&\sum_{|k|<\infty}\boldsymbol{\Gamma}_k \mathbf{x}_{t-k}
\end{eqnarray*} 
be stationary Gaussian processes. %In this general framework, forecasting or signal extraction are obtained by selecting suitable $\delta$ or $\gamma_k$ as shown in the empirical examples below. 
%The  estimate $y_t=\sum_{k=0}^{L-1}b_kx_{t-k}$ of $z_{t+\delta}$ is then called a forecast, a nowcast or a backcast depending on $\delta>0,\delta=0$ or $\delta<0$. 
Then target and predictor can be formally re-written as 
\begin{eqnarray*}
\mathbf{z}_t&=&\sum_{|k|<\infty}(\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_k \boldsymbol{\epsilon}_{t-k}\\
\mathbf{y}_t&=&\sum_{j\geq 0} (\mathbf{B}\cdot\boldsymbol{\Xi})_j\boldsymbol{\epsilon}_{t-j}
\end{eqnarray*} 
where $(\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_k =\sum_{m\leq k}\boldsymbol{\Gamma}_m\boldsymbol{\Xi}_{k-m}$ and $(\mathbf{B}\cdot\boldsymbol{\Xi})_j=\sum_{m=0}^{\min(L-1,j)}\mathbf{B}_m \boldsymbol{\Xi}_{j-m} $ are convolutions of the sequences $\boldsymbol{\Gamma}_k$ and $\mathbf{B}_j$ with the Wold-decomposition $\boldsymbol{\Xi}_m$ of $\mathbf{x}_t$. For given, $(\mathbf{B}\cdot\boldsymbol{\Xi})_j$, the 'original' $\mathbf{B}_{k}$ can be obtained from deconvolution
\begin{eqnarray}\label{con_inv}
\mathbf{B}_{k}&=&\left\{\begin{array}{cc}(\mathbf{B}\cdot\boldsymbol{\Xi})_{0}\boldsymbol{\Xi}_0^{-1}&,k=0\\
\left(  (\mathbf{B}\cdot\boldsymbol{\Xi})_{k}-\sum_{m=0}^{k-1}\mathbf{B}_m \boldsymbol{\Xi}_{j-m}\right)\boldsymbol{\Xi}_0^{-1}&,k=1,...,L-1
\end{array}\right.
\end{eqnarray}
This expression could be simplified by noting that $\boldsymbol{\Xi}_0^{-1}=\mathbf{I}$ in our parametrization. Let $(\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_{lmk}$ and $(\mathbf{B}\cdot\boldsymbol{\Xi})_{lmj}$ denote the $lm$ elements of the corresponding matrices, assume that $\delta\geq 0$ (for- and nowcasting\footnote{Similar but notationally more cumbersome expressions would be obtained for $\delta<0$ (backcast) which are skipped here.}) and denote
\begin{eqnarray*}
(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{ij\delta}&=&\left((\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_{ij\delta},(\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_{ ij\delta+1}...,(\boldsymbol{\Gamma}\cdot\boldsymbol{\Xi})_{ij\delta+L-1}\right)'\\
(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i\delta}&=&\Big((\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i1\delta}',(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i2\delta}',...,(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{in\delta}'\Big)'\\
(\mathbf{b}\cdot\boldsymbol{\xi})_{ij}&=&\left((\mathbf{B}\cdot\boldsymbol{\Xi})_{ ij0},(\mathbf{B}\cdot\boldsymbol{\Xi})_{ij1},...,(\mathbf{B}\cdot\boldsymbol{\Xi})_{ijL-1}\right)'\\
(\mathbf{b}\cdot\boldsymbol{\xi})_{i}&=&\Big((\mathbf{b}\cdot\boldsymbol{\xi})_{i1}',(\mathbf{b}\cdot\boldsymbol{\xi})_{i2}',...,(\mathbf{b}\cdot\boldsymbol{\xi})_{in}'\Big)'
\end{eqnarray*}
Note that if $\boldsymbol{\Xi}_k=\mathbf{0},~k>0$ so that $\mathbf{x}_t=\boldsymbol{\epsilon}_t$ then $(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i\delta}=\boldsymbol{\gamma}_{i\cdot\delta}$ and $(\mathbf{b}\cdot\boldsymbol{\xi})_{i}=\mathbf{b}_i$ as defined in the previous section. The SSA-criterion now becomes 
\begin{eqnarray}\label{gen_stat_x}
\max_{(\mathbf{b}\cdot\boldsymbol{\xi})_i}\frac{ (\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i\delta}'\mathbf{\tilde{I}} (\mathbf{b}\cdot\boldsymbol{\xi})_i}{\sqrt{(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i\delta}'\mathbf{\tilde{I}}(\boldsymbol{\gamma}\cdot\boldsymbol{\xi})_{i\delta}}\sqrt{(\mathbf{b}\cdot\boldsymbol{\xi})_i'\mathbf{\tilde{I}}(\mathbf{b}\cdot\boldsymbol{\xi})_i}}\\
\frac{(\mathbf{b}\cdot\boldsymbol{\xi})_i'\mathbf{\tilde{M}}(\mathbf{b}\cdot\boldsymbol{\xi})_i}{(\mathbf{b}\cdot\boldsymbol{\xi})_i'\mathbf{\tilde{I}}(\mathbf{b}\cdot\boldsymbol{\xi})_i}=\rho_i\nonumber
\end{eqnarray}
which can be solved for $(\mathbf{b}\cdot\boldsymbol{\xi})_i$, see theorem \ref{lambda_mult}. $\mathbf{b}_i$ could then be obtained from deconvolution \ref{con_inv}. While $(\mathbf{b}\cdot\boldsymbol{\xi})_i$ is applied to $\boldsymbol{\epsilon}_{\cdot t}$, $\mathbf{b}_i$ is applied to the original data $\mathbf{x}_{\cdot t}$: both predictors are identical up to generally negligible finite sample deviations\footnote{The infinite-length Wold-decomposition of $\mathbf{x}_t$  is truncated at lag $k=L-1$.}. Note that non-stationary integrated processes could be addressed in a similar way, assuming some initialization settings, see e.g. McElroy and Wildi (2020). However, since the concept of a holding-time, i.e. the expected duration between consecutive zero-crossings, would generally not be properly defined anymore, we henceforth assume non-stationary trending data to be suitably differenced. We refer to textbooks for a derivation of the multivariate Wold decomposition based on a finite sample $\mathbf{x}_1,...,\mathbf{x}_T$, see e.g. Brockwell and Davis (1993): a worked-out example is provided in section \ref{sign_het}.  Finally, for notational convenience we henceforth rely on the BMF, acknowledging that straightforward modifications would apply in the case of autocorrelated $\mathbf{x}_t$.     
  %From an empirical perspective, we argue that growth-rates of a wide range of economic time series are in accordance with our simplifying assumption, see e.g. the so-called 'typical spectral shape' of an economic variable in Granger (1966). %To conclude, we note that the procedure could be extended to non-stationary integrated processes. % and its utility would be questionable in the context of suitably transformed  data, typically differences or log-returns, at least if the transformation does not impede the analysis.   % assumption in terms of  conditional heteroscedasticity (vola-clustering) or so-called 'fat tails' (large kurtosis, outliers) is analyzed in section \ref{robustness_SSA}.








\section{Solution to the SSA Prediction Problem}\label{theorem_SSA}



%The structure of the problem is analyzed in section \ref{gen_sol} together with a numerical optimization algorithm and a special case closed-form solution is elaborated in section \ref{ar1closed} . 

%\subsection{General Solution and Numerical Optimization}\label{gen_sol}






Designate by $\tilde{\lambda}_m$, $m=1,...,nL$ the eigenvalues of $\mathbf{\tilde{M}}$ with corresponding eigenvector $\mathbf{\tilde{v}}_m$. Then $\tilde{\lambda}_m=\lambda_k\tilde{\sigma}_j$ and $\mathbf{\tilde{v}}_m=\mathbf{v}_{\sigma j}\otimes\mathbf{v}_k$ for some combination of eigenvalues $\lambda_k$,$\tilde{\sigma}_j$ and eigenvectors $\mathbf{v}_k$, $\mathbf{v}_{\sigma j}$ of $\mathbf{M},\boldsymbol{\Sigma}$, see e.g. Horn and Johnson (1991). We now index eigenvalues and eigenvectors of $\tilde{\mathbf{M}}$ alternatively by $\tilde{\lambda}_{kj}$ and $\mathbf{\tilde{v}}_{kj}$ whereby the double index $kj$ refers to $\lambda_k\tilde{\sigma}_j$ and we assume that $\mathbf{\tilde{v}}_{kj}$ is normalized to unit-length. The eigenvalues of $\mathbf{\tilde{I}}$ are $\tilde{\sigma}_j$ with multiplicity $L$ for each $j=1,...,n$ and the eigenvectors $\mathbf{\tilde{v}}_{kj}$ of $\mathbf{\tilde{M}}$ are also eigenvectors of $\mathbf{\tilde{I}}$. Grouping the orthonormal basis  $\mathbf{\tilde{v}}_{kj}$ in the $nL*nL$-dimensional matrix $\mathbf{\tilde{V}}$ we can then express the MSE-filter $\boldsymbol{\gamma}_{i\cdot\delta}$ by the spectral factorization  
\begin{eqnarray}\label{specdec_mult}
\boldsymbol{\gamma}_{i\cdot\delta}=\mathbf{\tilde{V}}\mathbf{w}_i
\end{eqnarray}
where $\mathbf{w}_i=w_{ikj}, k=1,...,L, j=1,...,n$ designate the spectral weights of $\boldsymbol{\gamma}_{i\cdot\delta}$. In the sequel we will often refer to $\boldsymbol{\gamma}_{i\cdot\delta}$ as the 'target' in the sense that $y_{it}$ should fit $\hat{z}_{it\delta}$,  recall proposition \ref{sa_crit_mult}, or, equivalently, $\mathbf{b}_i$ should match $\boldsymbol{\gamma}_{i\cdot\delta}$ conditional on the holding-time constraint.  
The target $\boldsymbol{\gamma}_{i\cdot\delta}$ is said to have complete (or incomplete) spectral support depending on $\sum_{j=0}^n|w_{ikj}|\neq 0$ for all $k\in\{1,...,L\}$ (or not).   


\begin{Theorem}\label{lambda_mult}
Consider the SSA optimization problem \ref{crit1_mult} under the BMF, or its extension \ref{gen_stat_x} to stationary processes. Assume $L\geq 2$ and consider the following set of regularity assumptions for $i=1,...,n$:
\begin{enumerate}
\item $\boldsymbol{\gamma}_{i\cdot\delta}\neq 0$ (identifiability).
%\item $\mathbf{b}$ is not an eigenvector of $\mathbf{M}$% $\rho_1\neq \lambda_{i_0N}$ for all $i_0$ such that $w_{i_0}\neq 0$ in the spectral decomposition \ref{specdec} of $\boldsymbol{\gamma}_{\delta}$ (indeterminacy)
\item The SSA estimate $\mathbf{b}_i$ is not proportional to $\boldsymbol{\gamma}_{i\cdot\delta}$, $\mathbf{b}_i\not\propto\boldsymbol{\gamma}_{i\cdot\delta}$ (non-degenerate case).
\item $|\rho_i|<\rho_{max}(L)$ (admissibility of the holding-time constraint).%\footnote{In the non-degenerate case $n\neq m$, see the proof of the theorem. Furthermore, the eigenvectors $\lambda_{i}$ of $\mathbf{M}$ are pairwise different.}
\item The MSE-estimate $\boldsymbol{\gamma}_{i\cdot\delta}$ has complete spectral support (completeness).
\end{enumerate}
Then 
\begin{enumerate}
\item \label{ass5_mult}If the third regularity assumption is violated (admissibility) and if $|\rho_i|>\rho_{max}(L)$, then the problem cannot be solved unless the filter-length $L$ is increased such that $|\rho_i|\leq\rho_{max}(L)$. On the other hand, if $\rho_i=\lambda_1=-\rho_{max}(L)$  then the solution of the SSA-problem is
\[
\mathbf{b}_i\propto\sum_{j=1}^n w_{i1j}\tilde{\mathbf{v}}_{1j}
\]
where $w_{i1j}$ and $\mathbf{\tilde{v}}_{1j}$ are the spectral weights and vectors in \ref{specdec_mult}.  Similarly, for $\rho_i=\lambda_L=\rho_{max}(L)$ the SSA-solution is 
\[
\mathbf{b}_i\propto\sum_{j=1}^n w_{iLj}\tilde{\mathbf{v}}_{Lj}
\]

\item \label{ass1_mult} For all $i$ for which the above regularity assumptions hold the solution $\mathbf{b}_{i}$ to \ref{crit1_mult} has the functional parametric form
\begin{eqnarray}\label{ssa_mult}
\mathbf{b}_{i}=D_i\boldsymbol{\tilde{\nu}}_i^{-1}\mathbf{\tilde{I}}\boldsymbol{\gamma}_{i\cdot\delta}=D_i\sum_{k=1}^L\frac{1}{(2\lambda_{k}-\nu_i)}\left(\sum_{j=1}^n w_{ikj}\mathbf{\tilde{v}}_{kj}\right)
\end{eqnarray}
where $D_i\neq 0$, $\nu_i\in \mathbb{R}-\{2\lambda_k|k=1,...,L\}$, $\boldsymbol{\tilde{\nu}}_i:=2\mathbf{\tilde{M}}-\nu_i\mathbf{\tilde{I}}$ is an invertible $nL*nL$ matrix, $w_{ikj}$ and $\mathbf{\tilde{v}}_{kj}$ are from the spectral decomposition \ref{specdec_mult}. Furthermore, $\mathbf{b}_i$ is uniquely determined by the scalar $\nu_i$, down to the arbitrary scaling term $D_i$, whereby the sign of $D_i$ is such that $\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}>0$ (positive criterion value). Similar expressions are obtained for the extension \ref{gen_stat_x} to autocorrelated processes.
\item The lag-one acf $\rho_i(\nu_i)$ determined by $\mathbf{b}_{i}=\mathbf{b}_{i}(\nu_i)$ in \ref{ssa_mult} is
\begin{eqnarray}\label{spec_dec_rho_mult}
\rho_i(\nu_i):=\frac{\mathbf{b}_{i}(\nu_i)'\mathbf{\tilde{M}}\mathbf{b}_{i}(\nu_i)}{\mathbf{b}_{i}(\nu_i)'\mathbf{\tilde{I}}\mathbf{b}_{i}(\nu_i)}=\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\left(\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2\right)}{\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\left(\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2\right)}
\end{eqnarray}
Any $\rho_i$ such that $|\rho_i|<\rho_{max}(L)$ is admissible in the holding-time constraint of \ref{crit1_mult} in the sense that there exists $\nu_i$ such that $\rho_i(\nu_i)=\rho_i$.
\item \label{ass4_mult} If $|\nu_i|>2\rho_{max}(L)$ %and if the vector $\boldsymbol{\gamma}_{\delta}$ with components $\gamma_{k+\delta}, k=0,...,L-1$ is not an eigenvector of $\mathbf{M}$ 
then $\rho_i(\nu_i)$ as defined in \ref{spec_dec_rho_mult} is a strictly monotonic function in $\nu_i$ and the parameter  $\nu_i$  in \ref{ssa_mult} is determined uniquely by the holding-time constraint $\rho_i(\nu_i)=\rho_i$. 

\end{enumerate}
\end{Theorem}

Proof\\
<<label=init,echo=FALSE,results=hide>>=
# Check that eigenvectors of M_tilde are made of appended eigenvectors of M
n<-3
L<-50
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
M_obj<-M_func(L,Sigma)
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
M<-M_obj$M
V_tilde<-eigen(M_tilde)$vectors
V<-eigen(M)$vectors
k<-3
# Check proportionality
# Proportionality term depends on j=1,...,3 because eigenvectors in V_tilde are normalized
V_tilde[,3]/V[,3]

eigen(Sigma)$values
eigen(I_tilde)$values

@
For notational convenience we here rely on the BFM, acknowledging that straightforward modifications would apply in the case of autocorrelated $\mathbf{x}_t$. The multivariate SSA-problem \ref{crit1_mult} can be rewritten for each $i=1,..,n$ as
\begin{eqnarray}
\textrm{max}_{\mathbf{b}_{i}}~\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}&&\label{crit_ssa_proof}\\  
\mathbf{b}_{i}'\mathbf{\tilde{I}}\mathbf{b}_{i}&=&1\label{unit_circle_mult}\\
\mathbf{b}_{i}'\mathbf{\tilde{M}}\mathbf{b}_{i}&=&\rho_i\label{hyperbola_mult}
\end{eqnarray}
where $\mathbf{b}_{i}'\mathbf{\tilde{I}}\mathbf{b}_{i\cdot}=1$ is an arbitrary scaling rule. 
Consider the spectral decomposition  
\begin{eqnarray}\label{specdecdecb_multi}
\mathbf{b}_i:=\sum_{k=1}^{L}\sum_{j=1}^n\alpha_{kj}\mathbf{\tilde{v}}_{kj}
\end{eqnarray}
of $\mathbf{b}_i$ where the indexing corresponds to the ordering of the eigenvectors according to the eigenvalues $\tilde{\lambda}_{kj}=\lambda_k\tilde{\sigma}_j$ of $\mathbf{\tilde{M}}$, from smallest to largest $\lambda_k$. Note that $\tilde{\mathbf{v}}_{kj}$ is an eigenvector of $\mathbf{\tilde{I}}$ with eigenvalue $\tilde{\sigma}_j>0$. Therefore, the length-constraint $\mathbf{b}_i'\mathbf{\tilde{I}}\mathbf{b}_i=1$ implies \begin{eqnarray}\label{length_const}
\sum_{j=1}^n\tilde{\sigma}_j\sum_{k=1}^{L}\alpha_{kj}^2=1
\end{eqnarray} 
(elliptic length constraint). Moreover, from the holding-time constraint we infer
\begin{eqnarray*}
\rho_i=\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i=\sum_{j=1}^n\tilde{\sigma}_j\sum_{k=1}^L \lambda_k \alpha_{kj}^2
\end{eqnarray*}
(hyperbolic holding-time constraint). Since $L\geq 2$ we can select $k=2,j=1$  and solve the hyperbolic constraint for $\alpha_{21}$
\begin{eqnarray}\label{alpha2}
\alpha_{21}^2&=&\frac{\rho_i}{\lambda_2\tilde{\sigma}_1}-\sum_{(k,j)\neq(2,1)}\frac{\lambda_k\tilde{\sigma}_j}{\lambda_2\tilde{\sigma}_1}\alpha_{kj}^2
\end{eqnarray}
Similarly, we solve the length constraint for $\alpha_{11}$, selecting $k=j=1$
\begin{eqnarray*}
\alpha_{11}^2&=&\frac{1}{\tilde{\sigma}_1}-\frac{1}{\tilde{\sigma}_1}\sum_{(k,j)\neq(1,1)}\tilde{\sigma}_j\alpha_{kj}^2\\
&=&\frac{1}{\tilde{\sigma}_1}-\frac{1}{\tilde{\sigma}_1}\sum_{(k,j)\notin\{(2,1),(1,1)\}}\tilde{\sigma}_j\alpha_{kj}^2-\frac{\tilde{\sigma}_1}{\tilde{\sigma}_1}\left(\frac{\rho_i}{\lambda_2\tilde{\sigma}_1}-\sum_{(k,j)\neq(2,1)}\frac{\lambda_k\tilde{\sigma}_j}{\lambda_2\tilde{\sigma}_1}\alpha_{kj}^2\right)
\end{eqnarray*}
where we inserted \ref{alpha2}. The last equation parametrizes the intersection of elliptical and hyperbolic constraints: if the intersection is empty then the SSA-problem cannot be solved. To check, we put  $\alpha_{11}^2$ in evidence:
\begin{eqnarray*}
\alpha_{11}^2\left(1-\frac{\lambda_1}{\lambda_2}\right)&=&\frac{1}{\tilde{\sigma}_1}\left(1-\frac{\rho_i}{\lambda_2}\right)+\sum_{(k,j)\notin\{(2,1),(1,1)\}}\frac{\tilde{\sigma}_j}{\tilde{\sigma}_1}\left(\frac{\lambda_k}{\lambda_2}-1\right)\alpha_{kj}^2
\end{eqnarray*}
Assume now that $\rho_i=\lambda_1=\min_k\lambda_k$  and simplify the previous expression to obtain
\begin{eqnarray}
\tilde{\sigma}_1\alpha_{11}^2&=&\frac{\lambda_2-\rho_i}{\lambda_2-\lambda_1}-\sum_{(k,j)\notin\{(2,1),(1,1)\}}\tilde{\sigma}_j\frac{\lambda_2-\lambda_k}{\lambda_2-\lambda_1}\alpha_{kj}^2\label{solve_alpha_11}\\
&=&1-\sum_{(k,j)\notin\{(2,1),(1,1:n)\}}\tilde{\sigma}_j\frac{\lambda_2-\lambda_k}{\lambda_2-\lambda_1}\alpha_{kj}^2-\sum_{j=2}^n \tilde{\sigma}_j\frac{\lambda_2-\lambda_1}{\lambda_2-\lambda_1}\alpha_{1j}^2\nonumber\\
&=&1-\sum_{(k,j)\notin\{(2,1),(1,1:n)\}}\tilde{\sigma}_j\frac{\lambda_2-\lambda_k}{\lambda_2-\lambda_1}\alpha_{kj}^2-\sum_{j=2}^n \tilde{\sigma}_j\alpha_{1j}^2\nonumber
\end{eqnarray}
where $\sum_{(k,j)\notin\{(2,1),(1,1:n)\}}$ means the sum over all combinations of $(k,j)$ excluding all cases with $k=1$ and the single combination $(k=2,j=1)$. We can rewrite this expression as
\begin{eqnarray}\label{ito_v}
\sum_{j=1}^n \tilde{\sigma}_j\alpha_{1j}^2&=&1-\sum_{(k,j)\notin\{(2,1),(1,1:n)\}}\tilde{\sigma}_j\frac{\lambda_2-\lambda_k}{\lambda_2-\lambda_1}\alpha_{kj}^2
\end{eqnarray}
Since $\tilde{\sigma}_j\frac{\lambda_2-\lambda_k}{\lambda_2-\lambda_1}\left\{\begin{array}{cc}=0&k=2\\<0&k>2\end{array}\right.$, we infer that $\alpha_{kj}=0$ for all $k,j$ such that $k>2$. Otherwise  $\sum_{j=1}^n \tilde{\sigma}_j\alpha_{1j}^2>1$ would contradict the elliptical length constraint \ref{length_const}. Therefore 
\begin{eqnarray}\label{reduced_rank_el_eq}
\sum_{j=1}^n \tilde{\sigma}_j\alpha_{1j}^2=1
\end{eqnarray} 
so that $\alpha_{kj}=0$ for all $k\neq 1$, by the elliptical length-constraint. We deduce that if $\rho_i=\lambda_1$, then ellipse and hyperbole have a reduced-rank $(n-1)$-dimensional elliptical intersection as specified by \ref{reduced_rank_el_eq}. %\footnote{This solution generalizes the univariate case in Wildi (2023 b) where the intersection corresponds to the vertices $\pm \mathbf{v}_1$ i.e. the eigenvector of $\mathbf{M}$ belonging to the smallest eigenvalue $\lambda_1$.}. Note that since $\mathbf{\tilde{v}}_{1j}$???  
Expression \ref{specdecdecb_multi} then simplifies to $\mathbf{b}_i=\sum_{j=1}^n \alpha_{1j}\tilde{\mathbf{v}}_{1j}$: plugging $\mathbf{b}_i$ into the criterion \ref{crit_ssa_proof} gives $\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}=\sum_{j=1}^n w_{i1j}\tilde{\sigma}_j\alpha_{1j}$. Thus the SSA-problem can be stated as
\begin{eqnarray*}
\max_{\alpha_{1j}}\sum_{j=1}^n w_{i1j}\tilde{\sigma}_j\alpha_{1j}~~\textrm{subject~to}~~\sum_{j=1}^n \tilde{\sigma}_j\alpha_{1j}^2=1
\end{eqnarray*}
with adjoined Lagrange-equations $\tilde{\sigma}_jw_{i1j}-2\lambda\tilde{\sigma}_j\alpha_{1j}=0$ so that
\[
\mathbf{b}_i\propto\sum_{j=1}^n w_{i1j}\tilde{\mathbf{v}}_{1j}
\]
as claimed. If $\rho_i<\lambda_1$ then $\frac{\lambda_2-\rho_i}{\lambda_2-\lambda_1}>1$ in \ref{solve_alpha_11} so that the resulting reduced-rank intersection would conflict with the length constraint \ref{length_const}: in this case the SSA-solution cannot be solved. By symmetry, similar arguments apply to the cases $\rho_i=\lambda_L$ and $\rho_i>\lambda_L$, as was to be shown. Note that if $\lambda_1<\rho_i<\lambda_{L}$ then the intersection of ellipse and hyperbola determines a  $Ln-2$-dimensional subspace of $\mathbb{R}^{Ln}$. The corresponding full-rank case is analyzed next.\\ 
<<label=init,echo=FALSE,results=hide>>=
# Compute eigenvectors v_1j, j=1,...,n corresponding to lambda_1*sigma_j where lambda_1 is smallest eigenvalue of M
# The task is not trivial because the eigenvalues of M_tilde are ordered in increasing size (which is not related to the indexing kj such that \tilde{lambda}_{kj}=lambda_k*sigma_j
n<-3
L<-50
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
eigen(Sigma)
M_obj<-M_func(L,Sigma)
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
M<-M_obj$M
V_tilde<-eigen(M_tilde)$vectors
V<-eigen(M)$vectors

# This is the eigenvector v_11 corresponding to smallest lambda1 and smallest sigma_1: it is always the latest entry
v_11<-V_tilde[,n*L]
# This is the eigenvector v_12 corresponding to smallest lambda1 and second smallet sigma_2
which_v_12<-which(min(abs(eigen(M_tilde)$values-eigen(M)$values[1]*eigen(Sigma)$values[2]))==abs(eigen(M_tilde)$values-eigen(M)$values[1]*eigen(Sigma)$values[2]))
v_12<-V_tilde[,which_v_12]

v_11/v_12

v_11/V[,L]

@
For a proof of the second assertion we consider the Lagrangian function 
\begin{eqnarray*}\label{lag_SSA_mult}
L_i:=\boldsymbol{\gamma}_{i\cdot\delta}'\tilde{\mathbf{I}}\mathbf{b}_{i}-\lambda_{i1}(\mathbf{b}_{i}'\mathbf{\tilde{I}}\mathbf{b}_{i}-1)-\lambda_{i2}(\mathbf{b}_{i}'\mathbf{\tilde{M}}\mathbf{b}_{i}-\rho_i)
\end{eqnarray*}
where $\lambda_{i1},\lambda_{i2}$ denote the adjoined multipliers. Since the ellipse $\mathbf{b}_{i}\mathbf{\tilde{I}}\mathbf{b}_{i\cdot}=1$ is free of boundary points, the solution $\mathbf{b}_{i}$ of the SSA-problem must conform to the stationary Lagrangian or vanishing gradient equations
\[
\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}=\lambda_{i1}(\mathbf{\tilde{I}}+\mathbf{\tilde{I}}')\mathbf{b}_{i}+\lambda_{i2} (\mathbf{\tilde{M}}+\mathbf{\tilde{M}}')\mathbf{b}_{i}=\lambda_{i1} 2\mathbf{\tilde{I}}\mathbf{b}_{i}+\lambda_{i2} 2\mathbf{\tilde{M}}\mathbf{b}_{i}
\]
Note that the second regularity assumption (non-degenerate case) implies that the holding-time constraint \ref{hyperbola_mult} is 'active' so that $\lambda_{i2}\neq 0$.  Dividing by $\lambda_{i2}$ then leads to 
\begin{eqnarray}
D_i\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}&=& \boldsymbol{\tilde{\nu}}_i\mathbf{b}_i\label{diff_non_hom_matrix_mult}\\
\boldsymbol{\tilde{\nu}}_i&:=&(2\mathbf{\tilde{M}}-\nu_i\mathbf{\tilde{I}})\label{labelNu_mult}
\end{eqnarray}
where $D_i=1/\lambda_{i2}$  and $\nu_i=-2\frac{\lambda_{i1}}{\lambda_{i2}}$. Since the SSA-solution  is realized on the $nL-2$-dimensional intersection of ellipse \ref{unit_circle_mult} and  holding-time hyperbola \ref{hyperbola_mult} (so-called full-rank case, see the proof of the first assertion), we infer that the  objective function is not overruled by the constraint so that $|\lambda_{i2}|<\infty$ and therefore $D_i\neq 0$ in \ref{diff_non_hom_matrix_mult}, as claimed. 
The eigenvalues of $\boldsymbol{\tilde{\nu}}_i$ are $\tilde{\sigma}_j(2\lambda_{k}-\nu_i)$, $1\leq k\leq L$, $1\leq j\leq n$. We note that if $\mathbf{b}_i$ is a solution of the SSA-problem, then $\nu_i/2$ cannot be an eigenvalue of $\mathbf{M}$. Otherwise, assuming $\nu_i/2=\lambda_{k_0}$ would imply that $\boldsymbol{\tilde{\nu}}_i$    maps the eigenvectors $\tilde{\mathbf{v}}_{k_0j}$, $j=1,...,n$, in the spectral decomposition of $\mathbf{b}_i$ to zero in \ref{diff_non_hom_matrix_mult}. Consider then the left-hand side of the equation 
\[
D_i\tilde{\mathbf{I}}\boldsymbol{\gamma}_{i\cdot\delta}=D_i\tilde{\mathbf{I}}\sum_{k=1}^L\sum_{j=1}^nw_{ikj}\tilde{\mathbf{v}}_{kj}=D_i\sum_{k=1}^L\sum_{j=1}^n\tilde{\sigma}_jw_{ikj}\tilde{\mathbf{v}}_{kj}
\]
If the right-hand side of \ref{diff_non_hom_matrix_mult} maps $\tilde{\mathbf{v}}_{k_0j}$ to zero we conclude that $D_i\tilde{\sigma}_jw_{ik_0j}=0$ so that $w_{ik_0,j}=0$ for $j=1,...,n$ (because $D_i\tilde{\sigma}_j\neq 0$) thus contradicting the last regularity assumption (completeness). 
Therefore we can assume that $\nu_i\in \mathbb{R}-\{2\lambda_k|k=1,...,L\}$, that $\boldsymbol{\tilde{\nu}}_i^{-1}$ exists and that
\[
\boldsymbol{\tilde{\nu}}_i^{-1}=\mathbf{\tilde{V}}\mathbf{D}_{\nu i}^{-1}\mathbf{\tilde{V}}'
\] 
<<label=init,echo=FALSE,results=hide>>=
# Check the equivalent formula bk=nu^{-1}I_tilde\gammak and bk=sum 1/(2lambda_k-nu)\sum w_{jk}v_{jk}
# Stealth bug to account for in R_code, see below!!!!!!!!!!
#rm(list=ls())
n<-8
L<-7

# Specify Sigma, M_tilde and I_tilde
set.seed(1)
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma

M_obj<-M_func(L,Sigma)

M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
M<-M_obj$M

# Specify arbitrary target
set.seed(1)
gamma_target<-matrix(rnorm(n^2*L),nrow=n)
  
  
V_M_tilde<-eigen(M_tilde)$vectors
V_M<-eigen(M)$vectors
V_Sigma<-eigen(Sigma)$vectors
eigen_M<-eigen(M)$values
eigen_Sigma<-eigen(Sigma)$values
#--------------------------------------------------
# Stealth bug

nu_opt<-2.5
nu_mat<-2*M_tilde-nu_opt*I_tilde

# Solution 1
bk<-solve(nu_mat)%*%I_tilde%*%gamma_target[k,]

w<-t(V_M_tilde)%*%gamma_target[k,]
# Alleged solution 2: the eigenvalues of I_tilde are ordered from smallest to largest: this ordering does not conform with ordering of eigenvalues of M_tilde. Thereforte the following expression is false:
bkh<-V_M_tilde%*%diag(eigen(I_tilde)$values/(2*eigen(M_tilde)$values-nu_opt*eigen(I_tilde)$values))%*%t(V_M_tilde)%*%V_M_tilde%*%w

# Check: they are not the same
max(abs(bk-bkh))

#------------------------------------------
# Solution: reorder evereything according to paper
# For that purpose we recompute eigenvectors of M_tilde based on kronecker(V_Sigma[,j],V_M[,k]), and accordingly we recompute/reorder the eigenvalues of I_tilde, M_tilde

# Check that kronecker(V_Sigma[,j],V_M[,1]) is eigenvector of I_tilde
V<-eigen_M_tilde_lambdaksigmaj<-eigen_I_tilde_sigmaj<-eigen_M_tilde<-NULL
for (j in 1:dim(Sigma)[1])
  for (k in 1:dim(V_M)[1])
  { 
    eigen_M_tilde_lambdaksigmaj<-c(eigen_M_tilde_lambdaksigmaj,eigen_M[k]*eigen_Sigma[j])
    eigen_I_tilde_sigmaj<-c(eigen_I_tilde_sigmaj,eigen_Sigma[j])
    eigen_M_tilde<-c(eigen_M_tilde,eigen_M[k])
    v<-kronecker(V_Sigma[,j],V_M[,k])
    V<-cbind(V,v)
# Check that image is multiple of vector     
    if (F)
    {  
      eigenv<-as.vector(I_tilde%*%v)/v
# Print ratios for strictly positive components only    
      print(eigenv[which(abs(v)>0.00000001)])
    }
  }
dim(V)

# Check that kronecker(V_Sigma[,j],V_M[,1]) is eigenvector of M_tilde
for (k in 1:dim(V_M)[1])#k<-3
  for (j in 1:dim(Sigma)[1])#j<-1
  { 
    v<-V[,(k-1)*dim(Sigma)[1]+j]
    eigenv<-as.vector(M_tilde%*%v)/v
# Print ratios for strictly positive components only    
    print(eigenv[which(abs(v)>0.00000001)])
  }


#  V_M_tilde are also eigenvectors of I_tilde
for (k in 1:dim(V_M_tilde)[1])
{  
  v<-V_M_tilde[,k]
  eigenv<-as.vector(I_tilde%*%v)/v
  # Print ratios for strictly positive components only    
  print(eigenv[which(abs(v)>0.00000001)])
  
}

# V_M_tilde and V are both orthonormal bases
max(abs(diag(rep(1,n*L))-t(V)%*%V))
max(abs(diag(rep(1,n*L))-t(V_M_tilde)%*%V_M_tilde))
# Both bases different
k<-1
(V-V_M_tilde)[,k]

#-----------------------------
# define nu
nu_opt<-2.5
nu_mat<-2*M_tilde-nu_opt*I_tilde
# Select a series (1<=k<=n)
k<-min(7,n)

# Solution 1
bk<-solve(nu_mat)%*%I_tilde%*%gamma_target[k,]

# Solution 2: we need to recompute spectral weights based on new reordered basis V
w<-t(V)%*%gamma_target[k,]
# Solution 2 true: Everything is reordered consistently according to theorem in multivariate paper
bkh<-V%*%diag(eigen_I_tilde_sigmaj/(2*eigen_M_tilde_lambdaksigmaj-nu_opt*eigen_I_tilde_sigmaj))%*%t(V)%*%V%*%w

ts.plot(cbind(bk,bkh))

max(abs(bk-bkh))

# Solution 3: parallel subsystems based on reordered eigenvectors, eigenvalues

bkhh<-V%*%diag(1/(2*eigen_M_tilde-nu_opt))%*%t(V)%*%V%*%w

ts.plot(cbind(bk,bkhh))

max(abs(bk-bkhh))

# False Solution 5: parallel subsystems based on original ordering: Problem is 
# 1. we have eigenvalues of M_tilde lambda_k*sigma_j but not in k,j ordering
# 2. Computing lambdak*sigma_j divided by eigenvalues sigma_j of I_tilde is non trivial becaus orderings are different
# 3. Could try repeating lambda_k n-times, as done below: but once again this ordering does not correspond to original ordering: therefore solution is false

w_tilde<-t(V_M_tilde)%*%gamma_target[k,]
eigen_M_tilde_original<-vec(t(matrix(rep(eigen_M,n),nrow=L)))

bkhhh<-V_M_tilde%*%diag(as.vector(1/(2*eigen_M_tilde_original-nu_opt)))%*%t(V_M_tilde)%*%V_M_tilde%*%w_tilde

# Do not match
ts.plot(cbind(bk,bkhhh))
max(abs(bk-bkhh))

@
where the diagonal matrix $\mathbf{D}_{\nu i}^{-1}$ has entries $1/(\tilde{\sigma}_j(2\lambda_{k}-\nu_i))$. We can then solve  \ref{diff_non_hom_matrix_mult} for $\mathbf{b}_i$ and obtain
\begin{eqnarray}\label{diff_non_hom_matrixe_mult}
\mathbf{b}_i&=&D_i\boldsymbol{\tilde{\nu}}_i^{-1}\mathbf{\tilde{I}}\boldsymbol{\gamma}_{i\cdot\delta}\\
&=&D_i\mathbf{\tilde{V}}\mathbf{D}_{\nu i}^{-1}\mathbf{\tilde{V}}' \mathbf{\tilde{I}}\mathbf{\tilde{V}}\mathbf{w}_i\nonumber\\
&=&D_i\sum_{k=1}^L\sum_{j=1}^n \frac{\tilde{\sigma}_jw_{ikj}}{\tilde{\sigma}_j(2\lambda_{k}-\nu_i)}\mathbf{\tilde{v}}_{kj}\nonumber\\
&=&D_i\sum_{k=1}^L\frac{1}{(2\lambda_{k}-\nu_i)}\left(\sum_{j=1}^n w_{ikj}\mathbf{\tilde{v}}_{kj}\right)\label{specdecb_mult0}%\\
%&=&D_i\sum_{k=1}^L\frac{1}{(2\lambda_{k}-\nu_i)}\tilde{w}_{ik}\mathbf{\tilde{\tilde{v}}}_{ik}\label{specdecb_mult}
\end{eqnarray}
%The last equation suggests that the functional one-parametric form $\mathbf{b}_i=\mathbf{b}_i(\nu_i)$ of the SSA-solution does not depend on $\boldsymbol{\Sigma}$\footnote{This result could have been obtained alternatively by multiplying both sides of the Lagrangian equations with $\boldsymbol{\Sigma}^{-1}\otimes\mathbf{I}_{L*L}=\mathbf{\tilde{I}}^{-1}$ and relying on spectral decomposition based on the orthonormal eigenvectors of $\mathbf{I}_{n*n}\otimes\mathbf{M}$ instead of $\boldsymbol{\Sigma}\otimes\mathbf{M}$.}. However, as shown below, the proper solution of the prediction problem will effectively depend on $\boldsymbol{\Sigma}$, namely through the determination of a suitable value $\nu_{0i}$ for the free parameter $\nu_i$. 
Since $\boldsymbol{\tilde{\nu}}_i$ has full rank, the solution of the SSA-problem is uniquely determined by $\nu_i$, at least down to arbitrary scaling, hereby completing the proof of assertion \ref{ass1_mult}.\\
A proof of \ref{spec_dec_rho_mult} follows from \ref{specdecb_mult0}, relying on orthonormality of $\tilde{\mathbf{v}}_{kj}$ 
\begin{eqnarray}
\rho_i(\nu_i)&=&\frac{\mathbf{b}_{i}(\nu_i)'\mathbf{\tilde{M}}\mathbf{b}_{i}(\nu_i)}{\mathbf{b}_{i}(\nu_i)'\mathbf{\tilde{I}}\mathbf{b}_{i}(\nu_i)}\\
&=&\displaystyle{\frac{\left(D_i\sum_{k=1}^L\sum_{j=1}^n  \frac{w_{ikj}}{2\lambda_{k }-\nu_i}\mathbf{\tilde{v}}_{kj}\right)'\mathbf{\tilde{M}}\left(D_i\sum_{k=1}^L\sum_{j=1}^n  \frac{w_{ikj}}{2\lambda_{k }-\nu_i}\mathbf{\tilde{v}}_{kj}\right)}{\left(D_i\sum_{k=1}^L\sum_{j=1}^n  \frac{w_{ikj}}{2\lambda_{k }-\nu_i}\mathbf{\tilde{v}}_{kj}\right)'\mathbf{\tilde{I}}\left(D_i\sum_{k=1}^L\sum_{j=1}^n  \frac{w_{ikj}}{2\lambda_{k }-\nu_i}\mathbf{\tilde{v}}_{kj}\right)}}\nonumber\\
&=&\displaystyle{\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\left(\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2\right)}{\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\left(\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2\right)}}\label{autocorr_mult}  
\end{eqnarray}
%In contrast to $\mathbf{b}_i(\nu_i)$ in \ref{specdecb_mult0}, 
%Note that the lag-one acf $\rho_i(\nu_i)$ of $y_{it}$ depends on $\boldsymbol{\Sigma}$. Therefore, selecting $\nu_i=\nu_{0i}$ such that $\rho_i(\nu_{0i})=\rho_i$ (holding-time constraint) implies that the SSA-solution $\mathbf{b}_i(\nu_{0i})$ generally depends on $\Sigma$, too. 
If $\nu_i\to 2\lambda_k$, then $\rho_i(\nu_i)\to \lambda_k$, for $1\leq k\leq L$ since, by assumption, $\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2>0$ for $k=1,...,L$ in \ref{spec_dec_rho_mult} (spectral completeness and $\tilde{\sigma}_j>0$ by the BMF). Therefore, $|\rho_i(\nu_i)|$ can reach $\rho_{max}(L)$ for $k=1$ or $k=L$. Continuity of $\rho_i(\nu_i)$ and the intermediate value theorem then imply that any $\rho_i$ such that $|\rho_i|<\rho_{max}(L)$ is admissible in the holding-time constraint, as claimed.  \\ 
<<label=init,echo=FALSE,results=hide>>=
#rm(list=ls())
n<-3
set.seed(1)
Sigma<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma%*%t(Sigma)
Sigma
eigen(Sigma)

L<-4
I<-diag(rep(1,L))
M<-matrix(nrow=L,ncol=L)
M[L,]<-rep(0,L)
M[L-1,]<-c(rep(0,L-1),0.5)
for (i in 1:(L-2))
  M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
M<-M+t(M)
# Two possible Kroneckerproducts (product is not commutative)
# This product corresponds to the ordering in paper: epsilon is obtained by appending epsilon_{1t-k}, k=0,...L-1, epsilon_{2t-k},...
Sigma_M<-kronecker(Sigma,M)
# This product corresponds to the other possible ordering: epsilon is obtained by appending epsilon_{it}, i=1,m, epsilon_{it-1},...
M_Sigma<-kronecker(M,Sigma)


I_Sigma<-kronecker(I,Sigma)
Sigma_I<-kronecker(Sigma,I)

V_Sigma_M<-eigen(Sigma_M)$vectors
V_Sigma_I<-eigen(Sigma_I)$vectors


# Orthonormal basis
V_Sigma_I%*%t(V_Sigma_I)
V_Sigma_M%*%t(V_Sigma_M)

# The eigenvectors of Sigma_M are also eigenvectors of Sigma_I
Sigma_I%*%V_Sigma_M/V_Sigma_M

# The eigenvectors of M_Sigma are not eigenvectors of Sigma_M
M_Sigma%*%V_Sigma_M/V_Sigma_M

@
We now proceed to assertion \ref{ass4_mult} by showing that the parameter $\nu_i$ is determined uniquely by $\rho_i$ in the holding-time constraint if $|\nu_i|>2\rho_{max}(L)$. Setting $\tilde{w}_{ik}^2:=\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2$ in \ref{autocorr_mult} we then obtain
\begin{eqnarray}
&&\frac{\partial}{\partial\nu_i}\rho\Big(y_i(\nu_i),y_i(\nu_i),1\Big)=\frac{\partial}{\partial\nu_i}\displaystyle{\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2}{\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2}}\nonumber\\
&=&2\displaystyle{\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^3}\tilde{w}_{ik}^2}{\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2}}-2\displaystyle{\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^3}\tilde{w}_{ik}^2}{\left(\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\right)^2}}\nonumber\\
&=&2\displaystyle{\frac{\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^3}\tilde{w}_{ik}^2\sum_{j=1}^{L}\frac{1}{(2\lambda_j-\nu_i)^2}\tilde{w}_{ij}^2-\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\sum_{j=1}^{L}\frac{1}{(2\lambda_j-\nu_i)^3}\tilde{w}_{ij}^2}{\left(\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\right)^2}}\nonumber\\
&=&2\left(\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^3}\tilde{w}_{ik}^2\sum_{j=1}^{L}\frac{1}{(2\lambda_j-\nu_i)^2}\tilde{w}_{ij}^2-\sum_{k=1}^{L}\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\sum_{j=1}^{L}\frac{1}{(2\lambda_j-\nu_i)^3}\tilde{w}_{ij}^2\right)\label{all_sum}
\end{eqnarray}
where we used the fact that $\sum_{k=1}^{L}\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2=\mathbf{b}_i'\mathbf{\tilde{I}}\mathbf{b}_i=1$. For $k=j$ the summands of the cross-product of both sums vanish in \ref{all_sum}. Therefore, assume $k\neq j$ in \ref{all_sum} and consider
\begin{eqnarray*}
&&\lambda_k\frac{1}{(2\lambda_k-\nu_i)^3}\tilde{w}_{ik}^2\frac{1}{(2\lambda_j-\nu_i)^2}\tilde{w}_{ij}^2-\lambda_k\frac{1}{(2\lambda_k-\nu_i)^2}\tilde{w}_{ik}^2\frac{1}{(2\lambda_j-\nu_i)^3}\tilde{w}_{ij}^2\\
&=&\lambda_k\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^2}\frac{1}{(2\lambda_j-\nu_i)^2}\left(\frac{1}{2\lambda_k-\nu_i}-\frac{1}{2\lambda_j-\nu_i}\right)\\
&=&\lambda_k\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^3}\frac{1}{(2\lambda_j-\nu_i)^3}\left(2\lambda_j-2\lambda_k\right)\\
&=&\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^3}\frac{1}{(2\lambda_j-\nu_i)^3}\left(2\lambda_k\lambda_j-2\lambda_k^2\right)\\
&=&\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^3}\frac{1}{(2\lambda_j-\nu_i)^3}\left(2\lambda_k\lambda_j-2\lambda_j^2\right)\\
\end{eqnarray*}
whereby the last equality is obtained from symmetry, exchanging $k$ and $j$. Therefore, assuming $k\neq j$ and adding the two symmetric terms for $k,j$ and $j,k$ we obtain 
\begin{eqnarray*}
&&\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^3}\frac{1}{(2\lambda_j-\nu_i)^3}2\left(2\lambda_k\lambda_j-\lambda_k^2-\lambda_j^2\right)\\
&=&-2\tilde{w}_{ik}^2\tilde{w}_{ij}^2\frac{1}{(2\lambda_k-\nu_i)^3}\frac{1}{(2\lambda_j-\nu_i)^3}\left(\lambda_k-\lambda_j\right)^2<0
\end{eqnarray*}
where strict inequality follows from $|\nu_i|>2\rho_{max}(L)\geq 2|\lambda_k|$, so that the product in the denominator is strictly positive, from pairwise difference of $\lambda_k$ and from $\tilde{w}_{ik}^2=\sum_{j=1}^{n}\tilde{\sigma}_jw_{ikj}^2>0$ because $\sum_{j=1}^nw_{ikj}^2>0$ (spectral completeness) and $\tilde{\sigma}_j>0$. We then infer that \ref{all_sum} must be strictly negative so that 
\[
\frac{\partial}{\partial\nu_i}\rho\Big(y_i(\nu_i),y_i(\nu_i),1\Big)<0
\]
and therefore $\rho\Big(y_i(\nu_i),y_i(\nu_i),1\Big)$ must be a strictly monotonic function of $\nu_i$ if $|\nu_i|>2\rho_{max}(L)$, as claimed.\\ 
<<label=init,echo=FALSE,results=hide>>=
# Check formula for derivative of rho with respect to nu
len<-11
set.seed(1)
gammak<-rnorm(len)



L<-len
M<-matrix(nrow=L,ncol=L)

M[L,]<-rep(0,L)
M[L-1,]<-c(rep(0,L-1),0.5)
for (i in 1:(L-2))
  M[i,]<-c(rep(0,i),0.5,rep(0,L-1-i))
M<-M+t(M)

eigen(M)$values

M%*%M

eig<-eigen(M)
lambda<-eig$values
V<-eig$vectors

k<-6
M%*%V[,k]-lambda[k]*V[,k]
k<-1
ts.plot(eig$vectors[,k])

nu<-rnorm(1)
nu<-3
Nu<-2*M-nu*diag(rep(1,L))

b<-solve(Nu)%*%gammak

rho0<-t(b)%*%(M%*%b)/(t(b)%*%b)
rho0
w<-solve(V)%*%gammak
sum(w^2*lambda/(2*lambda-nu)^2)/sum(w^2/(2*lambda-nu)^2)

t(b)%*%b
sum(w^2/(2*lambda-nu)^2)



#--------------------------
# Derivative:
delta<-0.0001
nu1<-nu+delta
Nu1<-2*M-nu1*diag(rep(1,L))

b1<-solve(Nu1)%*%gammak

rho1<-t(b1)%*%(M%*%b1)/(t(b1)%*%b1)
# Fourth equation checked
(rho1-rho0)/delta
# Check: should match
(sum(2*w^2*lambda/(2*lambda-nu)^3)*sum(w^2/(2*lambda-nu)^2)-sum(w^2*lambda/(2*lambda-nu)^2)*2*sum(w^2/(2*lambda-nu)^3))/(sum(w^2/(2*lambda-nu)^2))^2



@














\textbf{Remarks}\\
If $|\nu_i|\to \infty$ then $\boldsymbol{\tilde{\nu}_i}/\nu_i\to \mathbf{I}_{nL*nL}$ so that  $\mathbf{b}_i/\boldsymbol{\gamma}_{i\cdot\delta}\to c$ i.e. SSA is asymptotically proportional to MSE (asymptotically degenerate case: the holding-time constraint could be dropped). For $\nu_i\in]-\infty,-2\rho_{max}(L)]$, SSA can accommodate for holding-times $\rho_i<\rho_{MSE}$, where $\rho_{MSE}$ designates the lag-one acf of MSE: a corresponding 'unsmoothing' case is illustrated in section \ref{var1}. For $\nu_i\in[2\rho_{max}(L),\infty[$ SSA can accommodate for longer holding-times $\rho_i>\rho_{MSE}$: corresponding 'smoothing' cases are proposed in sections \ref{examples} and \ref{sign_het}, see also Wildi (2023 a) and (2023 b). For $\nu_i\in]-\infty,-2]$, Wildi (2023 b) shows that SSA can be interpreted as the convolution of a high-pass filter and $\boldsymbol{\gamma}_{i\cdot\delta}$; similarly, for $\nu_i\in[2,\infty[$, SSA is the convolution of a lowpass-filter and $\boldsymbol{\gamma}_{i\cdot\delta}$; for $\nu\in]-2,2[$ SSA is the convolution of a bandpass and $\boldsymbol{\gamma}_{i\cdot\delta}$. While all admissible $\rho_i$ can be addressed by $\nu_i\in ]-\infty,-2\rho_{max}(L)]\cup[2\rho_{max}(L),\infty[$ some 'atypical' forecast problems may require $\nu_i\in]-2\rho_{max}(L),2\rho_{max}(L)[$\footnote{\label{foo_aty}In this case the filter coefficients do not decay to zero for increasing lag, see Wildi (2023 b). This atypical profile might be due to an excessively demanding holding-time constraint ($|\rho_i|$ 'too large') or an 'uncommon' target $\boldsymbol{\gamma}_{i\cdot\delta}$ mostly irrelevant in applications (an example with incomplete spectral support is illustrated in Wildi (2023 b)).}. Finally, %while the arbitrary scaling term is the same across all SSA sub-filters $\mathbf{b}_{ij}$, $j=1,...,n$ of $\mathbf{b}_i$ for fixed $i$, the scale of $\mathbf{b}_i$ could change across targets $z_{it}$, $i=1,...,n$. 
we did not treat explicitly targets $\boldsymbol{\gamma}_{i\cdot\delta}$ with incomplete spectral support mainly because the corresponding singular case is irrelevant in applications. Wildi (2023 b) derives a formal solution which could be extended to a multivariate framework (not done here).  %Also, \ref{ssa_mult} suggests that the mutivariate SSA-filter assigns equal weight $\frac{1}{(2\lambda_{k}-\nu_i)}$ to the slicing or aggregate $\sum_{j=1}^n w_{ikj}\mathbf{\tilde{v}}_{kj}$ across spectral-weights of the vectors $\mathbf{\tilde{v}}_{kj}$ corresponding to $\boldsymbol{\epsilon}_{ij\cdot}=(\epsilon_{i1\cdot},...,\epsilon_{in\cdot})$. Finally, the orthormal basis $\mathbf{\tilde{v}}_{ik}$ and its adjoined subspace $\tilde{\mathbb{R}}_i^{L}$ of $\mathbb{R}^{nL}$ depend on $i$.\\



\begin{Corollary}\label{block_system}
Let the assumptions of theorem \ref{lambda_mult} hold. Then 
\begin{enumerate}
\item The functional one-parametric form of the solution $\mathbf{b}_i=(\mathbf{b}_{i1}',\mathbf{b}_{i2}',...,\mathbf{b}_{in}')'$ to the SSA-optimization problem \ref{crit1_mult} can be obtained alternatively from   \begin{eqnarray}\label{ssa_mult_break}
\mathbf{b}_{ij}=D_i\boldsymbol{{\nu}}_i^{-1}\boldsymbol{\gamma}_{ij\delta}
\end{eqnarray}
for $j=1,...,n$, where $D_i\neq 0$ and $\boldsymbol{{\nu}}_i:=2\mathbf{{M}}-\nu_i\mathbf{{I}}$ is an invertible $L*L$ matrix. The $n$ subsystems are linked by a common $D_i$ and $\nu_i$ but they can differ with respect to the target $\boldsymbol{\gamma}_{ij\delta}$ (straightforward modifications apply in the case of autocorrelated $\mathbf{x}_t$).
\item The SSA solution conforms to the following difference equation
\begin{eqnarray}\label{time_domain_mult}
b_{ijk+1}-\nu_i b_{ijk}+b_{ijk-1}&=&D_i\gamma_{ijk+\delta}~,~0\leq k\leq L-1,~j=1,...,n
\end{eqnarray}
with the boundary constraints $b_{ij,-1}=b_{ijL}=0$, whereby the additional $b_{ij,-1},b_{ijL}$ at lags $k=-1$ and $k=L$ are not explicit components of the solution $\mathbf{b}_i$. 
\end{enumerate}
\end{Corollary}

Proof\\
A proof of the first assertion follows by multiplying both sides of \ref{diff_non_hom_matrix_mult} by $\mathbf{\tilde{I}}^{-1}=\boldsymbol{\Sigma}^{-1}\otimes \mathbf{I}_{L*L}$ such that
\begin{eqnarray*}
D_i\boldsymbol{\gamma}_{i\cdot\delta}&=& \mathbf{\tilde{I}}^{-1}\boldsymbol{\tilde{\nu}}_i\mathbf{b}_i\\
&=&\Big(2(\mathbf{I}_{n*n}\otimes\mathbf{{M}})-\nu_i\mathbf{{I}}_{nL*nL}\Big)\mathbf{b}_i
\end{eqnarray*}
A proof then follows from the block-diagonal shape of $\mathbf{I}_{n*n}\otimes\mathbf{{M}}$. A proof of the second claim follows directly from \ref{ssa_mult_break}, multiplying both sides with $\boldsymbol{{\nu}}_i$.\\

\textbf{Remarks}\\
The criterion value $\rho(y_i,\hat{z}_{i\delta},\delta)$ and the lag-one acf $\rho(y_i,y_i,1)$ generally depend upon $\boldsymbol{\Sigma}$ which cannot be cancelled from numerators and denominators: in particular, $\tilde{\sigma}_j$ cannot be cancelled in \ref{autocorr_mult}. Interestingly,  \ref{ssa_mult_break} suggests that the functional (one-parametric) form of the solution $\mathbf{b}_i(\nu_i)$ does not depend on $\boldsymbol{\Sigma}$: as a confirmation, $\tilde{\sigma}_j$ can be cancelled in \ref{specdecb_mult0}. However, the proper solution $\mathbf{b}_i(\nu_{0i})$, based on an optimal $\nu_{0i}$, is dependent on $\boldsymbol{\Sigma}$ via the determination of $\nu_{0i}$ such that $y_{ti}$ conforms to the holding-time constraint. Specifically, the 'simplified' criterion
\[
\begin{array}{cc}
&\max_{\mathbf{b}_i}\displaystyle{\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\mathbf{b}_{i}}\sqrt{\boldsymbol{\gamma}_{i\cdot\delta}'\boldsymbol{\gamma}_{i\cdot\delta}}}}\\
&\displaystyle{\frac{\mathbf{b}_i'(\mathbf{I}_{n*n}\otimes\mathbf{{M}})\mathbf{b}_i}{\mathbf{b}_{i}'\mathbf{b}_{i}}=\rho_i}
\end{array}
\]
conforms with \ref{ssa_mult_break} (or \ref{specdecb_mult0}), but the holding-time constraint is wrong. On the other hand, the following simplification or alteration of \ref{crit1_mult}  
\[
\begin{array}{cc}
&\max_{\mathbf{b}_i}\displaystyle{\frac{\boldsymbol{\gamma}_{i\cdot\delta}'\mathbf{b}_{i}}{\sqrt{\mathbf{b}_{i}'\mathbf{b}_{i}}\sqrt{\boldsymbol{\gamma}_{i\cdot\delta}'\boldsymbol{\gamma}_{i\cdot\delta}}}}\\
&\displaystyle{\frac{\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i}{\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}}=\rho_i}
\end{array}
\]
can be deconstructed by looking at the corresponding transformed expression  
\begin{eqnarray*}
&&\max_{\mathbf{b}_i}\boldsymbol{\gamma}_{i\cdot\delta}'\mathbf{b}_{i}\\
&&\mathbf{b}_i'\mathbf{\tilde{M}}\mathbf{b}_i=\rho_i\\
&&\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}=1
\end{eqnarray*}
Here, the elliptical length-constraint $\mathbf{b}_{i}'\tilde{\mathbf{I}}\mathbf{b}_{i}=1$ does not ensure that $\mathbf{b}_{i}'\mathbf{b}_{i}=C$ is a fixed constant. Therefore,  $\boldsymbol{\gamma}_{i\cdot\delta}'\mathbf{b}_{i}\not \propto \rho(y_i,\hat{z}_{i\delta},\delta)$ i.e. $\boldsymbol{\gamma}_{i\cdot\delta}'\mathbf{b}_{i}$ is the wrong objective function.\\ %This alteration of the criterion would not conform with \ref{ssa_mult_break} anymore.\\


%The splitting of the solution into blocks in \ref{ssa_mult_break} suggests that the SSA-solution does not directly depend on $\boldsymbol{\Sigma}$. Also, equations \ref{ssa_mult_break} and \ref{time_domain_mult} correspond to frequency-domain and to time-domain specifications of the SSA-problem. We here skip a more comprehensive analysis of the latter, mainly because the former already provides the necessary background for deriving solutions in applications, as proposed in the following corollary. 

%In typical applications one can assume $|\nu_i|>2$ so that $|\nu_i|>2\rho_{max}(L)$, see Wildi (2023 b) for background\footnote{The case $|\nu_i|\leq 2$ would imply that the coefficients of $\mathbf{b}_i$ do not decay towards zero with increasing lag (due to the presence of a unit-root) which suggests evidence of an ill-posed estimation problem, see Wildi (2023 b).}. The resulting monotonicity of the lag-one acf $\rho(y_i(\nu_i),y_1(\nu_i),1)$ as a function of $\nu_i$, facilitates numerical computations, as stated in the following corollary.

\begin{Corollary}\label{lambda_num_gen_mult}
Let the assumptions of theorem \ref{lambda_mult} hold. Then the solution to the SSA-optimization problem is 
\begin{equation}\label{prop_sol_un_unc_fast}
\mathbf{b}_{i}(\nu_{0i})=\textrm{sign}_{\nu_{0i}}\sum_{k=1}^L\frac{1}{(2\lambda_{k}-\nu_{0i})}\tilde{w}_{ik}\mathbf{\tilde{\tilde{v}}}_{ik}
\end{equation}
where $\nu_{0i}$ is a solution of the non-linear equation
\begin{eqnarray}\label{uni_unco_min}
\frac{\mathbf{b}_i(\nu_{0i})'\mathbf{\tilde{M}}\mathbf{b}_i(\nu_{0i})}{\mathbf{b}_i(\nu_{0i})'\mathbf{\tilde{I}}\mathbf{b}_i(\nu_{0i})}=\rho_i
\end{eqnarray}
The sign $\textrm{sign}_{\nu_{0i}}=\pm 1$ is selected such that $\mathbf{b}_i(\nu_{0i})'\mathbf{\tilde{I}}\boldsymbol{\gamma}_{i\cdot\delta}> 0$ (positive criterion value). If $\nu_i$ can be confined to $|\nu_i|>2\rho_{max}(L)$ then the solution to \ref{uni_unco_min} is uniquely determined by $\rho_i$.
\end{Corollary}



A proof follows readily from the above theorem, noting that $\nu_i>2\rho_{max}(L)$ ensures uniqueness. In this case, numerical computations are swift due to strict monotonicity of the lag-one acf $\rho_i(\nu_i)$. If $|\nu_i|\leq 2\rho_{max}(L)$ then strict monotonicity of the lag-one acf and uniqueness of the solution of \ref{uni_unco_min} are compromised, see Wildi (2023 b) for a corresponding 'atypical' example (recall footnote \ref{foo_aty}). To conclude, the following corollary derives mean,  variance and distribution of the SSA-predictor, conditional on the MSE-predictor.  


\begin{Corollary}
Let all regularity assumptions of theorem \ref{lambda_mult} hold and let $\hat{\boldsymbol{\gamma}}_{i\cdot\delta}$ be a finite-sample estimate of the MSE-predictor ${\boldsymbol{\gamma}}_{i\cdot\delta}$ with mean ${\boldsymbol{\mu}}_{\gamma_{i\delta}}$ and variance ${\boldsymbol{\Sigma}}_{\gamma_{i\delta}}$. Then mean and variance of the SSA-predictor $\hat{\mathbf{b}}_i$ are
\begin{eqnarray*}
{\boldsymbol{\mu}}_{\mathbf{b}_i}&=&D_i\boldsymbol{\tilde{\nu}}_i^{-1}\mathbf{\tilde{I}}{\boldsymbol{\mu}}_{\gamma_{i\delta}}\\
{\boldsymbol{\Sigma}}_{\mathbf{b}_i}&=&D_i^2\boldsymbol{\tilde{\nu}}_i^{-1}\mathbf{\tilde{I}}{\boldsymbol{\Sigma}}_{\gamma_{i\delta}}\mathbf{\tilde{I}}\boldsymbol{\tilde{\nu}}_i^{-1}
\end{eqnarray*}
If $\hat{\boldsymbol{\gamma}}_{i\cdot\delta}$ is Gaussian distributed then so is $\hat{\mathbf{b}}_i$. 
\end{Corollary}
The proof readily follows from \ref{ssa_mult}. We then refer to standard textbooks for a derivation of the mean, the variance and the (asymptotic) distribution of the MSE-estimate under various assumptions about $\mathbf{x}_t$, see e.g. Brockwell and Davis (1993).




\section{Elements of Forecasting}\label{examples}

Our examples in this section address forecasting: a simple introductory univariate forecast exercise is proposed in section \ref{one_step_fore} and section \ref{var1} considers applications to a  VAR(1)-process. SSA-solutions are based on corollary \ref{lambda_num_gen_mult} and %, by search of $\lambda\in G$ where $G\subset]-1,1[-\{0\}$ is a finite set of size 1000 %\footnote{Finer resolutions are rarely useful in typical 'non-pathological' applications.} 
%of equidistant grid-points and $\nu:=\lambda+1/\lambda$. The solution $\nu_0$ is such that the absolute error in \ref{uni_unco_min} is minimized on the grid $G$. 
all examples are reproducible in an open source R-package (include link to Github).

\subsection{Univariate MA}\label{one_step_fore}


<<label=init,echo=FALSE,results=hide>>=

ht1<-round((acos(2/3)/pi)^{-1},3)
ht2<-round((acos(1/3)/pi)^{-1},3)
L<-L_short<-20
L_long<-50
ht_large<-10
rho_tt1<-rho_tt1_1<-2/3
# Mean holding-time MA(1): this will be larger/smaller than 2 depending on sign of ma1-coeff (if MA(1) is used)
ht_short<-1/(2*(0.25-asin(rho_tt1)/(2*pi)))
@
We consider a simple forecast exercise of a MA(2)-process
\[z_t=\epsilon_t+\epsilon_{t-1}+\epsilon_{t-2}\]
where $\gamma_k=1,k=0,1,2$ and with forecast horizon $\delta=1$ (one-step ahead). For comparison purposes we compute three different SSA-predictors $y_{ti},i=1,2,3$ for $z_t$: the first two are of identical length $L=\Sexpr{L}$ with dissimilar holding-times  $ht=$\Sexpr{round(ht_short,2)} and \Sexpr{ht_large}; the third predictor deviates from the second one by selecting $L=\Sexpr{L_long}$; the holding-time of the first predictor matches the lag-one autocorrelation of $z_t$  and is obtained by inserting $\rho(z,z,1)=2/3$ into \ref{ht}. In addition, we also consider the MSE forecast $\hat{z}_{t,1}^{MSE}=\epsilon_t+\epsilon_{t-1}$, as obtained by classic time series analysis, as well as a trivial 'lag-by-one' forecast $\hat{z}_{t,1}^{lag~1}=z_t$, see fig. \ref{filt_coef_example1} (an arbitrary scaling scheme is applied to SSA filters). Predictors based on the 'true' MA(2)-model of $z_t$ are virtually indistinguishable from predictors based on a fitted empirical model, see table \ref{perf_ex2e} below.  
<<label=init,echo=FALSE,results=hide>>=

target<-rep(1,3)
gamma_mse<-gammak_generic<-rep(1,2)
forecast_horizon<-1
L_short<-20
L_long<-50
rho0<-as.double(compute_holding_time_func(target)$rho_ff1)
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
with_negative_lambda<-F
grid_size<-1000

# We can specify either target with forecast horizon 1 or mse with forecast horizon 0, see proposition 3 in IJOF paper
#   -In the case of autocorrelated xt (xi is not NULL) it is often more convenient to fit the effective target with the corresponding forecast horizon delta because the function SSA_func makes automatically all adjustments (convolutions/deconvolutions)
# Note also that performances (criterion values with respect to bi-infinite target) improve with smaller delta 
# But relative performances (criterion values with respect to MSE) measured here do not necessarily improve with decreasing delta
# Here MSE nowcast
delta<-0
gamma_target<-gamma_mse
# White noise input
xi<-NULL
# Univariate
Sigma<-NULL
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"
# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F
# New optimization which splits unit-interval in suceesive halves: effective reolution is 2^split_grid: much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$
split_grid<-10

SSA_obj<-SSA_func(split_grid,L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

# Is the same as target forecast
delta<-forecast_horizon
gamma_target<-target
# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F
# New optimization which splits unit-interval in suceesive halves: effective reolution is 2^split_grid: much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$
split_grid<-10

SSA_obj<-SSA_func(split_grid,L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

ht<-ht_second<-10
rho0<-compute_rho_from_ht(ht)$rho
# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F

SSA_obj<-SSA_func(split_grid,L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

# Check holding times
compute_holding_time_func(bk_mat[,1])$ht
compute_holding_time_func(bk_mat[,2])$ht
# Criterion values (correlations)
t(bk_mat)%*%c(gamma_mse,rep(0,L_short-2))/sqrt(apply(bk_mat^2,2,sum)*sum(target^2))

bk_mat<-rbind(bk_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_mat)),ncol=ncol(bk_mat)))

# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F

SSA_obj<-SSA_func(split_grid,L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat=cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[3]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

mplot<-cbind(bk_mat,c(target,rep(0,L_long-3)),c(gamma_mse,rep(0,L_long-2)))
colnames(mplot)[4:5]<-c("Lag-by-one","MSE")
mplot[,1]<-3*mplot[,1]
mplot[,2]<-0.9*mplot[,2]
mplot[,3]<-1.3*mplot[,3]

bk_wn<-mplot

@

<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_example1.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

#mplot<-scale(mplot,scale=T,center=F)
colo<-c("blue","red","violet","black","green")
par(mfrow=c(1,2))
plot(mplot[,1],main="Predictors",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(0,max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot_short<-mplot[1:10,]
plot(mplot_short[,1],main="",axes=F,type="l",xlab="Lag-structure",ylab="",ylim=c(0,max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot_short)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot_short))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot_short[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot_short)[i],col=colo[i],line=-i)
}
axis(1,at=1:nrow(mplot_short),labels=-1+1:nrow(mplot_short))
axis(2)
box()


invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_example1"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{MSE-, SSA- and lag-by-one predictors with arbitrarily scaled SSA-designs. All lags (left panel) and first ten lags (right panel).", sep = "")
cat("\\label{filt_coef_example1}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
Except for the MSE (green) all other forecast-filters rely on past $\epsilon_{t-k}$ for $k>q=2$ which are required for compliance with the holding-time constraint (stronger smoothing). For a fixed filter-length $L$, a larger holding-time $ht$ asks for a slower zero-decay of filter coefficients (blue vs. red lines) and for fixed holding-time $ht$, a larger $L$ leads to a faster zero-decay but a long tail of the filter (red vs. violet lines). The distinguishing tips  of the SSA-predictors at lag one in this example are indicative of the  boundary constraint namely $b_{-1}=0$, see corollary \ref{block_system}.  Note that the 'lag-by-one' forecast (black) has the same holding time as the first SSA-filter (blue) so that the latter should outperform the former in terms of sign accuracy or, equivalently, in terms of correlation with the shifted target, as confirmed in table \ref{perf_ex2}.   
<<label=ats_mba_2,echo=FALSE,results=tex>>=
# Correlations with z_{t+1}
cor_vec<-ht_vec<-proba_vec<-NULL
for (i in 1:ncol(mplot))
{
  cor_vec<-c(cor_vec,  (mplot[1,i]+mplot[2,i])/(sqrt(3)*sqrt(sum(mplot[,i]^2,na.rm=T))))
  ht_vec<-c(ht_vec,compute_holding_time_func(mplot[,i])$ht)
  proba_vec<-c(proba_vec,1-(2*(0.25-asin(cor_vec[length(cor_vec)])/(2*pi))))
}



mat_re<-rbind(cor_vec,ht_vec,proba_vec)
rownames(mat_re)<-c("Correlation with target","Empirical holding-times","Empirical sign accuracy")
colnames(mat_re)[4]<-"Lag-by-one"
mat1<-round(mat_re,3)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(mat1, dec = 1,digits=rep(2,dim(mat_re)[2]+1),
paste("Performances of MSE and lag-by-one  benchmarks vs. SSA: All filters are applied to a sample of length 1000000 of Gaussian noise. Empirical holding-times are obtained by dividing the sample-length by the number of zero-crossings.  "),
label=paste("perf_ex2",sep=""),
center = "centering", file = "", floating = FALSE)
@
MSE outperforms all other forecasts in terms of correlation and sign accuracy  but it loses in terms of  smoothness or holding-time; SSA(\Sexpr{round(ht_first,2)},\Sexpr{delta}) outperforms the lag-by-one benchmark; both SSA(\Sexpr{round(ht_second,2)},\Sexpr{delta}) loose in terms of sign-accuracy but win in terms of smoothness and while the profiles of longer and shorter filters differ in figure \ref{filt_coef_example1}, their respective performances are virtually indistinguishable in table \ref{perf_ex2}, suggesting that the selection of $L$ is not critical (assuming it is at least twice the holding-time). The table also illustrates the tradeoff between MSE- or sign-accuracy performances of optimal designs, in the top and bottom rows, and smoothing-performances in the middle row (an explicit formal link can be obtained but is omitted here). 
<<label=init,echo=FALSE,results=hide>>=
# This is the same code as above but we rely on an estimate of the MSE-target based on a finite sample of zt
set.seed(10)
len<-50
eps<-rnorm(len)
z<-eps[3:len]+eps[2:(len-1)]+eps[1:(len-2)]
acf(z)
arima_obj<-arima(z,order=c(0,0,2))
tsdiag((arima_obj))
target<-c(1,arima_obj$coef[c("ma1","ma2")])
gamma_mse<-arima_obj$coef[c("ma1","ma2")]
if (F)
{ 
# This is the previous setting in the code above  
  target<-rep(1,3)
  gamma_mse<-gammak_generic<-rep(1,2)
}
forecast_horizon<-delta
L_short<-20
L_long<-50
rho0<-as.double(compute_holding_time_func(target)$rho_ff1)
ht<-ht_first<-as.double(compute_holding_time_func(target)$ht)
with_negative_lambda<-F

delta<-0
gamma_target<-gamma_mse
# white-noise input: xi<-NULL
xi<-NULL
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility
# We use this setting which is the default
lower_limit_nu<-"rhomax"
# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F
# New optimization which splits unit-interval in suceesive halves: effective reolution is 2^split_grid: much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$
split_grid<-10

SSA_obj<-SSA_func(split_grid,L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat<-SSA_obj$bk_mat
colnames(bk_mat)<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

ht<-ht_second<-10
rho0<-compute_rho_from_ht(ht)$rho

SSA_obj<-SSA_func(split_grid,L_short,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[2]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

# Check holding times
compute_holding_time_func(bk_mat[,1])$ht
compute_holding_time_func(bk_mat[,2])$ht
# Criterion values (correlations)
t(bk_mat)%*%c(gamma_mse,rep(0,L_short-2))/sqrt(apply(bk_mat^2,2,sum)*sum(target^2))

bk_mat<-rbind(bk_mat,matrix(rep(0,(L_long-L_short)*ncol(bk_mat)),ncol=ncol(bk_mat)))
# Symmetric target: we here have an asymmetric target: common forecasting (no signale xtraction)
symmetric_target<-F

SSA_obj<-SSA_func(split_grid,L_long,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

bk_mat=cbind(bk_mat,SSA_obj$bk_mat)
colnames(bk_mat)[3]<-paste("SSA(",round(ht,2),",",forecast_horizon,")",sep="")

bk_mat_forecast<-bk_mat
mplot<-cbind(bk_mat,c(target,rep(0,L_long-3)),c(gamma_mse,rep(0,L_long-2)))
colnames(mplot)[4:5]<-c("Lag-by-one","MSE")
mplot[,1]<-3*mplot[,1]
mplot[,2]<-0.9*mplot[,2]
mplot[,3]<-1.3*mplot[,3]

@ 
Finally, table \ref{perf_ex2e} displays results when all predictors rely on an empirical model fitted to $z_t$ on a data-sample of length \Sexpr{len}: a comparison of both tables suggests that performances are virtually unaffected by the additional estimation step. 
<<label=ats_mba_2,echo=FALSE,results=tex>>=
# Correlations with z_{t+1}
cor_vec<-ht_vec<-proba_vec<-NULL
for (i in 1:ncol(mplot))
{
  cor_vec<-c(cor_vec,  (mplot[1,i]+mplot[2,i])/(sqrt(3)*sqrt(sum(mplot[,i]^2,na.rm=T))))
  ht_vec<-c(ht_vec,compute_holding_time_func(mplot[,i])$ht)
  proba_vec<-c(proba_vec,1-(2*(0.25-asin(cor_vec[length(cor_vec)])/(2*pi))))
}



mat_re<-rbind(cor_vec,ht_vec,proba_vec)
rownames(mat_re)<-c("Correlation with target","Empirical holding-times","Empirical sign accuracy")
colnames(mat_re)<-colnames(mplot)
mat1<-round(mat_re,3)
#latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
#center = "centering", file = "", floating = FALSE)
xtable(mat1, dec = 1,digits=rep(2,dim(mat_re)[2]+1),
paste("Same case as above but all predictors rely on an empirical model of the MA(2)-process: the model is fitted on a sample of length 50.  "),
label=paste("perf_ex2e",sep=""),
center = "centering", file = "", floating = FALSE)
@














\subsection{Multivariate VAR(1)-Process: Smoothing and Unsmoothing}\label{var1}


<<label=init,echo=FALSE,results=hide>>=
#rm(list=ls())
n<-2
L<-100
set.seed(1)
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
eigen(Sigma)
# Note that performances (criterion values with respect to bi-infinite target) improve with smaller delta 
# But relative performances (criterion values with respect to MSE) measured here do not necessarily improve with decreasing delta
delta<-1
# Target: mix of AR(1), equally-weighted and AR(2) targets
A<-rbind(c(0.7,0.4),c(-0.6,0.9))
det(A)
Ak<-A
gamma_target<-matrix(nrow=dim(A)[1],ncol=L*dim(A)[1])
gamma_target[,L*(0:(dim(A)[1]-1))+1]<-c(1,1)
for (i in 2:L)
{
# The k-th row of gamma_target corresponds to the weights for the k-th target series
# The first L weights in the k-th row are assigned to the first noise series eps_{1t}, eps_{1t-1},...
# The the next L weights (i.e. L+1:L) in the k-th row are assigned to the second noise series eps_{2t}, eps_{2t-1},... 
# This parametrization corresponds to IJFOR paper: see also next examples with heterogenous targets
  gamma_target[,i]<-Ak[,1]
  gamma_target[,i+L]<-Ak[,2]
  Ak<-Ak%*%A
}  

# Hyperparameters for SSA estimation
grid_size<-1000
ht_vec<-matrix(c(min(3,L/2),min(8,L/2)),nrow=1)
rho0<-apply(ht_vec,1,compute_rho_from_ht)[[1]]$rho
with_negative_lambda<-T
# White noise input
xi<-NULL
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
#!!!!!!!!!!!!!!!!!!!!!!!!!!
# Potential problem: multiple solutions for given ht. The procedure so far selects the solution with the tighest holding-time fit: but this could have bad performances
# TO DO: generalize code such that it proposes multiples solutions with large(st) criterion values
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility but no unit-roots
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility: uniqueness and possibly unit-roots
# We use this setting which is the default
lower_limit_nu<-"rhomax"
xi<-NULL
# Symmetric target: we here have an asymmetric target: common forecasting of VAR (no signal extraction)
symmetric_target<-F
# New optimization which splits unit-interval in suceesive halves: effective reolution is 2^split_grid: much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$
split_grid<-10

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)
    
SSA_obj$crit_rhoyz
SSA_obj$crit_rhoyy
SSA_obj$nu_opt
bk_mat<-SSA_obj$bk_mat
bk_x_mat<-SSA_obj$bk_x_mat
gammak_mse<-SSA_obj$gammak_mse
SSA_obj$nu_opt
rhomax<-max(M_func(L,Sigma)$eigen_M_obj$values)

ts.plot(bk_mat)
  
len<-1000000
  
# Takes ~half a minute (simulation with long series)
if (recompute_calculations)  
{  
  
 # cbind(bk_math,bk_mat)
  #----------------------------
  # Generate data
  
  # 1. Noise process with variance Sigma
  
  set.seed(154)
  eps1iid<-rnorm(len)
  eps2iid<-rnorm(len)
  
  eps_mat<-matrix(ncol=dim(Sigma)[1],nrow=len)
      
  # Generate eps with cross-correlation corresponding to Sigma
  eigen_obj<-eigen(Sigma)
  # Square-root of diagonal
  D<-diag(sqrt(eigen_obj$values))
  U<-eigen_obj$vectors
  Sigma_sqrt<-(U)%*%D%*%t(U)
  # Check_ should vanish
  t(Sigma_sqrt)%*%Sigma_sqrt-Sigma
  # Generate eps_mat
  eps_mat<-t(Sigma_sqrt%*%rbind(eps1iid,eps2iid))
  # Check: empirical cov should match Sigma
  cov(eps_mat)
  Sigma
  
  # 2. Generate yt, zt and z_mse from eps_mat
  perf_mat_SSA<-perf_mat_MSE<-NULL
  z_mse_mat<-zdelta_mat<-y_mat<-NULL
  for (m in 1:dim(Sigma)[1])#m<-2
  {
  # Plot coefficients  
    mplot<-cbind(bk_mat[1:L,m],bk_mat[(L+1):(2*L),m])
    colo<-c("blue","red","green")
    ts.plot(mplot,col=colo)
    
  # yt  
  # Specify multivariate filter for m-th series   
    bk<-NULL
    for (j in 1:dim(Sigma)[1])
      bk<-cbind(bk,bk_mat[((j-1)*L+1):(j*L),m])
    y<-rep(NA,len)
    for (j in L:len)
      y[j]<-sum(apply(bk*eps_mat[j:(j-L+1),],2,sum))
    y_mat<-cbind(y_mat,y)
  # Target: zt    
    gammak<-NULL
    for (j in 1:dim(Sigma)[1])
      gammak<-cbind(gammak,gamma_target[m,((j-1)*L+1):(j*L)])
    z<-rep(NA,len)
    for (j in L:len)
      z[j]<-sum(apply(gammak*eps_mat[j:(j-L+1),],2,sum))
# Shift z by delta    
    if (delta>0)
    {  
      zdelta<-c(z[(delta+1):len],rep(0,delta))
    } else
    {
      if (delta<0)
      {
        zdelta<-c(rep(0,delta),z[1:(len-abs(delta))])
      } else
      {
        zdelta<-z
      }
    }
    par(mfrow=c(2,1))
    ts.plot(gammak)
    ts.plot(bk)
    
    ts.plot(scale(cbind(zdelta,y),scale=T)[500:600,],lty=1:2)
    zdelta_mat<-cbind(zdelta_mat,zdelta)
  # Alternative target: MSE (this is used in SSA_func internally)   
    gammak_n<-c(c(gamma_target[m,2:L],0),c(gamma_target[m,L+2:L],0))
    z_mse<-rep(NA,len)
    for (j in L:len)
      z_mse[j]<-sum(apply(gammak_n*eps_mat[j:(j-L+1),],2,sum))
    z_mse_mat<-cbind(z_mse_mat,z_mse)    
#------------
# Performances: empirical and theoretical criterion values and lag-one acfs
    perf_mat_SSA<-rbind(perf_mat_SSA,c(cor(na.exclude(cbind(z_mse[1:len],y[1:len])))[1,2], SSA_obj$crit_rhoyz[m],length(na.exclude(y))/length(which(y[(L+1):len]*y[L:(len-1)]<0)), ht_vec[m]))
    
    perf_mat_MSE<-rbind(perf_mat_MSE,c(length(na.exclude(z_mse))/length(which(z_mse[(L+1):len]*z_mse[L:(len-1)]<0))))
    
    
  # Criterion value is with respect to MSE-target: 
  #   Optimized solutions are identical but criterion measures performances against MSE i.e. against 'benchmark'    
  } 
  colnames(perf_mat_SSA)<-c("Sample crit.","True crit.","Sample ht SSA","True ht SSA")
  
  perf_mat_SSA
  y_mat<-y_mat[1:1000,]
  zdelta_mat<-zdelta_mat[1:1000,]
  z_mse_mat<-z_mse_mat[1:1000,]

#-------------------  
# Additional checks
# 1. Holding-times (lag-one acfs)  
  M_obj<-M_func(L,Sigma)
  
  M_tilde<-M_obj$M_tilde
  I_tilde<-M_obj$I_tilde
  rho_mse_1<-gammak_mse[1,]%*%M_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
  rho_ssa_1<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
  rho_mse_2<-gammak_mse[2,]%*%M_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
  rho_ssa_2<-bk_mat[,2]%*%M_tilde%*%bk_mat[,2]/bk_mat[,2]%*%I_tilde%*%bk_mat[,2]
  
# Check: best approximation on grid should be close to effective holding-time constraints  
  compute_holding_time_from_rho_func(rho_ssa_1)$ht
  compute_holding_time_from_rho_func(rho_ssa_2)$ht
  ht_vec
# 2. Criteria: MSE is trivially one since correlation of MSE with itself is one (our target is MSE which leads to the same solution as using z_{t+\delta})
#   The criteria computed here correspond to the values in perf_mat_SSA above  
  crit_mse_1<-gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
  crit_ssa_1<-gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
  crit_mse_2<-gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
  crit_ssa_2<-gammak_mse[2,]%*%I_tilde%*%bk_mat[,2]/(sqrt(bk_mat[,2]%*%I_tilde%*%bk_mat[,2])*sqrt(gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]))

#-------------------------    
# Save  
  save(file=paste(path.result,"perf_mat_SSA_var1",sep=""),perf_mat_SSA)
  save(file=paste(path.result,"perf_mat_MSE_var1",sep=""),perf_mat_MSE)
  save(file=paste(path.result,"zdelta_mat_var1",sep=""),zdelta_mat)
  save(file=paste(path.result,"z_mse_mat_var1",sep=""),z_mse_mat)
  save(file=paste(path.result,"y_mat_var1",sep=""),y_mat)
} else
{
  load(paste(path.result,"perf_mat_SSA_var1",sep=""))
  load(paste(path.result,"perf_mat_MSE_var1",sep=""))
  load(file=paste(path.result,"zdelta_mat_var1",sep=""))
  load(file=paste(path.result,"z_mse_mat_var1",sep=""))
  load(file=paste(path.result,"y_mat_var1",sep=""))
  
}

perf_all<-cbind(perf_mat_SSA,perf_mat_MSE)
colnames(perf_all)[ncol(perf_all)]<-"Sample ht MSE"
perf_all



@

We here consider the following VAR(1)-target
\[\left(\begin{array}{c}z_{1t}\\z_{2t}\end{array}\right)=\left(\begin{array}{cc}\Sexpr{A[1,1]}&\Sexpr{A[1,2]}\\\Sexpr{A[2,1]}&\Sexpr{A[2,2]}\end{array}\right)\left(\begin{array}{c}z_{1t-1}\\z_{2t-1}\end{array}\right)+\left(\begin{array}{c}\epsilon_{1t}\\\epsilon_{2t}\end{array}\right)~~~,~~~\boldsymbol{\Sigma}=\left(\begin{array}{cc}\Sexpr{round(Sigma[1,1],2)}&\Sexpr{round(Sigma[1,2],2)}\\\Sexpr{round(Sigma[2,1],2)}&\Sexpr{round(Sigma[2,2],2)}\end{array}\right)\]
where $\boldsymbol{\Sigma}$ is the variance-covariance matrix of $\boldsymbol{\epsilon}_t=(\epsilon_{1t},\epsilon_{2t})'$. We compute one-step ahead ($\delta=1$) SSA-predictors of length $L=\Sexpr{L}$ for $z_{1t+1}$ and $z_{2t+1}$ with holding-times  $ht_1=\Sexpr{ht_vec[1]}$, $ht_2=\Sexpr{ht_vec[2]}$. Weights of SSA- and MSE-predictors are displayed in fig.\ref{filt_coef_var1}: the plotted coefficients correspond to the Wold-decomposition of $\mathbf{z}_t$ i.e. they are applied to $\boldsymbol{\epsilon}_{t-k}$, $k\geq 0$.    
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_var1.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)


mplot<-cbind(bk_mat[1:L,1],bk_mat[L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2")
colo<-c("blue","red","violet","black","green")
par(mfrow=c(2,2))
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_mat[1:L,2],bk_mat[L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mse_mat<-cbind(c(gamma_target[1,2:L],0),c(gamma_target[1,L+2:L],0))
mplot<-mse_mat
colnames(mplot)<-c("Series 1","Series 2")
colo<-c("blue","red","violet","black","green")
plot(mplot[,1],main="MSE: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mse_mat<-cbind(c(gamma_target[2,2:L],0),c(gamma_target[2,L+2:L],0))
mplot<-mse_mat
colnames(mplot)<-c("Series 1","Series 2")
plot(mplot[,1],main="MSE: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_var1.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA- (top) and MSE- (bottom) predictors for the first target series $z_{1t+\\delta}$ (left) and the second target series $z_{2t+\\delta}$ (right) with $\\delta=1$ (one step ahead forecasting). Predictor weights as applied to $\\epsilon_{1t-k}$ (blue) $\\epsilon_{2t-k}$ (red)", sep = "")
cat("\\label{filt_coef_var1}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
Performances in terms of theoretical as well as empirical holding-times and criterion-values  are summarized in table \ref{perf_var1}. Empirical measures rely on a sample of length \Sexpr{as.integer(len)} of Gaussian random variables; theoretical numbers are based on proposition \ref{ht_formula} (holding-time) and $\rho(y_i,\hat{z}_{i\delta},\delta)$. The table illustrates that the SSA-predictor of the first target $z_{1t+\delta}$ has a shorter holding-time than the MSE-predictor: the predictor 'unsmooths' the target and accordingly $\nu_1$=\Sexpr{round(SSA_obj$nu_opt[1],2)}$\in]-\infty,-2\rho_{max}(L)]$, whereby $\rho_{max}(L)=\rho_{max}(\Sexpr{L})=\Sexpr{round(rhomax,4)}$,  and the corresponding predictor-weights (top-left panel in fig.\ref{filt_coef_var1})  are overlaid with high-frequency ripples. From the table we also infer that the SSA-predictor of the second target $z_{2t+\delta}$ has a longer holding-time than the MSE-design. Accordingly, $\nu_2$=\Sexpr{round(SSA_obj$nu_opt[2],3)}$\in]2\rho_{max}(L),\infty]$, the predictor 'smooths' the target while performing forecasting and its coefficients retain elements of the MSE-pattern but are otherwise 'lifted away' from the zero line (top-right vs. bottom-right panels in fig.\ref{filt_coef_var1}).  
<<label=ats_mba_2,echo=FALSE,results=tex>>=
rownames(perf_all)<-paste("Series ",1:2,sep="")
xtable(perf_all, dec = 1,digits=rep(2,dim(mat_re)[2]+1),
paste("Performances of SSA- and MSE-predictors: empirical and true holding-times as well as criterion values (correlations of SSA with MSE-predictors).  "),
label=paste("perf_var1",sep=""),
center = "centering", file = "", floating = FALSE)
@





<<label=init,echo=FALSE,results=hide>>=
file = "output_var1.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

anf<-5*L
enf<-6*L
colo<-c("cyan","violet","black")
par(mfrow=c(2,1))
for (i in 1:n)
{
mplot<-cbind(y_mat[anf:enf,i],z_mse_mat[anf:enf,i],zdelta_mat[anf:enf,i])

ts.plot(scale(mplot,center=F,scale=T),col=colo,main=paste("Series ",i,sep=""),lwd=2)
abline(h=0)
abline(v=1+which(mplot[2:nrow(mplot),1]*mplot[1:(nrow(mplot)-1),1]<0),col=colo[1],lty=2)
#abline(v=1+which(mplot[2:nrow(mplot),2]*mplot[1:(nrow(mplot)-1),2]<0),col=colo[2],lty=2)
lines(scale(mplot,center=F,scale=T)[1],col=colo[1])
}
invisible(dev.off())
@
A comparison of $y_{it}$ (\Sexpr{colo[1]}), $\hat{z}_{it\delta}$ (\Sexpr{colo[2]}) and $z_{it+\delta}$ (\Sexpr{colo[3]}) in fig.\ref{output_var1} demonstrates unsmoothing (first series) and smoothing (second series) entailed by the holding-time constraint. 
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "output_var1.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{A comparison of filter outputs: $y_{it}$ (cyan), $\\hat{z}_{it\\delta}$ (violet) and $z_{it+\\delta}$ (black) for $\\delta=1$. Zero-crossings by $y_{it}$  are marked by vertical lines.", sep = "")
cat("\\label{output_var1}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@







\section{Elements of Signal Extraction}\label{sign_het}


Signal extraction commonly refers to the determination and the estimation of so called components or signals $\mathbf{z}_t$ of a time series $\mathbf{x}_t$, for example a trend or a cycle or a seasonally adjusted series. Trend components in particular have received attention because of their implicit long-term outlook. We here propose an alternative trend definition derived from a special case of the SSA-criterion whose formal underpinning is sign-accuracy. We then extend this definition to generic stationary processes, to multivariate processes and finally we propose a hybrid proceeding, mixing established approaches with SSA. The section concludes with a generic signal extraction application which exploits the full flexibility of the approach.  


\subsection{SSA-Trend: an Alternative to Whittaker-Henderson Smoothing}




 Some of the early and classic trend specifications rely on a tradeoff between data-fitting and smoothing as formalized by the so-called  Whittaker-Henderson smoothing or graduation
\begin{eqnarray}\label{whithend}
\min_{\mathbf{b}_i}\sum_{t=L}^T(x_t-y_t)^2+\lambda\sum_{t=L+D}^T (\Delta^D y_t)^2
\end{eqnarray}
where $\Delta^D$ is the difference operator applied $D$-times, see  Orfanidis (2007): for $D=2$ the Hodrick-Prescott (HP-) filter is obtained, see Hodrick and Prescott (1997). The general principle consists in expressing a tradeoff between data-fitting (first term) and data-smoothing (second term) whereby the regularization weight $\lambda$ controls for the balance. An alternative tradeoff can be formalized by a special case of the generic SSA-framework 
\begin{eqnarray}\label{ssa_trend}
\left.\begin{array}{cc}
&\max_{\mathbf{b}} P(\textrm{sign}(x_t)=\textrm{sign}(y_t))\\
&ht(y|\mathbf{b})=ht_1\end{array}\right\}
\end{eqnarray}
where $\delta=0$ and $\gamma_k=\left\{\begin{array}{cc}1&k=0\\0&k\neq 0\end{array}\right.$ which we denote by $(\boldsymbol{\gamma})_k={Id}$ (univariate) or $(\boldsymbol{\Gamma})_k=\mathbf{I}$ (multivariate). Matching signs of the original data $x_t$ by $y_t$ while complying with the holding-time constraint might be considered as an equally pertinent trend proposal, not least in terms of simplicity and intuitive appeal. Indeed, while the hyperparameter $\lambda$ in \ref{whithend} is not self-explanatory and may require additional analysis for optimal selection, see for example  Morten and Uhlig (2002) or Hodrick and Prescott (1997), the holding-time concept and the underlying hyperparameter are straightforward to apprehend, see Wildi (2023 a) for illustration. Also, the data-generating process of $x_t$ can be accounted for by inserting the Wold-decomposition $\xi_k, k\geq 0$, see section \ref{ext_stat} and the examples below. We begin by assuming the simplest case $(\boldsymbol{\Gamma})_k=(\boldsymbol{\Xi})_k=\mathbf{I}$.



\subsection{Univariate SSA-Trend $(\boldsymbol{\gamma})_k=(\boldsymbol{\xi})_k=Id$: a Comparison with HP}\label{HP1}

The HP-filter, obtained by assuming $D=2$ in \ref{whithend}, can be interpreted as an optimal MSE-signal extraction filter for the trend in the smooth trend model, see Harvey (1989). Conceptually, this results in 'implied' models for the cycle and the trend, such that applying the HP filter results in MSE optimal estimates. In this framework, the bi-infinite symmetric expansion of the filter is obtained as 
\begin{equation}\label{hp_eq_tc}
(\gamma_{|k|}B^k)_{|k|<\infty}=\frac{1}{1+\lambda(1-B)^2(1-B^{-1})^2}
\end{equation}
where $\lambda$ is the previous 'smoothing' parameter and where $B,B^{-1}$ are backward and forward operators. The implicit model of $x_t$ is an ARIMA(0,2,2) whose MA-coefficients are determined by $\lambda$.  In finite samples, the filter behaves differently in the middle or toward the  boundaries of the data, where the symmetry is lost: an exact finite sample representation of the one-sided HP-concurrent filter is derived in McElroy (2006).  \\

<<label=init,echo=FALSE,results=hide>>=
# HP
L<-201
lambda_monthly<-14400

HP_obj<-HP_target_mse_modified_gap(L,lambda_monthly)

hp_symmetric=HP_obj$target
hp_one_sided=HP_obj$hp_trend

ts.plot(hp_symmetric)
ts.plot(hp_one_sided)
#---------------------------
# 3. SSA and hyperparameters
# Holding times
ht_obj<-compute_holding_time_func(hp_symmetric)
ht_HP_symmetric<-ht_obj$ht
rho_HP_symmetric<-ht_obj$rho_ff1

ht_obj<-compute_holding_time_func(hp_one_sided)
ht_HP_one_sided<-ht_obj$ht
rho_HP_one_sided<-ht_obj$rho_ff1

#----------------------------
# SSA
# 1: New SSA trend specification: relies on delta=0 and gammak=identity
# Compare trend specification by HP and SSA
# 1.1 One-sided SSA-filter: relies on ht of one-sided HP
delta<-0
with_negative_lambda<-T
lower_limit_nu<-"rhomax"
gamma_target<-1
Sigma<-NULL
split_grid<-20
xi<-NULL
ht_vec<-c(ht_HP_one_sided,20)
# Compute one-sided SSA which replicates ht of one-sided HP
rho0<-compute_rho_from_ht(ht_vec[1])$rho

SSA_obj_one<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

crit_rhoyz1<-SSA_obj_one$crit_rhoyz
crit_rhoyy1<-SSA_obj_one$crit_rhoyy
SSA_obj_one$nu_opt
bk_mat_one<-SSA_obj_one$bk_mat
ts.plot(bk_mat_one)


par(mfrow=c(1,1))
ts.plot(scale(cbind(hp_one_sided,bk_mat_one),center=F,scale=T),col=c("black","blue"))
mtext("SSA based on white noise and same ht as HP-concurrent",col="blue",line=-1)
mtext("HP-concurrent",col="black",line=-2)


# 1.2 Two-sided symmetric SSA: relies on ht of HP symmetric 
rho0<-compute_rho_from_ht(ht_HP_symmetric)$rho
# Backcast: middle point
delta<-delta_two_sided<--(L-1)/2
# Identity
gamma_target<-1

SSA_obj_one<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

crit_rhoyz2<-SSA_obj_one$crit_rhoyz
crit_rhoyy2<-SSA_obj_one$crit_rhoyy
SSA_obj_one$nu_opt
bk_mat_two<-SSA_obj_one$bk_mat
ts.plot(bk_mat_two)

# 1.3 Compute sample performances and verify conformity with theoretical numbers
len<-1000000

if (recompute_calculations)
{
#---------------------------------------
# Compute empirical and theoretical sign accuracies
#----------------------------------------------
# Filtering
  set.seed(16)
  eps<-rnorm(len)
  
  x_mat<-y_HP_mat<-y_SSA1_mat<-y_SSA2_mat<-NULL
  x<-y_HP_one_sided<-y_HP_two_sided<-y_SSA1<-y_SSA2<-rep(NA,len)
  for (i in L:len)
  {
    y_HP_one_sided[i]<-hp_one_sided%*%eps[i:(i-L+1)]
    y_HP_two_sided[i]<-hp_symmetric%*%eps[i:(i-L+1)]
    y_SSA1[i]<-as.vector(bk_mat_one)%*%eps[i:(i-L+1)]
    y_SSA2[i]<-as.vector(bk_mat_two)%*%eps[i:(i-L+1)]
  }

  
  # Correlations
  # Sample correlations
  sample_cor<-c(cor(na.exclude(cbind(eps,y_HP_one_sided)))[1,2],
    cor(na.exclude(cbind(eps,y_SSA1)))[1,2],
    cor(na.exclude(cbind(eps[1:(len-abs(delta_two_sided))],y_HP_two_sided[(1+abs(delta_two_sided)):len])))[1,2],
    cor(na.exclude(cbind(eps[1:(len-abs(delta_two_sided))],y_SSA2[(1+abs(delta_two_sided)):len])))[1,2])

  names(sample_cor)<-c("HP-one",paste("SSA(",round(ht_HP_one_sided,1),")",sep=""),"HP-sym",paste("SSA(",round(ht_HP_symmetric,1),")",sep=""))
  # Theoretical correlations: match!!!
  crit_rhoyz1
  crit_rhoyz2
#   
  sign_accuracies<-c(  0.5+asin(sample_cor)/pi,0.5+asin(crit_rhoyz1)/pi,0.5+asin(crit_rhoyz2)/pi)
  names(sign_accuracies)[5:6]<-paste("True ",names(sample_cor)[c(2,4)],sep="")
  
# Sample holding-times: empirical
  sample_ht<-c((len-L)/length(which(sign(y_HP_one_sided[1:(len-1)]*y_HP_one_sided[2:(len)])<0)),
                     (len-L)/length(which(sign(y_SSA1[1:(len-1)]*y_SSA1[2:(len)])<0)),
                 (len-L)/length(which(sign(y_HP_two_sided[1:(len-1)]*y_HP_two_sided[2:(len)])<0)),
                      (len-L)/length(which(sign(y_SSA2[1:(len-1)]*y_SSA2[2:(len)])<0)))
  names(sample_ht)<-names(sample_cor)
  sample_ht
  # theoretical ht: match
  compute_holding_time_from_rho_func(crit_rhoyy1)
  compute_holding_time_from_rho_func(crit_rhoyy2)
  ht_mat<-c(sample_ht,compute_holding_time_from_rho_func(crit_rhoyy1)$ht,compute_holding_time_from_rho_func(crit_rhoyy2)$ht)
  names(ht_mat)<-names(sign_accuracies)

  save(file=paste(path.out,"sample_cor_SSA_trend_simple",sep=""),sign_accuracies)
  save(file=paste(path.out,"sample_ht_SSA_trend_simple",sep=""),ht_mat)
} else
{
  load(file=paste(path.out,"sample_cor_SSA_trend_simple",sep=""))
  load(file=paste(path.out,"sample_ht_SSA_trend_simple",sep=""))

}
@
<<label=init,echo=FALSE,results=hide>>=
# Generate plots
file = "signal_extraction_HP_SSA_white_noise.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(1,2))
ts.plot(scale(cbind(hp_one_sided,bk_mat_one),center=F,scale=T),col=c("black","blue"),main="One-sided Trend")
mtext("SSA based on white noise and same ht as HP-concurrent",col="blue",line=-1)
mtext("HP-concurrent",col="black",line=-2)
ts.plot(scale(cbind(hp_symmetric,bk_mat_two),center=F,scale=T),col=c("black","blue"),main="Two-sided trend")
mtext("SSA based on white noise and same ht as HP-symmetric",col="blue",line=-1)
mtext("HP-symmetrict",col="black",line=-2)


invisible(dev.off())

@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "signal_extraction_HP_SSA_white_noise.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Trend-filters: HP-concurrent and SSA as based on white noise: both filters have the same holding-time.", sep = "")
cat("\\label{signal_extraction_HP_SSA_white_noise}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
Fig.\ref{signal_extraction_HP_SSA_white_noise} compares one-sided (left panel) and two-sided (right panel) symmetric HP and SSA-trends as based on \ref{ssa_trend}, whereby $\xi_k=0, k>0$ ($x_t=\epsilon_t$) and the holding-times of SSA match the respective holding-times of HP: $ht_1=$\Sexpr{round(ht_HP_one_sided,1)} (one-sided) and $ht_1=$\Sexpr{round(ht_HP_symmetric,1)} (symmetric). %The figure illustrates the rather close proximity of both trend specifications under these circumstances. 
Table \ref{SSA_trend_sample_cor_ht_white_noise} compares sample sign-accuracies and holding-times for samples of length $\Sexpr{as.integer(len)}$ (first four columns) as well as expected numbers (last two columns)  for one-sided trends (columns 1 and 2) and two-sided symmetric trends (columns 3 and 4). %: for identical holding-times, SSA only marginally outperforms HP-trend in terms of sign-accuracy. 
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-rbind(sign_accuracies,ht_mat)
rownames(perf_mat)<-c("Sign accuracy","holding-time")
xtable(round(perf_mat,5), dec = 1,digits=4,
paste("Sample sign accuracy (first row) and holding-times (second row) for HP and SSA one-sided (first two columns) and two-sided (columns 3 and 4). Expected true numbers for SSA in columns 5 and 6 assuming $x_t=\\epsilon_t$"),
label=paste("SSA_trend_sample_cor_ht_white_noise",sep=""),
center = "centering", file = "", floating = FALSE)
@
By design, SSA-trends slightly outperform HP, or any other alternative for that matter, in terms of sign-accuracy and we here argue that this property can serve as a formal justification for the novel trend. The next section proposes an extension to autocorrelated data.



\subsection{Extension of SSA-Trend to Autocorrelated Data: $(\boldsymbol{\gamma})_k=Id$,  $(\boldsymbol{\xi})_k\neq Id$}

<<label=init,echo=FALSE,results=hide>>=

#----------------
# 2. Same as above one-sided but for AR- and MA-processes instead of white noise i.e. xi!=0

# One-sided
delta<-0

# Specify various data generating processes
omega<-pi/20
a<-0.95
data_setting_list<-list(list(a1=0.5,b1=0),list(a1=0,b1=0.8),list(a1=0,b1=rep(0.6,4)),list(a1=c(2*a*cos(omega),-a^2),b1=0))
names(data_setting_list)<-c("AR(1)","MA(1)","MA(4)","AR(2)")
# Second holding-time
ht<-ht_vec[1]
rho0<-compute_rho_from_ht(ht)$rho

bk_mat<-bk_x_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-NULL

for (i in 1:length(data_setting_list))#i<-1
{
  gamma_target<-1
  xi<-ARMAtoMA(ar=data_setting_list[[i]]$a1,ma=c(1,data_setting_list[[i]]$b1),lag.max=L)
  if (F)
  { 
# Gives the same result for delta=0    
    gamma_target<-ARMAtoMA(ar=data_setting_list[[i]]$a1,ma=c(1,data_setting_list[[i]]$b1),lag.max=L)
    xi<-NULL
  }
  SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

  crit_rhoyz_vec<-c(crit_rhoyz_vec,SSA_obj$crit_rhoyz)
  crit_rhoyy_vec<-c(crit_rhoyy_vec,SSA_obj$crit_rhoyy)
  nu_opt_vec<-c(nu_opt_vec,SSA_obj$nu_opt)
  bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
  bk_x_mat<-cbind(bk_x_mat,SSA_obj$bk_x_mat)
}
crit_rhoyz_vec1<-crit_rhoyz_vec
crit_rhoyy_vec1<-crit_rhoyy_vec

colo<-c("blue","red","green","violet")
bk_x_mat1<-bk_x_mat
ts.plot(scale(bk_mat,center=F,scale=T),col=colo)
abline(h=0)
ts.plot(scale(bk_x_mat1,center=F,scale=T),col=colo)
abline(h=0)


ht<-ht_vec[2]
rho0<-compute_rho_from_ht(ht)$rho

bk_mat<-bk_x_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-NULL

for (i in 1:length(data_setting_list))#i<-3
{
  gamma_target<-1
  xi<-ARMAtoMA(ar=data_setting_list[[i]]$a1,ma=c(1,data_setting_list[[i]]$b1),lag.max=L)

  SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

  crit_rhoyz_vec<-c(crit_rhoyz_vec,SSA_obj$crit_rhoyz)
  crit_rhoyy_vec<-c(crit_rhoyy_vec,SSA_obj$crit_rhoyy)
  nu_opt_vec<-c(nu_opt_vec,SSA_obj$nu_opt)
  bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
  bk_x_mat<-cbind(bk_x_mat,SSA_obj$bk_x_mat)

}
crit_rhoyz_vec2<-crit_rhoyz_vec
crit_rhoyy_vec2<-crit_rhoyy_vec
bk_x_mat2<-bk_x_mat
ts.plot(scale(bk_x_mat2,center=F,scale=T),col=colo)
abline(h=0)

len<-100000

if (recompute_calculations)
{
#---------------------------------------
# Compute empirical and theoretical sign accuracies
#----------------------------------------------
# Filtering
  set.seed(16)
  eps<-rnorm(len)
  
  x_mat<-y_HP_mat<-y_SSA1_mat<-y_SSA2_mat<-NULL
  for (k in 1:length(data_setting_list))#k<-3
  {
    gamma_target<-ARMAtoMA(ar=data_setting_list[[k]]$a1,ma=c(1,data_setting_list[[k]]$b1),lag.max=L)
    
    x<-y_HP<-y_SSA1<-y_SSA2<-rep(NA,len)
    for (i in L:len)
    {
      x[i]<-gamma_target%*%eps[i:(i-L+1)]
      y_HP[i]<-hp_one_sided%*%x[i:(i-L+1)]
      y_SSA1[i]<-as.vector(bk_x_mat1[,k])%*%x[i:(i-L+1)]
      y_SSA2[i]<-as.vector(bk_x_mat2[,k])%*%x[i:(i-L+1)]
    }
    y_HP_mat<-cbind(y_HP_mat,y_HP)
    y_SSA1_mat<-cbind(y_SSA1_mat,y_SSA1)
    y_SSA2_mat<-cbind(y_SSA2_mat,y_SSA2)
    x_mat<-cbind(x_mat,x)
  }
  
  # Correlations
  # Sample correlations
  sample_cor<-NULL
  for (k in 1:length(data_setting_list))
  {
  # Correlations: empirical
    sample_cor<-rbind(sample_cor,c(cor(na.exclude(cbind(x_mat[,k],y_HP_mat[,k])))[1,2],
    cor(na.exclude(cbind(x_mat[,k],y_SSA1_mat[,k])))[1,2],
    cor(na.exclude(cbind(x_mat[,k],y_SSA2_mat[,k])))[1,2]))
  }
  rownames(sample_cor)<-names(data_setting_list)
  colnames(sample_cor)<-c("HP",paste("SSA",round(ht_vec,1),sep=""))
  sample_cor
  # Theoretical correlations: match!!!
  crit_rhoyz_vec1
  crit_rhoyz_vec2
  
  sample_ht<-NULL
  for (k in 1:length(data_setting_list))
  {
  # Correlations: empirical
    sample_ht<-rbind(sample_ht,c((len-L)/length(which(sign(y_HP_mat[1:(len-1),k]*y_HP_mat[2:(len),k])<0)),
                     (len-L)/length(which(sign(y_SSA1_mat[1:(len-1),k]*y_SSA1_mat[2:(len),k])<0)),
                      (len-L)/length(which(sign(y_SSA2_mat[1:(len-1),k]*y_SSA2_mat[2:(len),k])<0))))
  }
  rownames(sample_ht)<-names(data_setting_list)
  colnames(sample_ht)<-c("HP",paste("SSA",round(ht_vec,1),sep=""))
  sample_ht
  # theoretical ht
  apply(matrix(crit_rhoyy_vec1,nrow=1),2,compute_holding_time_from_rho_func)
  apply(matrix(crit_rhoyy_vec2,nrow=1),2,compute_holding_time_from_rho_func)

#---------------------------------------------------------------------
# Replicate holding-times of HP by SSA on ARMA-processes and compare correlations
  
  bk_mat<-bk_x_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-NULL
  
  for (i in 1:length(data_setting_list))#i<-1
  {
    gamma_target<-1
    xi<-ARMAtoMA(ar=data_setting_list[[i]]$a1,ma=c(1,data_setting_list[[i]]$b1),lag.max=L)
    if (F)
    { 
  # Gives the same result for delta=0    
      gamma_target<-ARMAtoMA(ar=data_setting_list[[i]]$a1,ma=c(1,data_setting_list[[i]]$b1),lag.max=L)
      xi<-NULL
    }
  # Replicate ht of HP on ARMA-process  
    rho0<-compute_rho_from_ht(sample_ht[i,1])
    SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)
  
    crit_rhoyz_vec<-c(crit_rhoyz_vec,SSA_obj$crit_rhoyz)
    crit_rhoyy_vec<-c(crit_rhoyy_vec,SSA_obj$crit_rhoyy)
    nu_opt_vec<-c(nu_opt_vec,SSA_obj$nu_opt)
    bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
    bk_x_mat<-cbind(bk_x_mat,SSA_obj$bk_x_mat)
  }
  crit_rhoyz_vec1<-crit_rhoyz_vec
  crit_rhoyy_vec1<-crit_rhoyy_vec
  
  ssa_vs_hp_same_ht<-cbind(0.5+asin(sample_cor[,1])/pi,0.5+asin(crit_rhoyz_vec1)/pi)
  colnames(ssa_vs_hp_same_ht)<-paste("Sign accuracy: ",c("HP","SSA"),sep="")
  rownames(ssa_vs_hp_same_ht)<-names(data_setting_list)
  ssa_vs_hp_same_ht
  
  y_SSA_mat<-NULL
  for (k in 1:length(data_setting_list))#k<-3
  {
    gamma_target<-ARMAtoMA(ar=data_setting_list[[k]]$a1,ma=c(1,data_setting_list[[k]]$b1),lag.max=L)
    y_SSA<-rep(NA,len)
    for (i in L:len)
    {
      y_SSA[i]<-as.vector(bk_x_mat[,k])%*%x_mat[i:(i-L+1),k]
    }
    y_SSA_mat<-cbind(y_SSA_mat,y_SSA)
  }
# Check holding-times: should match first column in sample_ht  
  for (k in 1:4)
   print((len-L)/length(which(sign(y_SSA_mat[1:(len-1),k]*y_SSA_mat[2:(len),k])<0)))
  sample_ht
  

  rownames(sample_cor)<-names(data_setting_list)
  colnames(sample_cor)<-c("HP",paste("SSA",round(ht_vec,1),sep=""))
  sample_cor

  save(file=paste(path.out,"sample_cor",sep=""),sample_cor)
  save(file=paste(path.out,"sample_ht",sep=""),sample_ht)
  save(file=paste(path.out,"ssa_vs_hp_same_ht",sep=""),ssa_vs_hp_same_ht)
  save(file=paste(path.out,"y_HP_mat",sep=""),y_HP_mat)
  save(file=paste(path.out,"y_SSA_mat",sep=""),y_SSA_mat)
  save(file=paste(path.out,"x_mat",sep=""),x_mat)
} else
{
  load(file=paste(path.out,"sample_cor",sep=""))
  load(file=paste(path.out,"sample_ht",sep=""))
  load(file=paste(path.out,"ssa_vs_hp_same_ht",sep=""))
  load(file=paste(path.out,"y_HP_mat",sep=""))
  load(file=paste(path.out,"y_SSA_mat",sep=""))
  load(file=paste(path.out,"x_mat",sep=""))
  
}

@

While the solution to the Whittaker-Henderson smoothing is indifferent to the data-generating process, the proposed SSA-trend specification \ref{ssa_trend} can account for autocorrelation by the extension proposed in section \ref{ext_stat}. Fig.\ref{signal_extraction_SSA_ARMA} compares SSA-trends for an \Sexpr{names(data_setting_list)[1]} with parameter $a_1=$\Sexpr{data_setting_list[[1]]$a1}, a \Sexpr{names(data_setting_list)[2]} with parameter $b_1=$\Sexpr{data_setting_list[[2]]$b1}, a \Sexpr{names(data_setting_list)[3]} with parameters $b_1=b_2=b_3=b_4=$\Sexpr{round(data_setting_list[[3]]$b1[1],2)} and an \Sexpr{names(data_setting_list)[4]} with parameters $a_1=$\Sexpr{round(data_setting_list[[4]]$a1[1],2)}, $a_2=$\Sexpr{round(data_setting_list[[4]]$a1[2],2)}; trends are obtained by stipulating two different holding times ht=\Sexpr{round(ht_vec[1],2)} (left panel) and ht=\Sexpr{round(ht_vec[2],2)} (right panel) and the bottom panels offer a closer view on early lags.
<<label=init,echo=FALSE,results=hide>>=
# Generate plots
file = "signal_extraction_HP_SSA_white_noise.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(1,2))
ts.plot(scale(cbind(hp_one_sided,bk_mat_one),center=F,scale=T),col=c("black","blue"),main="One-sided Trend")
mtext("SSA based on white noise and same ht as HP-concurrent",col="blue",line=-1)
mtext("HP-concurrent",col="black",line=-2)
ts.plot(scale(cbind(hp_symmetric,bk_mat_two),center=F,scale=T),col=c("black","blue"),main="Two-sided trend")
mtext("SSA based on white noise and same ht as HP-symmetric",col="blue",line=-1)
mtext("HP-symmetrict",col="black",line=-2)


invisible(dev.off())


file = "signal_extraction_SSA_ARMA.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
colo<-c("blue","red","green","violet")
ts.plot(scale(bk_x_mat1,center=F,scale=T),col=colo)
for (i in 1:length(data_setting_list))
  mtext(names(data_setting_list)[i],col=colo[i],line=-i)
abline(h=0)
ts.plot(scale(bk_x_mat2,center=F,scale=T),col=colo)
for (i in 1:length(data_setting_list))
  mtext(names(data_setting_list)[i],col=colo[i],line=-i)
abline(h=0)
bk_mat2<-bk_mat
ts.plot(scale(bk_x_mat1,center=F,scale=T)[1:20,],col=colo)
for (i in 1:length(data_setting_list))
  mtext(names(data_setting_list)[i],col=colo[i],line=-i)
abline(h=0)
ts.plot(scale(bk_x_mat2,center=F,scale=T)[1:20,],col=colo)
for (i in 1:length(data_setting_list))
  mtext(names(data_setting_list)[i],col=colo[i],line=-i)
abline(h=0)
invisible(dev.off())

@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "signal_extraction_SSA_ARMA.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Trend filters: SSA for various process specifications. All lags (top) and first 20 lags (bottom)", sep = "")
cat("\\label{signal_extraction_SSA_ARMA}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The particular patterns of the 'trend' filters can be explained by a look at table     \ref{SSA_trend_sample_cor_ht} which reports sample holding-times as well as sample correlations $\hat{\rho}(y,z,\delta)=\hat{\rho}(y_t,x_t)$ based on series of length \Sexpr{as.integer(len)}: in contrast to HP (fourth column), SSA maintains control on holding-times which are kept fixed across processes. If the original data is already 'smooth', in the sense that zero-crossings are sparsely distributed, then  the alternating high-frequency pattern of coefficients in fig.\ref{signal_extraction_SSA_ARMA} generates additional zero-crossings in conformity with the imposed constraints. In any case, the data-generating process, formalized by the Wold-decomposition ${\xi}_k\neq{Id}$, affects the SSA-trend for given holding-time.\\
In a next step, we allow SSA to match the variable holding-times of HP, as reported in the fourth column of  table \ref{SSA_trend_sample_cor_ht}: we are then in a position to set-up a meaningful comparison of sign-accuracies for each process, see table \ref{Sign_accuracy_for_equal_ht}. While SSA outperforms HP, by design, the comparison indicates that differences depend on the process as well as on the corresponding holding-time, and can become noticeable in the case of larger $ht_1$ (last row in the table). Fig.\ref{signal_extraction_SSA_ARMA_HP_out} displays filter outputs for the first and last row in table \ref{Sign_accuracy_for_equal_ht}: in the first case (left panel) the series nearly overlap and the particular trend specification is uncritical; in the second case (right panel) SSA differs qualitatively  from HP by a systematic left-shift (lead) at zero-crossings, see Wildi (2023 a) for further evidences of timeliness issues. To summarize, the SSA-trend is close to HP in the case of white noise and equal holding-times, see fig.\ref{signal_extraction_HP_SSA_white_noise}. SSA-trends can markedly differ across data-generating processes for given holding-times. Matching holding-times of SSA to HP implies that the former outperforms the latter in terms of sign-accuracy, by design; moreover, differences can be non-negligible in the case of larger holding-times (stronger smoothing). We then argue that the SSA-trend specification \ref{ssa_trend} might offer a worthwhile alternative to classic Whittaker-Henderson smoothing. The next section analyzes extensions of the proposed SSA-trend to multivariate processes.     
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-cbind(sample_cor,sample_ht)
colnames(perf_mat)[1:3]<-paste("Cor. ",colnames(sample_cor),sep="")
colnames(perf_mat)[3+1:3]<-paste("Ht ",colnames(sample_cor),sep="")
xtable(round(perf_mat,2), dec = 1,digits=2,
paste("Sample correlations $\\hat{\\rho}(y_t,z_t)$ of HP-concurrent and SSA with the data $x_t$ (first three columns) and sample holding-times (last three columns) for various data generating processes."),
label=paste("SSA_trend_sample_cor_ht",sep=""),
center = "centering", file = "", floating = FALSE)
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-ssa_vs_hp_same_ht   
perf_mat<-cbind(perf_mat,sample_ht[,1])
colnames(perf_mat)[3]<-"Common ht"
xtable(round(perf_mat,3), dec = 1,digits=3,
paste("Comparison of sign-accuracies for HP-concurrent against SSA for various processes: holding-times of SSA match holding-times of HP."),
label=paste("Sign_accuracy_for_equal_ht",sep=""),
center = "centering", file = "", floating = FALSE)
@

<<label=init,echo=FALSE,results=hide>>=
file = "signal_extraction_SSA_ARMA_HP_out.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(1,2))
for (i in c(1,4))#i<-1
{
  colo<-c("blue","red","black")
  mplot<-cbind(y_SSA_mat[,i],y_HP_mat[,i],x_mat[,i])
  mplot<-cbind(y_SSA_mat[,i],y_HP_mat[,i])
  ts.plot(scale(mplot,center=F,scale=T)[500:700,],col=colo,main=ifelse(i==1,"AR(1)","AR(2)"))
#  mtext("X",col="black",line=-1)
  mtext("HP",col="red",line=-3)
  mtext(paste("SSA: ht=",round(sample_ht[i,1],1)," matches HP",sep=""),col="blue",line=-2)
  abline(h=0)
} 
invisible(dev.off())
  
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "signal_extraction_SSA_ARMA_HP_out.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Filter outputs: SSA (blue) vs. HP (red)", sep = "")
cat("\\label{signal_extraction_SSA_ARMA_HP_out}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@









\subsection{Multivariate SSA-Trend Specification: $(\boldsymbol{\Gamma})_k=\mathbf{I}$, $(\boldsymbol{\Xi})_k$ Based on VAR(1)}

In contrast to the previous section we here extend the trend-specification to a multivariate VAR(1)-process. The SSA-criterion \ref{ssa_trend} remains unchanged, in particular $\delta=0$ and $\boldsymbol{\Gamma}_k=\left\{\begin{array}{cc}\mathbf{I}&k=0\\\mathbf{0}&k\neq 0\end{array}\right.$ so that $\mathbf{z}_{t+\delta}=\mathbf{x}_t$, but $\mathbf{x}_t$ is now a 3-dimensional VAR(1) 
<<label=init,echo=FALSE,results=hide>>=
# We compute one-sided and symmetric multivariate SSA-trends
n<-3
L<-50
set.seed(1)
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
eigen(Sigma)

#--------------
# 1. Nowcast: one-sided SSA trend
delta<-0
# Target: Gamma is the identity
# Idea: We nowcast the original data for pre-specified holding-times
# The corresponding component could be interpreted as 'trend' 
# Trend specification as an alternative to classic Whittaker-Henderson smoothing
gamma_target<-matrix(rep(0,n^2*L),nrow=n)
gamma_target[1,1]<-gamma_target[2,1+L]<-gamma_target[3,1+2*L]<-1

#----------
# Compute weights of Wold decomposition: VAR(1)
A<-rbind(c(0.7,0.4,-0.2),c(-0.6,0.9,0.3),c(0.5,0.2,-0.3))
B<-diag(rep(0,n))

if (F)
{ 
  A<-diag(rep(0,n))
  B<-0.05*rbind(c(0.5,0.4,-0.4),c(0.6,-0.9,0.3),c(0.8,-0.7,-0.8))
}
det(A)
Ak<-A
Akm1<-diag(rep(1,n))
xi<-matrix(nrow=dim(A)[1],ncol=L*dim(A)[1])
xi[,L*(0:(dim(xi)[1]-1))+1]<-diag(rep(1,n))
for (i in 2:L)
{
  # The k-th row of gamma_target corresponds to the weights for the k-th target series
  # The first L weights in the k-th row are assigned to the first noise series eps_{1t}, eps_{1t-1},...
  # The the next L weights (i.e. L+1:L) in the k-th row are assigned to the second noise series eps_{2t}, eps_{2t-1},... 
  # This parametrization corresponds to IJFOR paper: see also next examples with heteroclite targets
  for (j in 1:n)
    xi[,i+(j-1)*L]<-Ak[,j]+(Akm1%*%B)[,j]
  Ak<-Ak%*%A
  Akm1<-Akm1%*%A
} 


#--------------
# Hyperparameters for SSA estimation
grid_size<-1000
ht_vec<-matrix(c(min(12,L/2),min(9,L/2),min(15,L/2)),nrow=1)
ht_vec<-matrix(c(min(4,L/2),min(3,L/2),min(5,L/2)),nrow=1)
ht_vec<-matrix(c(min(8,L/2),min(6,L/2),min(10,L/2)),nrow=1)
rho0<-apply(ht_vec,1,compute_rho_from_ht)[[1]]$rho
with_negative_lambda<-T
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
#!!!!!!!!!!!!!!!!!!!!!!!!!!
# Potential problem: multiple solutions for given ht. The procedure so far selects the solution with the tighest holding-time fit: but this could have bad performances
# TO DO: generalize code such that it proposes multiples solutions with large(st) criterion values
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility but no unit-roots
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility: uniqueness and possibly unit-roots
# We use this setting which is the default
lower_limit_nu<-"rhomax"
# Symmetric target: Gammak is identity and therefore this setting has no effect on estimate (one can use T or F)
symmetric_target<-F
# New optimization whith half-way triangulation: effective resolution is 2^split_grid. It is much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$. If $|nu|<2*rho_max(L)$ one should use grid-search instead which computes and returns all possible solutions to the holding-time constraint (but computationally much more expensive)
split_grid<-10

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

SSA_obj$crit_rhoyz
SSA_obj$crit_rhoyy
SSA_obj$nu_opt
bk_mat<-SSA_obj$bk_mat
# SSA as applied to xt (deconvolution)
bk_x_mat<-SSA_obj$bk_x_mat
# MSE as applied to epsilont
gammak_mse<-SSA_obj$gammak_mse
# MSE is simply the Wold-decomposition because delta=0
gammak_mse-xi

bk_x_mat[2:L,3]/bk_x_mat[1:(L-1),3]
ts.plot(cbind(bk_x_mat[1:50,1],bk_x_mat[L+1:50,1],bk_x_mat[2*L+1:50,1]))

# 1. Holding-times (lag-one acfs)  
M_obj<-M_func(L,Sigma)

M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde

rho_mse_1<-gammak_mse[1,]%*%M_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
rho_ssa_1<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
rho_mse_2<-gammak_mse[2,]%*%M_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
rho_ssa_2<-bk_mat[,2]%*%M_tilde%*%bk_mat[,2]/bk_mat[,2]%*%I_tilde%*%bk_mat[,2]
rho_mse_3<-gammak_mse[3,]%*%M_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
rho_ssa_3<-bk_mat[,3]%*%M_tilde%*%bk_mat[,3]/bk_mat[,3]%*%I_tilde%*%bk_mat[,3]
# Check: best approximation on grid should be close to effective holding-time constraints  
compute_holding_time_from_rho_func(rho_ssa_1)$ht
compute_holding_time_from_rho_func(rho_ssa_2)$ht
compute_holding_time_from_rho_func(rho_ssa_3)$ht
ht_vec
# Compute ht of MSE
compute_holding_time_from_rho_func(rho_mse_1)$ht
compute_holding_time_from_rho_func(rho_mse_2)$ht
compute_holding_time_from_rho_func(rho_mse_3)$ht

# 2. Criteria: MSE is trivially one since correlation of MSE with itself is one (our target is MSE which leads to the same solution as using z_{t+\delta})
#   The criteria computed here correspond to the values in perf_mat_SSA above  
crit_mse_1<-gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
crit_ssa_1<-gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
crit_mse_2<-gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
crit_ssa_2<-gammak_mse[2,]%*%I_tilde%*%bk_mat[,2]/(sqrt(bk_mat[,2]%*%I_tilde%*%bk_mat[,2])*sqrt(gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]))
crit_mse_3<-gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
crit_ssa_3<-gammak_mse[3,]%*%I_tilde%*%bk_mat[,3]/(sqrt(bk_mat[,3]%*%I_tilde%*%bk_mat[,3])*sqrt(gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]))

criterion_mat<-rbind(c(crit_mse_1,crit_mse_2,crit_mse_3),c(crit_ssa_1,crit_ssa_2,crit_ssa_3))
colnames(criterion_mat)<-c(paste("Series ",1:n,paste=""))
rownames(criterion_mat)<-c("MSE","SSA")
criterion_mat

#--------------------------
# 2. Symmetric SSA-trend
# Backcasting
delta<--L/2

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

crit_sym<-SSA_obj$crit_rhoyz
ht_sym<-SSA_obj$crit_rhoyy
SSA_obj$nu_opt
bk_mat_sym<-SSA_obj$bk_mat
# SSA as applied to xt (deconvolution)
bk_x_mat_sym<-SSA_obj$bk_x_mat
# MSE as applied to epsilont
gammak_mse<-SSA_obj$gammak_mse
# MSE is simply the Wold-decomposition because delta=0
gammak_mse-xi

ts.plot(cbind(bk_x_mat[1:50,1],bk_x_mat[L+1:50,1],bk_x_mat[2*L+1:50,1]))
ts.plot(cbind(bk_x_mat[1:50,2],bk_x_mat[L+1:50,2],bk_x_mat[2*L+1:50,2]))
ts.plot(cbind(bk_x_mat[1:50,3],bk_x_mat[L+1:50,3],bk_x_mat[2*L+1:50,3]))


if (recompute_calculations)
{  
  #----------------------------
  # Generate data
  # Data must be very long for suitable convergence: takes couple minutes to process
  # 1. Noise process with variance Sigma
  len<-1000000
  set.seed(16)
  eps1iid<-rnorm(len)
  eps2iid<-rnorm(len)
  eps3iid<-rnorm(len)
    
  eps_mat<-matrix(ncol=dim(Sigma)[1],nrow=len)
    
  # Generate eps with cross-correlation corresponding to Sigma
  eigen_obj<-eigen(Sigma)
  # Square-root of diagonal
  D<-diag(sqrt(eigen_obj$values))
  U<-eigen_obj$vectors
  Sigma_sqrt<-(U)%*%D%*%t(U)
  # Check_ should vanish
  t(Sigma_sqrt)%*%Sigma_sqrt-Sigma
  # Generate eps_mat
  eps_mat<-t(Sigma_sqrt%*%rbind(eps1iid,eps2iid,eps3iid))
  # Check: empirical cov should match Sigma
  cov(eps_mat)
  Sigma
    
  # Generate x either of two ways: 
  # 1. Wold decomposition
  if (!is.null(xi))
  {  
    x<-rep(0,n)
    x_mat<-matrix(nrow=len,ncol=n)
    for (m in 1:n)#j<-1
    {
      x<-rep(NA,len)
      xi_mat<-cbind(xi[m,1:L],xi[m,L+1:L],xi[m,2*L+1:L])
      for (i in L:len)#i<-L
      {
        x[i]<-sum(apply(xi_mat*eps_mat[i:(i-L+1),],2,sum))
      }
      x_mat[,m]<-x
    }
  } else
  {
    x_mat<-eps_mat
  }
  # Check
  if (F)
  {  
  # 2. VAR(1) equation: note that Xi_0=xi[,(0:(n-1))*L+1] is an identity (otherwise one could account for identity with Sigma)
  #   Therefore we use eps_mat[i,] in the VAR-equation below i.e. e_{it} appears only in the equation of x_{it}   xx_mat<-NULL
    x<-x_mat[L,]
    xx_mat<-matrix(nrow=L,ncol=n)
    for (i in (L+1):len)#i<-L+1
    {
      xx_mat<-rbind(xx_mat,matrix(A%*%x+eps_mat[i,]+B%*%eps_mat[i-1,],nrow=1))
      x<-xx_mat[nrow(xx_mat),]
      x-x_mat[i,]
    }
      
  # Check: both VAR-computations are the same up to negligible errors dur to finite-length of Wold-decomposition   
    ts.plot(cbind(xx_mat[,1],x_mat[,1])[(len-100):len,],lty=1:2)
  }
    
  # 3. Generate yt, zt and z_mse from eps_mat
  perf_mat_sample<-perf_mat_true<-NULL
  y_mat<-zdelta_mat<-z_mse_mat<-NULL
    
  for (m in 1:dim(Sigma)[1])#m<-1
  {
  
  # Generate yt in either of two equivalent ways:        
  # 3.1. convolution of SSA with white noise solution bk_mat (solution of theorem)   
    bk<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
      bk<-cbind(bk,bk_mat[((j-1)*L+1):(j*L),m])
    y<-rep(NA,len)
    for (j in L:len)#j<-L
      y[j]<-sum(apply(bk*eps_mat[j:(j-L+1),],2,sum))
    y_mat<-cbind(y_mat,y[1: min(len,1000)])
  # Check:    
    if (F)
    {
      # 3.2 use deconvoluted solution bk_x_mat applied to xt (obtained from theorem after inversion of convolution) 
      bk<-NULL
      for (j in 1:dim(Sigma)[1])
        bk<-cbind(bk,bk_x_mat[((j-1)*L+1):(j*L),m])
      yh<-rep(NA,len)
      for (j in L:len)
        yh[j]<-sum(apply(bk*(x_mat[j:(j-L+1),]),2,sum))
  
      ts.plot(scale(cbind(y,yh)),lty=1:2)
    }
  # 3.3 Target: MSE-estimate of zt    
    gammak<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
  # in this example gammak_mse is identical with Wold-decomposition (delta=0)
  #  z_mse is just the original data   
      gammak<-cbind(gammak,gammak_mse[m,((j-1)*L+1):(j*L)])
    z_mse<-rep(NA,len)
    for (j in L:len)
      z_mse[j]<-sum(apply(gammak*eps_mat[j:(j-L+1),],2,sum))
    z_mse_mat<-cbind(z_mse_mat,z_mse[1: min(len,1000)])
  # Check: if Gamma is the identity then MSE should match Wold decomposition xi  
    max(abs(gammak_mse-xi))
  # Check: MSE is just X (because delta=0: nowcast)  
    max(abs(na.exclude(z_mse-x_mat[,m])))
  # 3.4 Target: zt+delta    
    gammak<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
  #  gamma_target is original Signal extraction filter as applied to xt     
      gammak<-cbind(gammak,gamma_target[m,((j-1)*L+1):(j*L)])
    z<-rep(NA,len)
  # Truncated bi-infinite sum (acausal filter) 
  # gamma_target is applied to xt (in contrast to gammak_mse above which is convolution of mse with wold-decomposition)    
    for (j in L:(len-L))
      z[j]<-sum(apply(gammak*x_mat[j:(j-L+1),],2,sum))+sum(apply(gammak[-1,]*x_mat[(j+1):(j+L-1),],2,sum))
  # Shift z by delta    
    if (delta>0)
    {  
      zdelta<-c(z[(delta+1):len],rep(0,delta))
    } else
    {
      if (delta<0)
      {
        zdelta<-c(rep(0,delta),z[1:(len-abs(delta))])
      } else
      {
        zdelta<-z
      }
    }
    zdelta_mat<-cbind(zdelta_mat,zdelta[1: min(len,1000)])
  #----------------
  # Series of checks    
    sample_var_target<-var(na.exclude(zdelta))
    sample_var_target
  # Compute theoretical variance of z_mse
    variance_mse<-t(gammak_mse[m,])%*%I_tilde%*%gammak_mse[m,]
  # Check sample and theoretical values    
    variance_mse
    var(na.exclude(z_mse))
  # Criterion value with MSE target    
    gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
  # Criterion value with target: covariance in nominator is the same as with MSE but variance of target changes in denominator: use sample variance of target zdelta    
    gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(sample_var_target))
  # Check: This should fit sample criterion with respect to effective target zdelta (instead of MSE as above)    
    cor(na.exclude(cbind(zdelta,y)))[2]
      
  #------------
  # Performances: empirical and theoretical criterion values and lag-one acfs
    sample_crit_SSA_ref_target<-cor(na.exclude(cbind(zdelta,y)))[2]
    sample_crit_SSA_ref_mse<-cor(na.exclude(cbind(z_mse,y)))[1,2]
    true_crit_SSA_ref_mse<-SSA_obj$crit_rhoyz[m]
  # Since the criterion is not calculated with respect to target in our function we use a trick:
  #   One can replace variance of MSE by variance of target in denominator of correlation
    true_crit_SSA_ref_target<-true_crit_SSA_ref_mse*sqrt(variance_mse)/sqrt(sample_var_target)
    sample_ht_SSA<-length(na.exclude(y))/length(which(y[(L+1):len]*y[L:(len-1)]<0))
    true_ht_SSA<-ht_vec[m]
    sample_ht_MSE<-length(na.exclude(z_mse))/length(which(z_mse[(L+1):len]*z_mse[L:(len-1)]<0))
    acf1_mse<-t(gammak_mse[m,])%*%M_tilde%*%gammak_mse[m,]/t(gammak_mse[m,])%*%I_tilde%*%gammak_mse[m,]
    true_ht_MSE<-pi/acos(acf1_mse)
  # Here we compute sign accuracies i.e. probabilities of same sign (see proof of proposition 1 in paper for 'true' criterion with arcsin transformation)  
    sample_SA_crit_SSA_ref_target<-length(which(sign(zdelta)==sign(y)))/(nrow(na.exclude(cbind(y,zdelta))))
    true_SA_crit_SSA_ref_target<-0.5+asin(true_crit_SSA_ref_target)/pi
      
    perf_mat_sample<-rbind(perf_mat_sample,c(sample_crit_SSA_ref_target,sample_crit_SSA_ref_mse,sample_SA_crit_SSA_ref_target,sample_ht_SSA,sample_ht_MSE))
    perf_mat_true<-rbind(perf_mat_true,c(true_crit_SSA_ref_target,true_crit_SSA_ref_mse,true_SA_crit_SSA_ref_target,true_ht_SSA,true_ht_MSE))
  # Criterion value is with respect to MSE-target: 
  #   Optimized solutions are identical but criterion measures performances against MSE i.e. against 'benchmark'    
  } 
  colnames(perf_mat_sample)<-colnames(perf_mat_true)<-c("Cor. with target","Cor. with MSE","Sign accuracy","ht","ht MSE")
  rownames(perf_mat_sample)<-rownames(perf_mat_true)<-paste("Series ",1:n,sep="")
    
  perf_mat_sample
  perf_mat_true
  perf_mat<-rbind(perf_mat_sample,perf_mat_true)
  cor(na.exclude(x_mat))

  save(file=paste(path.result,"bk_alternative_signal_extraction_mat",sep=""),bk_mat)
  save(file=paste(path.result,"gammak_mse_alternative_signal_extraction",sep=""),gammak_mse)
  save(file=paste(path.result,"perf_mat_alternative_signal_extraction",sep=""),perf_mat)
  save(file=paste(path.result,"y_mat_alternative",sep=""),y_mat)
  save(file=paste(path.result,"zdelta_mat_alternative",sep=""),zdelta_mat)
  save(file=paste(path.result,"z_mse_mat_alternative",sep=""),z_mse_mat)
  save(file=paste(path.result,"x_mat_alternative",sep=""),x_mat)
} else
{
  load(paste(path.result,"bk_alternative_signal_extraction_mat",sep=""))
  load(file=paste(path.result,"gammak_mse_alternative_signal_extraction",sep=""))
  load(paste(path.result,"perf_mat_alternative_signal_extraction",sep=""))
  load(file=paste(path.result,"y_mat_alternative",sep=""))
  load(file=paste(path.result,"zdelta_mat_alternative",sep=""))
  load(file=paste(path.result,"z_mse_mat_alternative",sep=""))
  load(file=paste(path.result,"x_mat_alternative",sep=""))
}
#perf_math: note that we insert the theoretical ht for MSE (which is identical with empirical up to neggligible finite sample issues)
perf_mat_sample<-perf_mat[1:n,]
perf_mat_true<-perf_mat[n+1:n,]
perf_mat_sample
perf_mat_true

@
\[
\mathbf{x}_t=\left(\begin{array}{ccc}
\Sexpr{A[1,1]}&\Sexpr{A[1,2]}&\Sexpr{A[1,3]}\\
\Sexpr{A[2,1]}&\Sexpr{A[2,2]}&\Sexpr{A[2,3]}\\
\Sexpr{A[3,1]}&\Sexpr{A[3,2]}&\Sexpr{A[3,3]}
\end{array}\right)\mathbf{x}_{t-1}+\boldsymbol{\epsilon}_t~,~\boldsymbol{\Sigma}=\left(\begin{array}{ccc}
\Sexpr{round(Sigma[1,1],2)}&\Sexpr{round(Sigma[1,2],2)}&\Sexpr{round(Sigma[1,3],2)}\\
\Sexpr{round(Sigma[2,1],2)}&\Sexpr{round(Sigma[2,2],2)}&\Sexpr{round(Sigma[2,3],2)}\\
\Sexpr{round(Sigma[3,1],2)}&\Sexpr{round(Sigma[3,2],2)}&\Sexpr{round(Sigma[3,3],2)}
\end{array}\right)
\]
with Wold-decomposition $\boldsymbol{Xi}_k=\mathbf{A}^k$. 
We select $L=\Sexpr{L}$, $ht_1=\Sexpr{ht_vec[1]}$, $ht_2=\Sexpr{ht_vec[2]}$ and $ht_3=\Sexpr{ht_vec[3]}$ for the three SSA-trends $y_{1t},y_{2t}$ and $y_{3t}$ whose coefficients  are displayed in fig.\ref{filt_coef_alternative_signal_extraction}, top panels. Interestingly, the coefficients $\mathbf{b}_{i2}$ assigned to  $x_{2t}$ (red), dominate. An explanation for the suggested importance of $x_{2t}$ is provided in the bottom panels of the figure: $x_{2t}$ is systematically leading the other two series as illustrated by the peak-correlations (left and middle bottom panels: the peaks are left-shifted) as well as by the sample filter outputs (bottom right panel: $x_{2t}$ in red is left-shifted). This result confirms the importance of leading time series in a multivariate nowcast framework, at least in terms of sign-accuracy, and it additionally asserts the importance of a lead in the specification of the trend at the current end-point $t=T$ of a finite sample. Table \ref{filt_coef_alternative_signal_extraction} reports expected (true) performance numbers.
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_alternative_signal_extraction.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)


mplot<-cbind(bk_x_mat[1:L,1],bk_x_mat[L+1:L,1],bk_x_mat[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
par(mfrow=c(2,3))
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat[1:L,2],bk_x_mat[L+1:L,2],bk_x_mat[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat[1:L,3],bk_x_mat[L+1:L,3],bk_x_mat[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

lag_max<-20
j<-2
k<-1
pc<-peak_cor_func(x_mat,j,k,lag_max)
j<-3
k<-1
pc<-cbind(pc,peak_cor_func(x_mat,j,k,lag_max))
j<-1
k<-1
pc<-cbind(pc,peak_cor_func(x_mat,j,k,lag_max))
col_vec<-c("red","green","blue")
lwd_vec<-rep(1,3)
lty_vec<-c(2,2,1)
plot(pc[,1],lty=c(2,2,1),main=paste("Peak-cor series ",k,sep=""),axes=F,type="l",xlab="Lag-structure",ylim=c(min(pc),max(pc)))
for (i in 1:ncol(pc))
  lines(pc[,i],col=col_vec[i],lwd=lwd_vec[i],lty=lty_vec[i])
abline(h=0)
abline(v=lag_max)
axis(1,at=1:nrow(pc),labels=-lag_max+1:nrow(pc))
axis(2)
box()
col_vec<-c("red","blue","green")

lag_max<-20
j<-2
k<-3
pc<-peak_cor_func(x_mat,j,k,lag_max)
j<-1
k<-3
pc<-cbind(pc,peak_cor_func(x_mat,j,k,lag_max))
j<-3
k<-3
pc<-cbind(pc,peak_cor_func(x_mat,j,k,lag_max))
plot(pc[,1],lty=c(2,2,1),main=paste("Peak-cor series ",k,sep=""),axes=F,type="l",xlab="Lag-structure",ylim=c(min(pc),max(pc)))
for (i in 1:ncol(pc))
  lines(pc[,i],col=col_vec[i],lwd=lwd_vec[i],lty=lty_vec[i])
abline(h=0)
abline(v=lag_max)
axis(1,at=1:nrow(pc),labels=-lag_max+1:nrow(pc))
axis(2)
box()

colo<-c("blue","red","green")
ts.plot(x_mat[830:900,],col=c("blue","red","green"))
for (i in 1:3)
  mtext(paste("Series ",i,sep=""),col=colo[i],line=-i)


invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_alternative_signal_extraction.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA applied to $x_t$ (deconvolution). Nowcasts of the first target series $x_{1t}$ (left), second target $x_{2t}$ (middle) and third target $x_{3t}$ (right). Filter-weights  applied to $x_{1t}$ (blue) $x_{2t}$ (red) and $x_{3t}$ (green)", sep = "")
cat("\\label{filt_coef_alternative_signal_extraction}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
<<label=init,echo=FALSE,results=hide>>=
file = "output_alternative_signal_extraction.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

anf<-10*L
enf<-11*L
y_mat[anf:enf,]
z_mse_mat[anf:enf,]
zdelta_mat[anf:enf,]
colo<-c("cyan","violet","black")
par(mfrow=c(2,2))
# MSE, target and x are identical: gammak is an identity and delta=0
# We here demonstrate the multivariate SSA trend specification
#   yt matches xt conditional on holding-time constraint
for (i in 1:n)
{
mplot<-cbind(y_mat[anf:enf,i],z_mse_mat[anf:enf,i],zdelta_mat[anf:enf,i])

ts.plot(scale(mplot,center=F,scale=T),col=colo,main=paste("Series ",i,sep=""),lwd=2)
abline(h=0)
abline(v=1+which(mplot[2:nrow(mplot),1]*mplot[1:(nrow(mplot)-1),1]<0),col=colo[1],lty=2)
#abline(v=1+which(mplot[2:nrow(mplot),2]*mplot[1:(nrow(mplot)-1),2]<0),col=colo[2],lty=2)
lines(scale(mplot,center=F,scale=T)[1],col=colo[1])
}
invisible(dev.off())
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-perf_mat_true[,c("Sign accuracy","Cor. with MSE","ht","ht MSE")]
colnames(perf_mat)[c(2:4)]<-c("Cor. with data","ht SSA-trend","ht of data")
xtable(round(perf_mat,2), dec = 1,digits=2,
paste("Expected (true) performances of multivariate SSA-trends: Sign accuracy ${P}\\Big(\\sign(z_{it+\\delta})=\\sign(y_{it})\\Big)=0.5+\\frac{\\arcsin\\left(\\rho(y_i,\\hat{{z}}_{i\\delta},\\delta)\\frac{\\boldsymbol{\\gamma}_{i\\cdot\\delta}'\\tilde{\\mathbf{I}}\\boldsymbol{\\gamma}_{i\\cdot\\delta}}{\\sum_{|k|<\\infty}\\boldsymbol{\\gamma}_{i k}'\\boldsymbol{\\Sigma}\\boldsymbol{\\gamma}_{i k}}\\right)}{\\pi}$, correlation with data ${\\rho}(y_{it},x_{it})$ and holding-time of SSA-trend $ht(y_i|\\mathbf{b}_i)=\\frac{\\pi}{\\arccos(\\rho(y_i,y_i,\\delta))}$ with $\\delta=0$ (nowcast). Expected (true) holding-time of data $x_{it}$ in last Column."),
label=paste("perf_signal_alternative_extraction",sep=""),
center = "centering", file = "", floating = FALSE)
@
Fig.\ref{output_signal_extraction_heteroclite} compares  original data $x_{it}$ (\Sexpr{colo[3]}) and multivariate SSA-trends $y_{it}$ (\Sexpr{colo[1]}) as determined by the specified holding-times. 
%smoothing effect entailed by the holding-time constraint: %We here argue that the trend could be specified such that the frequency of its zero-crossings is kept under control by a hyper-parameter, the holding-time, which can be selected in view of the research purpose of a particular analysis: Wildi (2023 a) examines turning-points of a trend whose  holding-time matches the historical length of recession episodes. \\
%Figure \ref{output_signal_extraction_heteroclite}  also illustrates that 
%stronger smoothing, by SSA, is not necessarily compromised by increased lag, when compared to MSE, and (some of the) noisy crossings of MSE can be avoided by SSA. Wildi (2023 a) and (2023 b) discuss more extensively timeliness-smoothness issues: in particular, it is shown that SSA can outperform MSE in terms of timeliness (relative lead at zero-crossings) as well as in terms of crossing-parsimony (fewer random sign-changes), at once.
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "output_alternative_signal_extraction.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{A comparison of original data (black) and SSA-filters (cyan). Zero-crossings of the trends are marked by vertical lines.", sep = "")
cat("\\label{output_alternative_signal_extraction}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
While the above SSA-trends are pertinent for the endpoint $t=T$ (nowcast) we might also consider symmetric backcasts by setting $\delta=-L/2$, see fig.\ref{filt_coef_alternative_signal_extraction_sym} for corresponding trend coefficients. 
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_alternative_signal_extraction_sym.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)


mplot<-cbind(bk_x_mat_sym[1:L,1],bk_x_mat_sym[L+1:L,1],bk_x_mat_sym[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
par(mfrow=c(1,3))
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat_sym[1:L,2],bk_x_mat_sym[L+1:L,2],bk_x_mat_sym[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat_sym[1:L,3],bk_x_mat_sym[L+1:L,3],bk_x_mat_sym[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()



invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_alternative_signal_extraction_sym.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA applied to $x_t$ (deconvolution). Backcast of the first target series $x_{1t}$ (left), second target $x_{2t}$ (middle) and third target $x_{3t}$ (right). Filter-weights  applied to $x_{1t}$ (blue) $x_{2t}$ (red) and $x_{3t}$ (green)", sep = "")
cat("\\label{filt_coef_alternative_signal_extraction_sym}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The optimal symmetric backcast filters of each series $x_{it}$ correspond to optimal univariate symmetric designs: the weights assigned to non-target series $x_{jt}, j\neq i$ vanish. Table \ref{perf_signal_alternative_extraction_sym} reports performances of the optimal symmetric backcasts.  
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-cbind(0.5+asin(crit_sym)/pi,crit_sym,1/(2*(0.25-asin(ht_sym)/(2*pi))))
colnames(perf_mat)<-c("Sign accuracy","Cor. with data","ht SSA-trend")
rownames(perf_mat)<-paste("Series ",1:3,sep="")
xtable(round(perf_mat,2), dec = 1,digits=2,
paste("Expected (true) performances of multivariate symmetric SSA-trends"),
label=paste("perf_signal_alternative_extraction_sym",sep=""),
center = "centering", file = "", floating = FALSE)
@
As expected, performances improve over one-sided designs in table \ref{perf_signal_alternative_extraction} because the left branches of the symmetric filters reach future observations (acausal designs).  






\subsection{Multivariate SSA-Trend Specification: $(\boldsymbol{\Gamma})_k=\mathbf{I}$, $(\boldsymbol{\Xi})_k$ from VMA(1)}

In order to illustrate the effect of longitudinal as well as of cross-sectional dependence-structures on SSA-trends we here propose two additional VMA(1)-processes. 
<<label=init,echo=FALSE,results=hide>>=
# This is the only change: an additional VMA(1) added to VAR(1) to become a VARMA(1,1)
B<-0.7*rbind(c(0.5,0.4,-0.4),c(0.6,-0.9,0.3),c(0.8,-0.7,-0.8))

# If A!=0 then recompute Wold decomposition: in our example A=0 so that Wold=MA (but we recompute Wold anyway...)
A<-diag(rep(0,dim(A)[1]))
Ak<-A
Akm1<-diag(rep(1,n))
xi<-matrix(nrow=dim(A)[1],ncol=L*dim(A)[1])
xi[,L*(0:(dim(xi)[1]-1))+1]<-diag(rep(1,n))
for (i in 2:L)
{
  # The k-th row of gamma_target corresponds to the weights for the k-th target series
  # The first L weights in the k-th row are assigned to the first noise series eps_{1t}, eps_{1t-1},...
  # The the next L weights (i.e. L+1:L) in the k-th row are assigned to the second noise series eps_{2t}, eps_{2t-1},... 
  # This parametrization corresponds to IJFOR paper: see also next examples with heteroclite targets
  for (j in 1:n)
    xi[,i+(j-1)*L]<-Ak[,j]+(Akm1%*%B)[,j]
  Ak<-Ak%*%A
  Akm1<-Akm1%*%A
} 
# Nowcasts
delta<-0

#--------------

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

SSA_obj$crit_rhoyz
SSA_obj$crit_rhoyy
SSA_obj$nu_opt
# SSA as applied to xt (deconvolution)
bk_x_mat_VARMA<-SSA_obj$bk_x_mat

set.seed(31)
Sigma_sqrt2<-matrix(rnorm(n*n),ncol=n)
Sigma2<-Sigma_sqrt2%*%t(Sigma_sqrt2)
Sigma2

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma2,symmetric_target)

SSA_obj$crit_rhoyz
SSA_obj$crit_rhoyy
SSA_obj$nu_opt
# SSA as applied to xt (deconvolution)
bk_x_mat_VARMA2<-SSA_obj$bk_x_mat


@
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_alternative_signal_extraction_VARMA.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)


mplot<-cbind(bk_x_mat[1:L,1],bk_x_mat[L+1:L,1],bk_x_mat[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
par(mfrow=c(3,3))
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat[1:L,2],bk_x_mat[L+1:L,2],bk_x_mat[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat[1:L,3],bk_x_mat[L+1:L,3],bk_x_mat[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat_VARMA[1:L,1],bk_x_mat_VARMA[L+1:L,1],bk_x_mat_VARMA[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat_VARMA[1:L,2],bk_x_mat_VARMA[L+1:L,2],bk_x_mat_VARMA[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat_VARMA[1:L,3],bk_x_mat_VARMA[L+1:L,3],bk_x_mat_VARMA[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat_VARMA2[1:L,1],bk_x_mat_VARMA2[L+1:L,1],bk_x_mat_VARMA2[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat_VARMA2[1:L,2],bk_x_mat_VARMA2[L+1:L,2],bk_x_mat_VARMA2[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat_VARMA2[1:L,3],bk_x_mat_VARMA2[L+1:L,3],bk_x_mat_VARMA2[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()



invisible(dev.off())
@
The first one is
\[
\mathbf{x}_t=\boldsymbol{\epsilon}_t+\left(\begin{array}{ccc}\Sexpr{round(B[1,1],2)}&\Sexpr{round(B[1,2],2)}&\Sexpr{round(B[1,3],2)}\\
\Sexpr{round(B[2,1],2)}&\Sexpr{round(B[2,2],2)}&\Sexpr{round(B[2,3],2)}\\
\Sexpr{round(B[3,1],2)}&\Sexpr{round(B[3,2],2)}&\Sexpr{round(B[3,3],2)}\end{array}\right)\boldsymbol{\epsilon}_{t-1}
\]
All other settings are pasted from the VAR(1)-case, including $L$, $\boldsymbol{\Sigma}$ and the holding-times. The second VMA(1)-model differs only in terms of $\boldsymbol{\Sigma}=\left(\begin{array}{ccc}
\Sexpr{round(Sigma2[1,1],2)}&\Sexpr{round(Sigma2[1,2],2)}&\Sexpr{round(Sigma2[1,3],2)}\\
\Sexpr{round(Sigma2[2,1],2)}&\Sexpr{round(Sigma2[2,2],2)}&\Sexpr{round(Sigma2[2,3],2)}\\
\Sexpr{round(Sigma2[3,1],2)}&\Sexpr{round(Sigma2[3,2],2)}&\Sexpr{round(Sigma2[3,3],2)}
\end{array}\right)$. Fig.\ref{filt_coef_alternative_signal_extraction_VARMA} compares SSA-trends of the previous VAR(1) (top panel) and the two VMA(1)-models. As shown, the profiles of SSA-trends are reliant upon  longitudinal dependence (VAR(1) vs. VMA(1) top and middle panel-rows) as well as upon cross-sectional dependence (both VMA(1) in middle and bottom panel rows). In all cases, a formal justification of the proposed trends relies on maximized sign-accuracy, with respect to the original data %($\boldsymbol{\gamma})_k=\left\{\begin{array}{cc}1&k=0\\0&k\neq 0\end{array}\right.$) 
and conditional on the imposed holding-time constraints.
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_alternative_signal_extraction_VARMA.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA-trends of VAR(1) (top), VMA(1) with identical $\\boldsymbol{\\Sigma}$ as VAR(1) (middle) and VMA(1) with different $\\boldsymbol{\\Sigma}$ (bottom).", sep = "")
cat("\\label{filt_coef_alternative_signal_extraction_VARMA}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@


\subsection{Hybrid Trend Specification: $(\boldsymbol{\gamma})_k\neq Id$, $(\boldsymbol{\xi})_k=Id$}


<<label=init,echo=FALSE,results=hide>>=

#----------------------------
# 2:  SSA as nowcast or predictor for symmetric HP
# Main difference: relies on gammak=hp_symmetric instead of identity as above
# Note that hp_symmetric shifts the target by L/2 time point to the right (it is a causal filter symmetric around lag k=L/2).
# Therefore we impose forecast horizons delta=0 (instead of delta=-L/2: backcast) and delta=L/2 (instead of delta=0: nowcast)

# 2.1 Symmetric filters
delta_vec<-rep(0,2)
delta<-0
delta_vec[1]<-delta
gamma_target<-hp_symmetric
L<-201

# Compute SSA which replicates ht of one-sided HP
rho0<-rho_HP_one_sided

ht_vec<-c(5,ht_HP_one_sided,12,24,ht_HP_symmetric,2*ht_HP_symmetric)

bk_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-NULL
xi<-NULL
Sigma<-NULL
gamma_target<-hp_symmetric
bk_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-crit_rhoy_target_vec<-NULL
# Need a slightly higher resolution because of the extreme largest holding-time
split_grid<-30
for (i in 1:length(ht_vec))
{  
  rho0<-compute_rho_from_ht(ht_vec[i])$rho

  SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

  crit_rhoyz_vec<-c(crit_rhoyz_vec,SSA_obj$crit_rhoyz)
  crit_rhoyy_vec<-c(crit_rhoyy_vec,SSA_obj$crit_rhoyy)
  crit_rhoy_target_vec<-c(crit_rhoy_target_vec,SSA_obj$crit_rhoy_target)
  nu_opt_vec<-c(nu_opt_vec,SSA_obj$nu_opt)
  bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
}
crit_rhoyz_vec1<-crit_rhoyz_vec
crit_rhoyy_vec1<-crit_rhoyy_vec
bk_mat1<-bk_mat

# 2.2 One-sided filters
delta<-as.integer(L/2)
delta_vec[2]<-delta
bk_mat<-crit_rhoyz_vec<-crit_rhoyy_vec<-nu_opt_vec<-crit_rhoy_target_vec<-NULL
for (i in 1:length(ht_vec))
{  
  rho0<-compute_rho_from_ht(ht_vec[i])$rho

  SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)

  crit_rhoyz_vec<-c(crit_rhoyz_vec,SSA_obj$crit_rhoyz)
  crit_rhoyy_vec<-c(crit_rhoyy_vec,SSA_obj$crit_rhoyy)
  crit_rhoy_target_vec<-c(crit_rhoy_target_vec,SSA_obj$crit_rhoy_target)
  nu_opt_vec<-c(nu_opt_vec,SSA_obj$nu_opt)
  bk_mat<-cbind(bk_mat,SSA_obj$bk_mat)
}
crit_rhoyz_vec2<-crit_rhoyz_vec
crit_rhoyy_vec2<-crit_rhoyy_vec
crit_rhoy_target_vec2<-crit_rhoy_target_vec
bk_mat2<-bk_mat
# Can set resolution back to 20
split_grid<-20

@
<<label=init,echo=FALSE,results=hide>>=
# Plots
file = "signal_extraction_SSA_HP_hybrid.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
colo=c("red","blue","green","orange","violet","cyan")
ts.plot(scale(bk_mat1,scale=T,center=F),col=colo,xlab="Lags",main=paste("Forecast horizon ",delta_vec[1]-(L-1)/2,sep=""))
abline(h=0)
for (i in 1:length(ht_vec))
  mtext(paste("ht=",round(ht_vec[i],2),sep=""),col=colo[i],line=-i,side=1)


ts.plot(scale(bk_mat2,scale=T,center=F),col=colo,xlab="Lags",main=paste("Forecast horizon ",delta_vec[2]-(L-1)/2,sep=""))
abline(h=0)
for (i in 1:length(ht_vec))
  mtext(paste("ht=",round(ht_vec[i],2),sep=""),col=colo[i],line=-i)
invisible(dev.off())


@
The previous sections compared various SSA-trends based on a special case \ref{ssa_trend} of the SSA-criterion \ref{gen_stat_x} where the target $\mathbf{z}_{t+\delta}=\mathbf{x}_t$ was identified with the data i.e. $(\boldsymbol{\Gamma})_k=\mathbf{I}$. In contrast, we here  identify $(\gamma)_{|k|\leq L}$ with the  symmetric HP-trend, assuming $\lambda=\Sexpr{lambda_monthly}$ (monthly setting). We select $L=\Sexpr{L}$, and we propose modifications of the symmetric filter profile of HP as well as one-sided nowcasts matching some pre-specified holding-time constraints.  For simplicity of exposition we assume $x_t=\epsilon_t$ so that $(\boldsymbol{\xi})_k=Id$. 'Pseudo'-symmetric SSA-designs are obtained by selecting $\delta=-(L-1)/2=\Sexpr{(L-1)/2}$ (backcasting) so that $b_{\Sexpr{(L-1)/2+1}-k}=b_{\Sexpr{(L-1)/2+1}+k}$ (symmetric about lag $k=\Sexpr{(L-1)/2+1}$). On the other hand, nowcasts rely on $\delta=0$, see also Wildi (2023 a) for a corresponding application.  
Fig.\ref{signal_extraction_SSA_HP_hybrid} displays alterations of the symmetric HP (left panel) as well as one-sided nowcasts (right panel) for holding-times ranging from $ht=\Sexpr{round(ht_vec[1],1)}$ up to $ht=\Sexpr{round(ht_vec[length(ht_vec)],1)}$: for comparison purposes, the holding-time of the original symmetric HP is \Sexpr{round(ht_HP_symmetric,2)}. 
<<label=signal_extraction_SSA_HP_hybrid.pdf,echo=FALSE,results=tex>>=
file = "signal_extraction_SSA_HP_hybrid.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Alterations of HP-symmetric by SSA-backcasts matching a selection of holding-times (left panel). Nowcasts of HP-symmetric for various holding-times (right panel)", sep = "")
cat("\\label{signal_extraction_SSA_HP_hybrid}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
%The filter profiles in the right panel can be compared with fig.\ref{signal_extraction_HP_SSA_white_noise} since both assume white noise. However, the former are a nowcast of HP-symmetric whereas the latter are a nowcast of the identity, subject to a holding-time constraint.
<<label=init,echo=FALSE,results=hide>>=

#----------------------------------------------
# Filtering
if (recompute_calculations)
{
  len<-1000000
  set.seed(1324)
  eps<-rnorm(len)
  
  y_HP_one_sided<-y_HP_two_sided<-y_SSA1<-y_SSA2<-rep(NA,len)
# Apply hp_symmetric without a shift:  y_HP_two_sided will be shifted by L/2 to the right 
  for (i in L:len)#i<-L/2+1
  {
    y_HP_two_sided[i]<-hp_symmetric%*%eps[i:(i-L+1)]
  } 
# Apply hp_symmetric without a shift:  y_HP_two_sided will be shifted by L/2 to the right 
  for (i in L:len)#i<-L/2+1
  {
    y_HP_one_sided[i]<-hp_one_sided%*%eps[i:(i-L+1)]
  } 
# We have to shift y_HP_two_sided by delta_vec in order to match y_HP_one_sided    
  cor_HP_HP<-cor(na.exclude(cbind(y_HP_two_sided[(delta_vec[2]+1):(len-delta_vec[2])],y_HP_one_sided[1:(len-2*delta_vec[2])])))[1,2]
  0.5+asin(cor_HP_HP)/pi

  perf_mat<-NULL
  for (k in 1:ncol(bk_mat1))#k<-2
  {
    for (i in L:len)
    {
      y_SSA1[i]<-as.vector(bk_mat1[,k])%*%eps[i:(i-L+1)]
      y_SSA2[i]<-as.vector(bk_mat2[,k])%*%eps[i:(i-L+1)]
    }

# We have to shift y_HP_two_sided by delta_vec in order to match SSA    
    cor(na.exclude(cbind(y_HP_two_sided[(delta_vec[1]+1):(len-delta_vec[1])],y_SSA1[1:(len-2*delta_vec[1])])))[1,2]
# If delta is zero then the sample correlation matches the theoretical correlation
#  Theoretical correlation is between SSA and MSE: if delta=0 then MSE is HP-symmetric
    crit_rhoyz_vec1[k]
# We have to shift y_HP_two_sided by delta_vec in order to match SSA    
    cor(na.exclude(cbind(y_HP_two_sided[(delta_vec[2]+1):(len-delta_vec[2])],y_SSA2[1:(len-2*delta_vec[2])])))[1,2]
# If delta is not zero (as ist the case here) then the sample correlation does not match the theoretical correlation
#  Theoretical correlation is between SSA and MSE: if delta>0 then MSE!=HP-symmetric
    crit_rhoyz_vec2[k]
# This is the correct criterion: can be compared with sample cor above    
    crit_rhoy_target_vec2[k]    
    perf_mat<-rbind(perf_mat,c(    cor(na.exclude(cbind(y_HP_two_sided[(delta_vec[1]+1):(len-delta_vec[1])],y_SSA1[1:(len-2*delta_vec[1])])))[1,2],
      crit_rhoyz_vec1[k],
      (len-L)/length(which(sign(y_SSA1[1:(len-1)]*y_SSA1[2:(len)])<0)),
cor(na.exclude(cbind(y_HP_two_sided[(delta_vec[2]+1):(len-delta_vec[2])],y_SSA2[1:(len-2*delta_vec[2])])))[1,2],
crit_rhoy_target_vec2[k],
(len-L)/length(which(sign(y_SSA2[1:(len-1)]*y_SSA2[2:(len)])<0))))

  }  
  
  colnames(perf_mat)<-c("Sample cor.","true cor.","sample ht","sample cor.","true cor.","sample ht")
  rownames(perf_mat)<-paste("ht=",round(ht_vec,1),sep="")
  perf_mat
  
  save(perf_mat,file=paste(path.out,"perf_mat_se_hybrid",sep=""))
  save(cor_HP_HP,file=paste(path.out,"cor_HP_HP",sep=""))
  
} else
{
  load(file=paste(path.out,"perf_mat_se_hybrid",sep=""))
  load(file=paste(path.out,"cor_HP_HP",sep=""))
 
}

@
Table \ref{SSA_trend_sample_cor_ht_sym_one_sided} compares empirical and theoretical objective functions (sign accuracy with respect to the symmetric HP-trend) and holding-times: performances of symmetric backcast SSA-filters ($\delta=\Sexpr{delta_vec[1]-(L-1)/2}$) are reported in the first three columns; performances of one-sided nowcasts ($\delta=\Sexpr{delta_vec[2]-(L-1)/2}$) are in the next three columns; the 'true' imposed holding-times are in the last column for reference. These performances can be benchmarked against the classic HP-concurrent nowcast, recall section \ref{HP1}, with sign-accuracy \Sexpr{round(0.5+asin(cor_HP_HP)/pi,3)} and holding-time \Sexpr{round(ht_vec[2],3)}, the latter matching the second row in the table. Interestingly, SSA nowcasts (fourth column) outperform HP-concurrent for holding times up to \Sexpr{round(ht_vec[4],3)} i.e. roughly three times the latter's own $ht$, see also Wildi (2023 a) for a more comprehensive analysis involving timeliness (relative lead/lags) also. As expected, sign accuracies are much larger for the symmetric backcasts in the first two columns since the left tail of the filters reaches future observations: in particular, sign accuracies in the fifth row are exactly one since the imposed holding-time $ht=\Sexpr{round(ht_vec[5],1)}$ matches HP-symmetric so that SSA optimization leads to a perfect replication of the former (so-called degenerate case in theorem \ref{lambda_mult}). For holding-times smaller than $ht=\Sexpr{round(ht_vec[5],1)}$, the filters 'unsmooth' the target, as indicated by the presence of high-frequency ripples overlaying the corresponding profiles in fig.\ref{signal_extraction_SSA_HP_hybrid}, left panel.
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat[,c(1,2,4,5)]<-0.5+asin(perf_mat[,c(1,2,4,5)])/pi
perf_mat<-cbind(perf_mat,ht_vec)
colnames(perf_mat)<-c("Sample SA","True SA","Sample ht","Sample SA","True SA","Sample ht","Imposed ht")
xtable(round(perf_mat,3), dec = 1,digits=3,
paste("Sample and true sign-accuracies and sample holding-times for $\\delta=-100$ (first three columns: symmetric backcast filters) and $\\delta=0$ (next three columns: one-sided nowcast filters). Imposed holding-times in last column."),
label=paste("SSA_trend_sample_cor_ht_sym_one_sided",sep=""),
center = "centering", file = "", floating = FALSE)
@
Once again, a formal justification for the above filters relies on maximization of sign-accuracy but in contrast to previous sections, where the 'target' was the original data, the target here is the output $z_{t+\delta}$ of the symmetric HP with $\delta=\Sexpr{delta_vec[1]-(L-1)/2}$ (symmetric designs) or $\delta=\Sexpr{delta_vec[2]-(L-1)/2}$ (one-sided designs). % Clearly, this setting could be applied to any alternative target (trend-)specification. 










\subsection{Combining Signal Extraction and SSA: $(\boldsymbol{\Gamma})_k\neq \mathbf{Id}$, $(\boldsymbol{\Xi})_k\neq \mathbf{Id}$}



We here exploit the full flexibility of the approach, allowing both $(\boldsymbol{\Gamma})_k\neq \mathbf{Id}$ and $(\boldsymbol{\Xi})_k\neq \mathbf{Id}$ to differ from identities. In addition to Gaussian data we also  explore the effect of heavy tails on SSA performance measures. 


\subsubsection{Gaussian Data}

<<label=init,echo=FALSE,results=hide>>=
#rm(list=ls())
  
n<-3
L<-50
set.seed(1)
Sigma_sqrt<-matrix(rnorm(n*n),ncol=n)
Sigma<-Sigma_sqrt%*%t(Sigma_sqrt)
Sigma
eigen(Sigma)
# Note that performances (criterion values with respect to bi-infinite target) improve with smaller delta 
# But relative performances (criterion values with respect to MSE) measured here do not necessarily improve with decreasing delta
delta<-0
# Target: mix of AR(1), equally-weighted and AR(2) targets
L_ma1<-min(7,L)
L_ma2<-min(5,L)
L_ma3<-min(9,L)
a11<-0.7
a131<-1.5
a132<--0.8
a231<-0.5
a232<--0.9
a32<-0.9
a331<-1.2
a332<--0.6
# Target z_{1t}: we concatenate three filters of length L each which correspond to epsilon_{1t},epsilon_{2t} and epsilon_{3t}
gamma_target_1<-c(a11^(1:L),c(rep(1/L_ma1,L_ma1),rep(0,L-L_ma1)),ARMAtoMA(ar=c(a131,a132),lag.max=L))
# Target z_{2t}
gamma_target_2<-c(c(rep(1/L_ma2,L_ma2),rep(0,L-L_ma2)),c(rep(1/L_ma3,L_ma3),rep(0,L-L_ma3)),ARMAtoMA(ar=c(a231,a232),lag.max=L))
# Target z_{3t}
gamma_target_3<-c(c(rep(1/L_ma3,L_ma3),rep(0,L-L_ma3)),a32^(1:L),ARMAtoMA(ar=c(a331,a332),lag.max=L))
# Bind to target for vector z_t
gamma_target<-rbind(gamma_target_1,gamma_target_2,gamma_target_3)

#----------
# Compute weights of Wold decomposition
A<-rbind(c(0.7,0.4,-0.2),c(-0.6,0.9,0.3),c(0.5,0.2,-0.3))
det(A)
Ak<-A
xi<-matrix(nrow=dim(A)[1],ncol=L*dim(A)[1])
xi[,L*(0:(dim(xi)[1]-1))+1]<-diag(rep(1,n))
for (i in 2:L)
{
# The k-th row of gamma_target corresponds to the weights for the k-th target series
# The first L weights in the k-th row are assigned to the first noise series eps_{1t}, eps_{1t-1},...
# The the next L weights (i.e. L+1:L) in the k-th row are assigned to the second noise series eps_{2t}, eps_{2t-1},... 
# This parametrization corresponds to IJFOR paper: see also next examples with heteroclite targets
  for (j in 1:n)
    xi[,i+(j-1)*L]<-Ak[,j]
  Ak<-Ak%*%A
} 

#--------------
# Hyperparameters for SSA estimation
grid_size<-1000
ht_vec<-matrix(c(min(3.900156,L/2),min(2.581775,L/2),min(4.213013,L/2)),nrow=1)
ht_vec<-matrix(c(min(8,L/2),min(6,L/2),min(10,L/2)),nrow=1)
rho0<-apply(ht_vec,1,compute_rho_from_ht)[[1]]$rho
with_negative_lambda<-T
# nu>0 : all nu possible in grid from 2/sqrt(gridsize)~0 to sqrt(gridsize)~infty
#!!!!!!!!!!!!!!!!!!!!!!!!!!
# Potential problem: multiple solutions for given ht. The procedure so far selects the solution with the tighest holding-time fit: but this could have bad performances
# TO DO: generalize code such that it proposes multiples solutions with large(st) criterion values
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  
lower_limit_nu<-"0"
# nu>2 we do not achieve the limit of admissibility but no unit-roots
lower_limit_nu<-"2"
# nu>2*rhomax(L) i.e. we achieve the limit of admissibility: uniqueness and possibly unit-roots
# We use this setting which is the default
lower_limit_nu<-"rhomax"
# Symmetric target: we here have a symmetric signal extraction filter
symmetric_target<-T
# New optimization whith half-way triangulation: effective resolution is 2^split_grid. It is much faster than brute-force grid-search. Makes use of monotonicity of lag-one acf if $|nu|>2*rho_max(L)$. If $|nu|<2*rho_max(L)$ one should use grid-search instead which computes and returns all possible solutions to the holding-time constraint (but computationally much more expensive)
split_grid<-10

SSA_obj<-SSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)
    
SSA_obj$crit_rhoyz
SSA_obj$crit_rhoyy
SSA_obj$nu_opt

# SSA as applied to epsilont (convolution)
bk_mat<-SSA_obj$bk_mat
# SSA as applied to xt (deconvolution)
bk_x_mat<-SSA_obj$bk_x_mat
# MSE as applied to epsilont
gammak_mse<-SSA_obj$gammak_mse

# 1. Holding-times (lag-one acfs)  
M_obj<-M_func(L,Sigma)
  
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde

rho_mse_1<-gammak_mse[1,]%*%M_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
rho_ssa_1<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
rho_mse_2<-gammak_mse[2,]%*%M_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
rho_ssa_2<-bk_mat[,2]%*%M_tilde%*%bk_mat[,2]/bk_mat[,2]%*%I_tilde%*%bk_mat[,2]
rho_mse_3<-gammak_mse[3,]%*%M_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
rho_ssa_3<-bk_mat[,3]%*%M_tilde%*%bk_mat[,3]/bk_mat[,3]%*%I_tilde%*%bk_mat[,3]
# Check: best approximation on grid should be close to effective holding-time constraints  
compute_holding_time_from_rho_func(rho_ssa_1)$ht
compute_holding_time_from_rho_func(rho_ssa_2)$ht
compute_holding_time_from_rho_func(rho_ssa_3)$ht
ht_vec
# Compute ht of MSE
compute_holding_time_from_rho_func(rho_mse_1)$ht
compute_holding_time_from_rho_func(rho_mse_2)$ht
compute_holding_time_from_rho_func(rho_mse_3)$ht

# 2. Criteria: MSE is trivially one since correlation of MSE with itself is one (our target is MSE which leads to the same solution as using z_{t+\delta})
#   The criteria computed here correspond to the values in perf_mat_SSA above  
crit_mse_1<-gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
crit_ssa_1<-gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
crit_mse_2<-gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
crit_ssa_2<-gammak_mse[2,]%*%I_tilde%*%bk_mat[,2]/(sqrt(bk_mat[,2]%*%I_tilde%*%bk_mat[,2])*sqrt(gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]))
crit_mse_3<-gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
crit_ssa_3<-gammak_mse[3,]%*%I_tilde%*%bk_mat[,3]/(sqrt(bk_mat[,3]%*%I_tilde%*%bk_mat[,3])*sqrt(gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]))

criterion_mat<-rbind(c(crit_mse_1,crit_mse_2,crit_mse_3),c(crit_ssa_1,crit_ssa_2,crit_ssa_3))
colnames(criterion_mat)<-c(paste("Series ",1:n,paste=""))
rownames(criterion_mat)<-c("MSE","SSA")
criterion_mat



# Empirical measures and checks of convolution/deconvolutions
if (recompute_calculations)
{  

#----------------------------
# Generate data
# Data must be very long for suitable convergence: takes couple minutes to process
# 1. Noise process with variance Sigma
  len<-1000000
  set.seed(16)
  eps1iid<-rnorm(len)
  eps2iid<-rnorm(len)
  eps3iid<-rnorm(len)
    
  eps_mat<-matrix(ncol=dim(Sigma)[1],nrow=len)
    
# Generate eps with cross-correlation corresponding to Sigma
  eigen_obj<-eigen(Sigma)
# Square-root of diagonal
  D<-diag(sqrt(eigen_obj$values))
  U<-eigen_obj$vectors
  Sigma_sqrt<-(U)%*%D%*%t(U)
# Check_ should vanish
  t(Sigma_sqrt)%*%Sigma_sqrt-Sigma
# Generate eps_mat
  eps_mat<-t(Sigma_sqrt%*%rbind(eps1iid,eps2iid,eps3iid))
# Check: empirical cov should match Sigma
  cov(eps_mat)
  Sigma
  
# Generate x either of two ways: 
# 1. Wold decomposition
  if (!is.null(xi))
  {  
    x<-rep(0,n)
    x_mat<-matrix(nrow=len,ncol=n)
    for (m in 1:n)#j<-1
    {
      x<-rep(NA,len)
      xi_mat<-cbind(xi[m,1:L],xi[m,L+1:L],xi[m,2*L+1:L])
      for (i in L:len)#i<-L
      {
        x[i]<-sum(apply(xi_mat*eps_mat[i:(i-L+1),],2,sum))
      }
      x_mat[,m]<-x
    }
  } else
  {
    x_mat<-eps_mat
  }
# Check
  if (F)
  {  
# 2. VAR(1) equation: note that Xi_0=xi[,(0:(n-1))*L+1] is an identity (otherwise one could account for identity with Sigma)
#   Therefore we use eps_mat[i,] in the VAR-equation below i.e. e_{it} appears only in the equation of x_{it}   xx_mat<-NULL
    x<-x_mat[L,]
    xx_mat<-matrix(nrow=L,ncol=n)
    for (i in (L+1):len)#i<-L+1
    {
      xx_mat<-rbind(xx_mat,matrix(A%*%x+eps_mat[i,],nrow=1))
      x<-xx_mat[nrow(xx_mat),]
      x-x_mat[i,]
    }

# Check: both VAR-computations are the same up to negligible errors dur to finite-length of Wold-decomposition   
    ts.plot(cbind(xx_mat[,1],x_mat[,1])[(len-100):len,],lty=1:2)
  }
  
# 3. Generate yt, zt and z_mse from eps_mat
  perf_mat_sample<-perf_mat_true<-NULL
  y_mat<-zdelta_mat<-z_mse_mat<-NULL
  
  for (m in 1:dim(Sigma)[1])#m<-1
  {
    print(m)

# Generate yt in either of two equivalent ways:        
# 3.1. convolution of SSA with white noise solution bk_mat (solution of theorem)   
    bk<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
      bk<-cbind(bk,bk_mat[((j-1)*L+1):(j*L),m])
    y<-rep(NA,len)
    for (j in L:len)#j<-L
      y[j]<-sum(apply(bk*eps_mat[j:(j-L+1),],2,sum))
    y_mat<-cbind(y_mat,y[1: min(len,1000)])
# Check:    
    if (F)
    {
# 3.2 use deconvoluted solution bk_x_mat applied to xt (obtained from theorem after inversion of convolution) 
      bk<-NULL
      for (j in 1:dim(Sigma)[1])
        bk<-cbind(bk,bk_x_mat[((j-1)*L+1):(j*L),m])
      yh<-rep(NA,len)
      for (j in L:len)
        yh[j]<-sum(apply(bk*(x_mat[j:(j-L+1),]),2,sum))
      
      ts.plot(scale(cbind(y,yh)),lty=1:2)
    }
# 3.3 Target: MSE-estimate of zt    
    gammak<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
# gammak_mse is convolution of MSE with Wold-decomposition: must be applied to epsilont      
      gammak<-cbind(gammak,gammak_mse[m,((j-1)*L+1):(j*L)])
    z_mse<-rep(NA,len)
    for (j in L:len)
      z_mse[j]<-sum(apply(gammak*eps_mat[j:(j-L+1),],2,sum))
    z_mse_mat<-cbind(z_mse_mat,z_mse[1: min(len,1000)])

# 3.4 Target: zt+delta    
    gammak<-NULL
    for (j in 1:dim(Sigma)[1])#j<-1
#  gamma_target is original Signal extraction filter as applied to xt     
      gammak<-cbind(gammak,gamma_target[m,((j-1)*L+1):(j*L)])
    z<-rep(NA,len)
# Truncated bi-infinite sum (acausal filter) 
# gamma_target is applied to xt (in contrast to gammak_mse above which is convolution of mse with wold-decomposition)    
    for (j in L:(len-L))
      z[j]<-sum(apply(gammak*x_mat[j:(j-L+1),],2,sum))+sum(apply(gammak[-1,]*x_mat[(j+1):(j+L-1),],2,sum))
# Shift z by delta    
    if (delta>0)
    {  
      zdelta<-c(z[(delta+1):len],rep(0,delta))
    } else
    {
      if (delta<0)
      {
        zdelta<-c(rep(0,delta),z[1:(len-abs(delta))])
      } else
      {
        zdelta<-z
      }
    }
    zdelta_mat<-cbind(zdelta_mat,zdelta[1: min(len,1000)])
#----------------
# Series of checks    
    sample_var_target<-var(na.exclude(zdelta))
    sample_var_target
# Compute theoretical variance of z_mse
    variance_mse<-t(gammak_mse[m,])%*%I_tilde%*%gammak_mse[m,]
# Check sample and theoretical values    
    variance_mse
    var(na.exclude(z_mse))
# Criterion value with MSE target    
    gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
# Criterion value with target: covariance in nominator is the same as with MSE but variance of target changes in denominator: use sample variance of target zdelta    
    gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(sample_var_target))
# Check: This should fit sample criterion with respect to effective target zdelta (instead of MSE as above)    
    cor(na.exclude(cbind(zdelta,y)))[2]

#------------
# Performances: empirical and theoretical criterion values and lag-one acfs
    sample_crit_SSA_ref_target<-cor(na.exclude(cbind(zdelta,y)))[2]
    sample_crit_SSA_ref_mse<-cor(na.exclude(cbind(z_mse,y)))[1,2]
    true_crit_SSA_ref_mse<-SSA_obj$crit_rhoyz[m]
# Since the criterion is not calculated with respect to target in our function we use a trick:
#   One can replace variance of MSE by variance of target in denominator of correlation
    true_crit_SSA_ref_target<-true_crit_SSA_ref_mse*sqrt(variance_mse)/sqrt(sample_var_target)
    sample_ht_SSA<-length(na.exclude(y))/length(which(y[(L+1):len]*y[L:(len-1)]<0))
    true_ht_SSA<-ht_vec[m]
    sample_ht_MSE<-length(na.exclude(z_mse))/length(which(z_mse[(L+1):len]*z_mse[L:(len-1)]<0))
    acf1_mse<-t(gammak_mse[m,])%*%M_tilde%*%gammak_mse[m,]/t(gammak_mse[m,])%*%I_tilde%*%gammak_mse[m,]
    true_ht_MSE<-pi/acos(acf1_mse)
# Here we compute sign accuracies i.e. probabilities of same sign (see proof of proposition 1 in paper for 'true' criterion with arcsin transformation)  
    sample_SA_crit_SSA_ref_target<-length(which(sign(zdelta)==sign(y)))/(nrow(na.exclude(cbind(y,zdelta))))
    true_SA_crit_SSA_ref_target<-0.5+asin(true_crit_SSA_ref_target)/pi
    
    perf_mat_sample<-rbind(perf_mat_sample,c(sample_crit_SSA_ref_target,sample_crit_SSA_ref_mse,sample_SA_crit_SSA_ref_target,sample_ht_SSA,sample_ht_MSE))
    perf_mat_true<-rbind(perf_mat_true,c(true_crit_SSA_ref_target,true_crit_SSA_ref_mse,true_SA_crit_SSA_ref_target,true_ht_SSA,true_ht_MSE))
# Criterion value is with respect to MSE-target: 
#   Optimized solutions are identical but criterion measures performances against MSE i.e. against 'benchmark'    
  } 
  colnames(perf_mat_sample)<-colnames(perf_mat_true)<-c("Cor. with target","Cor. with MSE","Sign accuracy","ht","ht MSE")
  rownames(perf_mat_sample)<-rownames(perf_mat_true)<-paste("Series ",1:n,sep="")

  perf_mat_sample
  perf_mat_true
  perf_mat<-rbind(perf_mat_sample,perf_mat_true)
#-------------------------  
# Additional checks
# 1. Holding-times (lag-one acfs)  
  M_obj<-M_func(L,Sigma)
  
  M_tilde<-M_obj$M_tilde
  I_tilde<-M_obj$I_tilde

  rho_mse_1<-gammak_mse[1,]%*%M_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
  rho_ssa_1<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
  rho_mse_2<-gammak_mse[2,]%*%M_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
  rho_ssa_2<-bk_mat[,2]%*%M_tilde%*%bk_mat[,2]/bk_mat[,2]%*%I_tilde%*%bk_mat[,2]
  rho_mse_3<-gammak_mse[3,]%*%M_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
  rho_ssa_3<-bk_mat[,3]%*%M_tilde%*%bk_mat[,3]/bk_mat[,3]%*%I_tilde%*%bk_mat[,3]
# Check: best approximation on grid should be close to effective holding-time constraints  
  compute_holding_time_from_rho_func(rho_ssa_1)$ht
  compute_holding_time_from_rho_func(rho_ssa_2)$ht
  compute_holding_time_from_rho_func(rho_ssa_3)$ht
  ht_vec
# Compute ht of MSE
  compute_holding_time_from_rho_func(rho_mse_1)$ht
  compute_holding_time_from_rho_func(rho_mse_2)$ht
  compute_holding_time_from_rho_func(rho_mse_3)$ht

# 2. Criteria: MSE is trivially one since correlation of MSE with itself is one (our target is MSE which leads to the same solution as using z_{t+\delta})
#   The criteria computed here correspond to the values in perf_mat_SSA above  
  crit_mse_1<-gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
  crit_ssa_1<-gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
  crit_mse_2<-gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
  crit_ssa_2<-gammak_mse[2,]%*%I_tilde%*%bk_mat[,2]/(sqrt(bk_mat[,2]%*%I_tilde%*%bk_mat[,2])*sqrt(gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]))
  crit_mse_3<-gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
  crit_ssa_3<-gammak_mse[3,]%*%I_tilde%*%bk_mat[,3]/(sqrt(bk_mat[,3]%*%I_tilde%*%bk_mat[,3])*sqrt(gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]))

  criterion_mat<-rbind(c(crit_mse_1,crit_mse_2,crit_mse_3),c(crit_ssa_1,crit_ssa_2,crit_ssa_3))
  colnames(criterion_mat)<-c(paste("Series ",1:n,paste=""))
  rownames(criterion_mat)<-c("MSE","SSA")
  criterion_mat

  save(file=paste(path.result,"bk_signal_extraction_mat_heteroclite",sep=""),bk_mat)
  save(file=paste(path.result,"gammak_mse_signal_extraction_heteroclite",sep=""),gammak_mse)
  save(file=paste(path.result,"perf_mat_signal_extraction_heteroclite",sep=""),perf_mat)
  save(file=paste(path.result,"y_mat",sep=""),y_mat)
  save(file=paste(path.result,"zdelta_mat",sep=""),zdelta_mat)
  save(file=paste(path.result,"z_mse_mat",sep=""),z_mse_mat)
} else
{
  load(paste(path.result,"bk_signal_extraction_mat_heteroclite",sep=""))
  load(file=paste(path.result,"gammak_mse_signal_extraction_heteroclite",sep=""))
  load(paste(path.result,"perf_mat_signal_extraction_heteroclite",sep=""))
  load(file=paste(path.result,"y_mat",sep=""))
  load(file=paste(path.result,"zdelta_mat",sep=""))
  load(file=paste(path.result,"z_mse_mat",sep=""))

  
}
#perf_math: note that we insert the theoretical ht for MSE (which is identical with empirical up to neggligible finite sample issues)
perf_mat_sample<-perf_mat[1:n,]
perf_mat_true<-perf_mat[n+1:n,]
perf_mat_sample
perf_mat_true

@
<<label=init,echo=FALSE,results=hide>>=
file = "filt_coef_signal_extraction_heteroclite.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)


mplot<-cbind(bk_x_mat[1:L,1],bk_x_mat[L+1:L,1],bk_x_mat[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
par(mfrow=c(3,3))
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_x_mat[1:L,2],bk_x_mat[L+1:L,2],bk_x_mat[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_x_mat[1:L,3],bk_x_mat[L+1:L,3],bk_x_mat[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()





mplot<-cbind(bk_mat[1:L,1],bk_mat[L+1:L,1],bk_mat[2*L+1:L,1])
colnames(mplot)<-c("Series 1","Series 2","Series 3")
colo<-c("blue","red","green","violet","black")
plot(mplot[,1],main="SSA: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mplot<-cbind(bk_mat[1:L,2],bk_mat[L+1:L,2],bk_mat[2*L+1:L,2])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()

mplot<-cbind(bk_mat[1:L,3],bk_mat[L+1:L,3],bk_mat[2*L+1:L,3])
colnames(mplot)<-c("Series 1","Series 2","Series 3")

plot(mplot[,1],main="SSA: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mse_mat<-cbind(c(gammak_mse[1,2:L],0),c(gammak_mse[1,L+2:L],0),c(gammak_mse[1,2*L+2:L],0))
mplot<-mse_mat
colnames(mplot)<-c("Series 1","Series 2","Series 3")
plot(mplot[,1],main="MSE: first target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mse_mat<-cbind(c(gammak_mse[2,2:L],0),c(gammak_mse[2,L+2:L],0),c(gammak_mse[2,2*L+2:L],0))
mplot<-mse_mat
colnames(mplot)<-c("Series 1","Series 2","Series 3")
plot(mplot[,1],main="MSE: second target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


mse_mat<-cbind(c(gammak_mse[3,2:L],0),c(gammak_mse[3,L+2:L],0),c(gammak_mse[3,2*L+2:L],0))
mplot<-mse_mat
colnames(mplot)<-c("Series 1","Series 2","Series 3")
plot(mplot[,1],main="MSE: third target",axes=F,type="l",xlab="Lag-structure",ylab="filter-weights",ylim=c(min(mplot),max(na.exclude(mplot))),col=colo[1],lwd=1)
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 2:ncol(mplot))
{
#  lines(mplot[,i]-ifelse(i==ncol(mplot),0.3,0),col=colo[i],lwd=1)
  lines(mplot[,i],col=colo[i],lwd=1)
  mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=1:nrow(mplot),labels=-1+1:nrow(mplot))
axis(2)
box()


invisible(dev.off())
@
Signal extraction commonly refers to the determination and the estimation of so called components or signals $\mathbf{z}_t$ of a time series $\mathbf{x}_t$, for example a trend or a cycle or a seasonally adjusted component. Under suitable assumptions, see e.g. McElroy and Wildi (2020), these 'signals' can be obtained by applying filters $(\mathbf{\Gamma})_{|k|<\infty}$ to the data: 
\[\mathbf{z}_t=\sum_{|k|<\infty}\boldsymbol{\Gamma}_k\mathbf{x}_{t-k}
\]
Trends and cycles are typically obtained from lowpass- and bandpass-filters. In some cases,  $(\mathbf{\Gamma})_{|k|<\infty}$ is an a-causal bi-infinite symmetric design i.e. $\mathbf{\Gamma}_{k}=\mathbf{\Gamma}_{-k}$. For an a-causal signal extraction filter, the estimation of the signal $z_t$ at the current end-point $t=T$ of a finite sample can amount to a challenging prediction exercise, since the left tail $\sum_{k<0}\boldsymbol{\Gamma}_k\mathbf{x}_{T-k}$ is not observed, yet. The corresponding estimation problem, commonly referred to as nowcasting, is relevant for assessing the \emph{current} 'state' of a particular phenomenon: for example recession vs. expansion, bear- vs. bull-market, down- vs. up-swing or current seasonally adjusted figure. %Nowcasting can be a challenging estimation problem, in particular when the left-tail $\mathbf{\Gamma}_{-k}$  of the acausal filter decays slowly towards zero for increasing lead $-k$. 
In some cases, the classic MSE-approach can lead to nowcasts subject to unduly many 'false alarms' or noisy zero-crossings and therefore a control of the frequency of zero-crossings by the holding-time constraint is a worthwhile consideration, see e.g. Wildi (2023a). % based on monthly macro-indicators, the industrial production indices of various countries with long and consistent cycle histories.
We here consider a multivariate artificial simulation framework for comparing theoretical and empirical performance measures under the Gaussian assumption but in an otherwise general context involving the extension to autocorrelated processes proposed in section \ref{ext_stat} (an extension to non-Gaussian data is provided in section \ref{non_gaussian}). Specifically, let $\mathbf{z}_t$ and $\mathbf{x}_t$ be defined by 
\begin{eqnarray*}
\mathbf{z}_t%\left(\begin{array}{c}z_{1t}\\z_{2t}\\z_{3t}\end{array}\right)
=\sum_{k=-\infty}^{\infty}\boldsymbol{\Gamma}_k\mathbf{x}_{t-k}\\
\mathbf{x}_t=\mathbf{A}\mathbf{x}_{t-1}+\boldsymbol{\epsilon}_t
\end{eqnarray*}
where $\mathbf{x}_t$ is a VAR(1)-process with $\mathbf{A}=\left(\begin{array}{ccc}\Sexpr{A[1,1]}&\Sexpr{A[1,2]}&\Sexpr{A[1,3]}\\
\Sexpr{A[2,1]}&\Sexpr{A[2,2]}&\Sexpr{A[3,2]}\\
\Sexpr{A[3,1]}&\Sexpr{A[3,2]}&\Sexpr{A[3,3]}\end{array}\right)$, $\boldsymbol{\Sigma}=\left(\begin{array}{ccc}\Sexpr{round(Sigma[1,1],2)}&\Sexpr{round(Sigma[1,2],2)}&\Sexpr{round(Sigma[1,3],2)}\\
\Sexpr{round(Sigma[2,1],2)}&\Sexpr{round(Sigma[2,2],2)}&\Sexpr{round(Sigma[2,3],2)}\\
\Sexpr{round(Sigma[3,1],2)}&\Sexpr{round(Sigma[3,2],2)}&\Sexpr{round(Sigma[3,3],2)}\end{array}\right)$ and 
\begin{eqnarray*}
%a11<-0.7
%a131<-1.5
%a132<--0.8
%a231<-0.5
%a232<--0.9
%a32<-0.9
%a331<-1.2
%a332<--0.6
%# Target z_{1t}: we concatenate three filters of length L each which correspond to epsilon_{1t},epsilon_{2t} and epsilon_{3t}
%gamma_target_1<-c(a11^(1:L),c(rep(1/L_ma1,L_ma1),rep(0,L-L_ma1)),ARMAtoMA(ar=c(a131,a132),lag.max=L))
%# Target z_{2t}
%gamma_target_2<-c(c(rep(1/L_ma2,L_ma2),rep(0,L-L_ma2)),c(rep(1/L_ma3,L_ma3),rep(0,L-L_ma3)),ARMAtoMA(ar=c%(a231,a232),lag.max=L))
%# Target z_{3t}
%gamma_target_3<-c(c(rep(1/L_ma3,L_ma3),rep(0,L-L_ma3)),a32^(1:L),ARMAtoMA(ar=c(a331,a332),lag.max=L))
%# Bind to target for vector z_t
%gamma_target<-rbind(gamma_target_1,gamma_target_2,gamma_target_3)
\boldsymbol{\Gamma}_{|k|}=\left(\begin{array}{ccc}\gamma_{11|k|}=\Sexpr{a11}^{|k|}& \gamma_{12|k|}=\left\{\begin{array}{cc}1/\Sexpr{L_ma1},&|k|\leq \Sexpr{L_ma1}\\0&\textrm{otherwise}\end{array}\right.& \gamma_{13|k|}=\xi_{13|k|}\\
\gamma_{21|k|}=\left\{\begin{array}{cc}1/\Sexpr{L_ma2},&|k|\leq \Sexpr{L_ma2}\\0&\textrm{otherwise}\end{array}\right. & \gamma_{22|k|}=\left\{\begin{array}{cc}1/\Sexpr{L_ma3},&|k|\leq \Sexpr{L_ma3}\\0&\textrm{otherwise}\end{array}\right.& \gamma_{23|k|}=\xi_{23|k|}\\
\gamma_{31|k|}=\left\{\begin{array}{cc}1/\Sexpr{L_ma3},&|k|\leq \Sexpr{L_ma3}\\0&\textrm{otherwise}\end{array}\right. & \gamma_{32|k|}=\Sexpr{a32}^{k} & \gamma_{33|k|}=\xi_{33|k|}\end{array}\right)
\end{eqnarray*}
where $\xi_{13|k|},\xi_{23|k|}$ and $\xi_{33|k|}$ are the coefficients of the MA-inversions of the AR(2)-filters with parameters $(\Sexpr{a131},\Sexpr{a132})$, $(\Sexpr{a231},\Sexpr{a232})$ and  $(\Sexpr{a331},\Sexpr{a332})$ respectively.  In applications, filters can be specified by formal optimization criteria such as in section \ref{sign_het} or they can be derived from time-series models of the data, see e.g. McElroy and Wildi (2020). Either way, we here do not preclude this choice and propose a  mix of heterogenous lowpass designs chiefly emphasizing the flexibility of our approach. Concerning the target we aim at a nowcast of $\mathbf{z}_{t+\delta}$ with $\delta=0$, imposing holding-time constraints $ht_1=\Sexpr{ht_vec[1]}$, $ht_2=\Sexpr{ht_vec[2]}$ and $ht_3=\Sexpr{ht_vec[3]}$ for the SSA-nowcasts of $z_{1t+1},z_{2t+1}$ and $z_{3t+1}$. Furthermore, we derive nowcast-weights $ (\mathbf{b}\cdot\boldsymbol{\xi})_{i\delta}$, as assigned to $\boldsymbol{\epsilon}_{it}$ (convolution), or $\mathbf{b}_i$ as applied to $\mathbf{x}_{it}$ (deconvolution), see section \ref{ext_stat}. Fig.\ref{filt_coef_signal_extraction_heteroclite} displays all filters: SSA as applied to $\mathbf{x}_t$ (top panels) and to $\boldsymbol{\epsilon}_t$ (mid panels) and MSE as applied to $\boldsymbol{\epsilon}_t$ (bottom panels). From left to right the nowcasts target $z_{1t}$, $z_{2t}$ and $z_{3t}$ while the colors distinguish filters as applied to $\epsilon_{1t-k}$ or $x_{1t-k}$ (\Sexpr{colo[1]}), $\epsilon_{2t-k}$ or $x_{2t-k}$ (\Sexpr{colo[2]}) and $\epsilon_{3t-k}$ or $x_{3t-k}$ (\Sexpr{colo[3]}). The characteristic SSA-profiles towards lag $k=0$ in the mid-panels hint at the implicit boundary constraint $(b\cdot\xi)_{ij,-1}=0$ (the constraints for the deconvoluted ${b}_{ij,-1}$ in the top panels are different, due to linear transformation). Empirical sample performances of the filters are summarized in table \ref{perf_signal_extraction_heteroclite_sample}. A comparison with expected (true) numbers in table \ref{perf_signal_extraction_heteroclite_true} confirms pertinence of the approach: cross-correlations and lag-one acfs are based on \ref{gen_stat_x}, sign-accuracies P$\Big(\sign(z_{it+\delta})=\sign(y_{it})\Big)$ rely on \ref{arcsin} and expected holding-times are derived from \ref{ht}, inserting the correspondent lag-one acfs. %A comparison of holding-times suggests that all SSA-designs emphasize smoothness in this example. 
%When the holding-times of SSA and MSE are comparable in size (last row) then the criteria are similar, too (the criterion value of MSE is always one since MSE is the target, recall proposition \ref{sa_crit_mult}). 
The effect of imposing a markedly larger holding-time of SSA, when compared to MSE, consists in pulling filter-weights away from the zero-line  while taking over some of the main-patterns of MSE, see middle column, middle and bottom graphs in fig.\ref{filt_coef_signal_extraction_heteroclite}. A similar holding-time leads to a tight fit of SSA- and MSE-nowcasts (last row in table \ref{perf_signal_extraction_heteroclite_sample}). Note that the SSA-solution is invariant to the choice of any of the criteria in the first three columns of the table: each one represents an equivalent objective in optimization terms.
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "filt_coef_signal_extraction_heteroclite.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{SSA applied to $x_t$ (deconvolution: top graphs), SSA applied to $\\epsilon_t$ (convolution, middle graphs) and MSE- applied to $\\epsilon_t$ (bottom). Nowcasts of the first target series $z_{1t}$ (left), second target $z_{2t}$ (middle) and third target $z_{3t}$ (right). Filter-weights  applied to $x_{1t}$ or $\\epsilon_{1t}$ (blue) $x_{2t}$ or $\\epsilon_{2t}$ (red) and $x_{3t}$ or $\\epsilon_{3t}$ (green)", sep = "")
cat("\\label{filt_coef_signal_extraction_heteroclite}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-perf_mat_sample[,c("Sign accuracy","Cor. with target","Cor. with MSE","ht","ht MSE")]
xtable(round(perf_mat,2), dec = 1,digits=2,
paste("Sample performances of SSA-nowcasts: Sign accuracy $\\hat{P}\\Big(\\sign(z_{it+\\delta})=\\sign(y_{it})\\Big)$, correlation with target $\\hat{\\rho}(y_i,z_i,\\delta)$, correlation with MSE $\\hat{\\rho}(y_i,\\hat{z}_{i\\delta})$ and holding-time (length of time series divided by number of crossings), with $\\delta=0$ (nowcast). Sample holding-time of MSE in last column. All estimates are based on samples of length 1000000  "),
label=paste("perf_signal_extraction_heteroclite_sample",sep=""),
center = "centering", file = "", floating = FALSE)
@
<<label=ats_mba_2,echo=FALSE,results=tex>>=
perf_mat<-perf_mat_true[,c("Sign accuracy","Cor. with target","Cor. with MSE","ht","ht MSE")]
xtable(round(perf_mat,2), dec = 1,digits=2,
paste("Expected (true) performances of SSA-nowcasts: Sign accuracy ${P}\\Big(\\sign(z_{it+\\delta})=\\sign(y_{it})\\Big)=0.5+\\frac{\\arcsin\\left(\\rho(y_i,\\hat{{z}}_{i\\delta},\\delta)\\frac{\\boldsymbol{\\gamma}_{i\\cdot\\delta}'\\tilde{\\mathbf{I}}\\boldsymbol{\\gamma}_{i\\cdot\\delta}}{\\sum_{|k|<\\infty}\\boldsymbol{\\gamma}_{i k}'\\boldsymbol{\\Sigma}\\boldsymbol{\\gamma}_{i k}}\\right)}{\\pi}$, correlation with target ${\\rho}(y_i,z_i,\\delta)$, correlation with MSE ${\\rho}(y_i,\\hat{z}_{i\\delta})$ and holding-time $ht(y_i|\\mathbf{b}_i)=\\frac{\\pi}{\\arccos(\\rho(y_i,y_i,\\delta))}$ with $\\delta=0$ (nowcast). Expected (true) holding-time of MSE in last Column."),
label=paste("perf_signal_extraction_heteroclite_true",sep=""),
center = "centering", file = "", floating = FALSE)
@
<<label=init,echo=FALSE,results=hide>>=
file = "output_signal_extraction_heteroclite.pdf"
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

anf<-10*L
enf<-11*L
y_mat[anf:enf,]
z_mse_mat[anf:enf,]
zdelta_mat[anf:enf,]
colo<-c("cyan","violet","black")
par(mfrow=c(2,2))
for (i in 1:n)
{
mplot<-cbind(y_mat[anf:enf,i],z_mse_mat[anf:enf,i],zdelta_mat[anf:enf,i])

ts.plot(scale(mplot,center=F,scale=T),col=colo,main=paste("Series ",i,sep=""),lwd=2)
abline(h=0)
abline(v=1+which(mplot[2:nrow(mplot),1]*mplot[1:(nrow(mplot)-1),1]<0),col=colo[1],lty=2)
#abline(v=1+which(mplot[2:nrow(mplot),2]*mplot[1:(nrow(mplot)-1),2]<0),col=colo[2],lty=2)
lines(scale(mplot,center=F,scale=T)[1],col=colo[1])
}
invisible(dev.off())
@
A comparison of $y_{it}$ (\Sexpr{colo[1]}), $\hat{z}_{it\delta}$ (\Sexpr{colo[2]}) and $z_{it+\delta}$ (\Sexpr{colo[3]}) in fig.\ref{output_signal_extraction_heteroclite} demonstrates the smoothing effect entailed by the holding-time constraint: %We here argue that the trend could be specified such that the frequency of its zero-crossings is kept under control by a hyper-parameter, the holding-time, which can be selected in view of the research purpose of a particular analysis: Wildi (2023 a) examines turning-points of a trend whose  holding-time matches the historical length of recession episodes. \\
%Figure \ref{output_signal_extraction_heteroclite}  also illustrates that 
stronger smoothing, by SSA, is not necessarily compromised by increased lag, when compared to MSE, and (some of the) noisy crossings of MSE can be avoided by SSA. Wildi (2023 a) and (2023 b) discuss more extensively timeliness-smoothness issues: in particular, it is shown that SSA can outperform MSE in terms of timeliness (relative lead at zero-crossings) as well as in terms of crossing-parsimony (fewer random sign-changes), at once.
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "output_signal_extraction_heteroclite.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{A comparison of filter outputs: $y_{it}$ (cyan), $\\hat{z}_{it\\delta}$ (violet) and $z_{it+\\delta}$ (black). Zero-crossings by $y_{it}$  are marked by vertical lines.", sep = "")
cat("\\label{output_signal_extraction_heteroclite}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@





\subsubsection{Non-Gaussian Data}
\label{non_gaussian}

<<label=init,echo=FALSE,results=hide>>=
#rm(list=ls())
  
len<-100000
df_vec<-c(2,4,6,8)
# Empirical measures and checks of convolution/deconvolutions
if (recompute_calculations)
{  

#----------------------------
# Generate data
# Data must be very long for suitable convergence: takes couple minutes to process
# 1. Noise process with variance Sigma
  for (ih in 1:length(df_vec))#ih<-1
  {
# Use the same seed for each experiment    
    set.seed(16)
    eps1iid<-rt(len, df_vec[ih])
    eps2iid<-rt(len, df_vec[ih])
    eps3iid<-rt(len, df_vec[ih])

    eps_mat<-matrix(ncol=dim(Sigma)[1],nrow=len)
      
  # Generate eps with cross-correlation corresponding to Sigma
    eigen_obj<-eigen(Sigma)
  # Square-root of diagonal
    D<-diag(sqrt(eigen_obj$values))
    U<-eigen_obj$vectors
    Sigma_sqrt<-(U)%*%D%*%t(U)
  # Check_ should vanish
    t(Sigma_sqrt)%*%Sigma_sqrt-Sigma
  # Generate eps_mat
    eps_mat<-t(Sigma_sqrt%*%rbind(eps1iid,eps2iid,eps3iid))
  # Check: empirical cov should match Sigma
    cov(eps_mat)
    Sigma
    
  # Generate x either of two ways: 
  # 1. Wold decomposition
    x<-rep(0,n)
    x_mat<-NULL
    for (m in 1:n)#j<-1
    {
      x<-NULL
      xi_mat<-cbind(xi[m,1:L],xi[m,L+1:L],xi[m,2*L+1:L])
      for (i in L:len)#i<-L
      {
        x<-c(x,sum(apply(xi_mat*eps_mat[i:(i-L+1),],2,sum)))
      }
      x_mat<-cbind(x_mat,c(rep(NA,L-1),x))
    }
    

  # 2. Generate yt, zt and z_mse from eps_mat
    perf_mat<-NULL
    for (m in 1:dim(Sigma)[1])#m<-3
    {
  
  # Generate yt in either of two equivalent ways:        
  # 1. convolution of SSA with white noise solution bk_mat (solution of theorem)   
      bk<-NULL
      for (j in 1:dim(Sigma)[1])#j<-1
        bk<-cbind(bk,bk_mat[((j-1)*L+1):(j*L),m])
      y<-rep(NA,len)
      for (j in L:len)#j<-L
        y[j]<-sum(apply(bk*eps_mat[j:(j-L+1),],2,sum))
  
  # 2. Target: MSE-estimate of zt    
      gammak<-NULL
      for (j in 1:dim(Sigma)[1])#j<-1
        gammak<-cbind(gammak,gammak_mse[m,((j-1)*L+1):(j*L)])
      z_mse<-rep(NA,len)
      for (j in L:len)
        z_mse[j]<-sum(apply(gammak*eps_mat[j:(j-L+1),],2,sum))
      
# 3.3 Target: zt+delta    
      gammak<-NULL
      for (j in 1:dim(Sigma)[1])#j<-1
#  gamma_target is original Signal extraction filter as applied to xt     
        gammak<-cbind(gammak,gamma_target[m,((j-1)*L+1):(j*L)])
      z<-rep(NA,len)
# Truncated bi-infinite sum (acausal filter) 
# gamma_target is applied to xt (in contrast to gammak_mse above which is convolution of mse with wold-decomposition)    
      for (j in L:(len-L))
        z[j]<-sum(apply(gammak*x_mat[j:(j-L+1),],2,sum))+sum(apply(gammak[-1,]*x_mat[(j+1):(j+L-1),],2,sum))
# Shift z by delta    
      if (delta>0)
      {  
        zdelta<-c(z[delta:len],rep(0,delta))
      } else
      {
        if (delta<0)
        {
          zdelta<-c(rep(0,delta),z[1:(len-abs(delta))])
        } else
        {
          zdelta<-z
        }
      }

# Sample sign accuracy            
      sample_SA_crit_SSA_ref_target<-length(which(sign(zdelta)==sign(y)))/(nrow(na.exclude(cbind(y,zdelta))))

  #------------
  # Performances: empirical and theoretical criterion values and lag-one acfs
      perf_mat<-rbind(perf_mat,c(cor(na.exclude(cbind(z_mse[1:len],y[1:len])))[1,2], SSA_obj$crit_rhoyz[m],sample_SA_crit_SSA_ref_target,perf_mat_true[m,"Sign accuracy"],length(na.exclude(y))/length(which(y[(L+1):len]*y[L:(len-1)]<0)), ht_vec[m]))
  # Criterion value is with respect to MSE-target: 
  #   Optimized solutions are identical but criterion measures performances against MSE i.e. against 'benchmark'    
    } 
    colnames(perf_mat)<-c("sample crit.","theor. crit.","sample SA","theor. SA","sample ht","theor. ht")
      
    perf_mat
  #-------------------------  
  # Additional checks
  # 1. Holding-times (lag-one acfs)  
    M_obj<-M_func(L,Sigma)
    
    M_tilde<-M_obj$M_tilde
    I_tilde<-M_obj$I_tilde
  
    rho_mse_1<-gammak_mse[1,]%*%M_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
    rho_ssa_1<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
    rho_mse_2<-gammak_mse[2,]%*%M_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
    rho_ssa_2<-bk_mat[,2]%*%M_tilde%*%bk_mat[,2]/bk_mat[,2]%*%I_tilde%*%bk_mat[,2]
    rho_mse_3<-gammak_mse[3,]%*%M_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
    rho_ssa_3<-bk_mat[,3]%*%M_tilde%*%bk_mat[,3]/bk_mat[,3]%*%I_tilde%*%bk_mat[,3]
  # Check: best approximation on grid should be close to effective holding-time constraints  
    compute_holding_time_from_rho_func(rho_ssa_1)$ht
    compute_holding_time_from_rho_func(rho_ssa_2)$ht
    compute_holding_time_from_rho_func(rho_ssa_3)$ht
    ht_vec
  # Compute ht of MSE
    compute_holding_time_from_rho_func(rho_mse_1)$ht
    compute_holding_time_from_rho_func(rho_mse_2)$ht
    compute_holding_time_from_rho_func(rho_mse_3)$ht
  
  # 2. Criteria: MSE is trivially one since correlation of MSE with itself is one (our target is MSE which leads to the same solution as using z_{t+\delta})
  #   The criteria computed here correspond to the values in perf_mat_SSA above  
    crit_mse_1<-gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]/gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]
    crit_ssa_1<-gammak_mse[1,]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[1,]%*%I_tilde%*%gammak_mse[1,]))
    crit_mse_2<-gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]/gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]
    crit_ssa_2<-gammak_mse[2,]%*%I_tilde%*%bk_mat[,2]/(sqrt(bk_mat[,2]%*%I_tilde%*%bk_mat[,2])*sqrt(gammak_mse[2,]%*%I_tilde%*%gammak_mse[2,]))
    crit_mse_3<-gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]/gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]
    crit_ssa_3<-gammak_mse[3,]%*%I_tilde%*%bk_mat[,3]/(sqrt(bk_mat[,3]%*%I_tilde%*%bk_mat[,3])*sqrt(gammak_mse[3,]%*%I_tilde%*%gammak_mse[3,]))
  
    criterion_mat<-rbind(c(crit_mse_1,crit_mse_2,crit_mse_3),c(crit_ssa_1,crit_ssa_2,crit_ssa_3))
    colnames(criterion_mat)<-c(paste("Series ",1:n,paste=""))
    rownames(criterion_mat)<-c("MSE","SSA")
    criterion_mat
  
    save(file=paste(path.result,paste("bk_signal_extraction_mat_heteroclite_",df_vec[ih],sep=""),sep=""),bk_mat)
    save(file=paste(path.result,paste("gammak_mse_signal_extraction_heteroclite_",df_vec[ih],sep=""),sep=""),gammak_mse)
    save(file=paste(path.result,paste("perf_mat_signal_extraction_heteroclite_",df_vec[ih],sep=""),sep=""),perf_mat)
  }  
} else
{
  bk_mat_all<-gammak_mse_all<-perf_mat_all<-NULL
  for (ih in 1:length(df_vec))
  {  
    load(file=paste(path.result,paste("bk_signal_extraction_mat_heteroclite_",df_vec[ih],sep=""),sep=""))
    load(file=paste(path.result,paste("gammak_mse_signal_extraction_heteroclite_",df_vec[ih],sep=""),sep=""))
    load(file=paste(path.result,paste("perf_mat_signal_extraction_heteroclite_",df_vec[ih],sep=""),sep=""))
# Append all solutions into large matrices    
    bk_mat_all<-cbind(bk_mat_all,bk_mat)
    gammak_mse_all<-cbind(gammak_mse_all,gammak_mse)
    perf_mat_all<-rbind(perf_mat_all,perf_mat)
  }
}
#perf_math: note that we insert the theoretical ht for MSE (which is identical with empirical up to neggligible finite sample issues)
 rownames(perf_mat_all)<-1:nrow(perf_mat_all)
name_s<-paste("Series ",1:3,sep="")
for (i in 1:length(df_vec))
  rownames(perf_mat_all)[(i-1)*length(name_s)+1:length(name_s)]<-paste(name_s," df=",df_vec[i],sep="")

@
We here analyze departures from the Gaussian assumption. Wildi (2023 a) demonstrates resilience of the univariate approach when applied to economic time series (equity index and macro-indicators) and Wildi (2023 b) analyzes robustness of the univariate SSA-approach when applied to t-distributed random variables with degrees of freedom $df\in\{2,4,6,8\}$. We here rely on the latter framework and extend robustness results to the multivariate signal extraction case proposed in the previous section. Table \ref{perf_signal_extraction_heteroclite_t} summarizes empirical and theoretical performance measures: the former rely on t-distributed data of samples of length \Sexpr{as.integer(len)}; the latter theoretical  expressions correspond to table \ref{perf_signal_extraction_heteroclite_true} and wrongly assume Gaussianity. Differences between sample and theoretical numbers are a measure for the effect of heavy tails on the proposed SSA-approach.  
<<label=ats_mba_2,echo=FALSE,results=tex>>=
# Match names with previous two tables
colnames(perf_mat_all)[1]<-"sample cor."
colnames(perf_mat_all)[2]<-"cor. with MSE"
colnames(perf_mat_all)[4]<-"SA"
colnames(perf_mat_all)[6]<-"ht"
xtable(round(perf_mat_all,2), dec = 1,digits=2,
paste("Performances of SSA-nowcasts of the signal extraction problem in the previous section when applied to t-distributed data: sample and theoretical correlations, sample and theoretical sign accuracies as well as sample and theoretical holding-times. Theoretical numbers correspond to the previous section and wrongly assume Gaussianity.  "),
label=paste("perf_signal_extraction_heteroclite_t",sep=""),
center = "centering", file = "", floating = FALSE)
@
The table suggests that sample criterion values, i.e. correlations and sign accuracies, are nearly unaffected so that SSA-solutions are effectively maximizing the relevant (sample) objective functions\footnote{Note that second order moments are not defined for the t-distribution with $df=2$.}. However, the last two columns in the table indicate that sample holding-times can be subject to biases in the case of heavy-tailed distributions ($df=\Sexpr{df_vec[1]}$), but the effect becomes practically negligible for degrees of freedom $df\geq 4$. A possible explanation is provided in Wildi (2023 b) who argues that extreme observations trigger the impulse responses of the filters so that sign-changes of the outputs match crossings of the impulse responses: if the latter deviate from the imposed holding-time then a systematic bias could result   





\section{Conclusion}\label{conclusion}

We propose a multivariate extension of a novel  SSA-criterion which emphasizes sign accuracy and zero-crossings of the predictor subject to a holding-time constraint. Under the Gaussian assumption, the classic MSE-criterion is equivalent to  unconstrained SSA-optimization: in the absence of a holding-time constraint and down to an arbitrary scaling nuisance. We argue that the proposed concept is resilient against various departures of the Gaussian assumption. Moreover, %Resilience against departures of the Gaussian hypothesis, in terms of  stylized facts of financial times series, has been verified by comparing  expected holding times with empirical means based on the S$\&$P500-index as well as BTC (crypto-currency). While heavy tails or autocorrelation can generate biases, the latter problem could be corrected by the proposed extension of our approach to stationary processes.   
%Notwithstanding, 
the approach is interpretable and appealing %beyond the promoted sign-accuracy perspective, in part 
due to its actual simplicity and because the criterion merges relevant facets of the prediction problem.  Finally, the proposed holding-time is an interpretable hyperparameter whose determination leads to an alternative specification for a smooth trend- or cycle-component of a time series.  %The  multivariate extension generalizes previous results  and is applicable to generic forecast and signal extraction problems. %The example also illustrates that timeliness (advance or retard at the zero-line) and smoothing-capability (spread between consecutive crossings) of SSA-designs can be improved both, at once, when compared to established benchmarks. %These somehow intriguing observations hint towards existence of a richer tradeoff, a trilemma, which reconciles particular empirical findings  in a common formal framework. 
%In summary,  the SSA-criterion reconciles MSE, sign accuracy and smoothing requirements in a flexible, consistent and interpretable manner. \\









%
\begin{thebibliography}{99}
%






\bibitem{} Barnett J.T. (1996) Zero-crossing rates of some non-Gaussian processes with application to detection and estimation.  {\it Thesis report Ph.D.96-10, University of Maryland}.

\bibitem{} Brockwell P.J. and Davis R.A. (1993) Time Series: Theories and Methods (second edition).  {\it Springer Verlag}.


\bibitem{} Harvey, A. 1989. Forecasting, structural time series models and the Kalman filter.  {\it Cambridge: Cambridge University Press}.



\bibitem{} Hodrick, R. and Prescott, E. (1997) Postwar U.S. business
cycles: an empirical investigation.  {\it Journal of Money, Credit,
and Banking} {\bf 29}, 1--16.



\bibitem{} Horn, R., Johnson, C. (1991) Topics in Matrix Analysis. Cambridge: Cambridge University Press.

\bibitem{} Kedem, B. (1986) Zero-crossings analysis.  {\it Research report AFOSR-TR-86-0413, Univ. of Maryland.}


\bibitem{} Kratz, M. (2006) Level crossings and other level functionals of stationary Gaussian processes.  {\it Probability surveys} {\bf Vol. 3}, 230-288.


\bibitem{} McElroy, T. (2006) Exact Formulas for the Hodrick-Prescott Filter.  {\it Research report series (Statistics 2006-9). U.S. Census Bureau }.


\bibitem{} McElroy, T. and Wildi , M. (2019) The trilemma between accuracy, timeliness and smoothness in real-time signal extraction.  {\it International Journal of Forecasting  } {\bf 35 (3)}, 1072-1084.



\bibitem{} McElroy, T. and Wildi , M. (2020) The multivariate linear prediction problem: model-based and direct filtering solutions.  {\it Econometrics and Statistics } {\bf 14}, 112-130.


\bibitem{} Morten, O. and Uhlig, H. (2002) On Adjusting the Hodrick-Prescott Filter for the Frequency of Observations. {\it The Review of Economics and Statistics} {\bf 84} (2), 371-376. 


\bibitem{} Sophocles, J. Orfanidis (2007) Optimum signal processing. McGraw-Hill.



\bibitem{} Rice,S.O. (1944) Mathematical analysis of random noise.  {\it I. Bell. Syst. Tech. J } {\bf 23}, 282-332.

\bibitem{} Wildi, M. (2023 a) Business-Cycle Analysis and Zero-Crossings of Time Series: a Generalized Forecast Approach. Submitted for publication.


\bibitem{} Wildi, M. (2023 b) Mean-Square Error, Zero-Crossings, Sign Accuracy and a Holding-Time Constraint: a Generalized Forecast Approach. Submitted for publication.




\end{thebibliography}





\section{Conclusion}\label{conclusion}

We propose a novel  SSA-criterion which emphasizes sign accuracy and zero-crossings of the predictor subject to a holding-time constraint. Under the Gaussian assumption, the classic MSE-criterion is equivalent to  unconstrained SSA-optimization: in the absence of a holding-time constraint and down to an arbitrary scaling nuisance. We argue that the proposed concept is resilient against various departures of the Gaussian assumption, as illustrated in our applications. Moreover, %Resilience against departures of the Gaussian hypothesis, in terms of  stylized facts of financial times series, has been verified by comparing  expected holding times with empirical means based on the S$\&$P500-index as well as BTC (crypto-currency). While heavy tails or autocorrelation can generate biases, the latter problem could be corrected by the proposed extension of our approach to stationary processes.   
%Notwithstanding, 
the proposed approach is interpretable and appealing %beyond the promoted sign-accuracy perspective, in part 
due to its actual simplicity and because the criterion merges relevant concepts of prediction in terms of sign accuracy, MSE, and smoothing requirements. The proposed business-cycle application illustrates that our approach can be applied to an existing benchmark in view of modifing predictors according to particular research priorities. In this context, timeliness and smoothness of real-time designs can be controlled in an effective and interpretable way. %The example also illustrates that timeliness (advance or retard at the zero-line) and smoothing-capability (spread between consecutive crossings) of SSA-designs can be improved both, at once, when compared to established benchmarks. %These somehow intriguing observations hint towards existence of a richer tradeoff, a trilemma, which reconciles particular empirical findings  in a common formal framework. 
%In summary,  the SSA-criterion reconciles MSE, sign accuracy and smoothing requirements in a flexible, consistent and interpretable manner. \\











 



\section{Appendix}


To  quantify leads or lags of filters at zero-crossings we here propose a simple formal test-statistic. Let  $y_{tn}$, $n=1,...,N$ be a set of competing filters and let $ZC_n$ denote the set of zero-crossings $t_{jn}, j=1,...,|ZC_n|$ of each filter $y_{tn}$
\[ZC_n=\left\{t_{jn}|\textrm{sign}(y_{t_j,n})\neq \textrm{sign}(y_{t_j-1,n})\right\}\]
where $|ZC_n|$ means the cardinality of the set. Let $ZC_n^+$ and $ZC_n^-$ designate the sub-sets of up- and downturns of $y_{tn}$, at which $y_{tn}$ crosses the zero-line from below or from above.
Assume that $y_{tN}$ has been selected to be benchmarked against filter $n<N$ according to the following measure
\begin{eqnarray}\label{leadlagstat}
\tau(N,n):=\frac{1}{|ZC_N|}\left(\sum_{j=1}^{|ZC_N^+|} (t_{f(j,N),n}^+-t_{jN}^+)+\sum_{j=1}^{|ZC_N^-|}(t_{f(j,N),n}^--t_{jN}^-)\right)
\end{eqnarray}
where $f(j,N)$ is the index of the zero-crossing of $y_{tn}$ closest to $t_{jN}^+$ or $t_{jN}^-$ in the corresponding subsets $ZC_n^+$ or $ZC_n^-$  i.e. 
$|t_{f(j,N),n}^+-t_{jN}^+|=\min_{i}|t_{in}^+-t_{jN}^+|$ and similarly for the downturns. Note that  sums are taken over  zero-crossings of the reference filter $y_{tN}$, which is always a SSA-design in the empirical sections. We recommend that $y_{tn}$, $n=1,...,N$ should have 'similar' crossings for the comparison to be meaningful (comparing a lowpass to a highpass would lead to difficulties when interpreting results); also, ideally, the reference filter should be smoother, with fewer zero-crossings, as is the case in our examples. In our BCA application, we mainly rely on \Sexpr{colnames(filter_mat)[4]}: we prefer this reference to the proper target, i.e. the HP-symmetric filter, because the latter cannot be used towards the sample-end, thus excluding the latest and important great-lockdown crisis, see fig.\ref{business_cycle_trend_covid}. Also, timeliness performances of all concurrent filters can be assessed against another causal filter, which facilitates direct comparisons. \\
The $\tau$-statistic \ref{leadlagstat} is called \emph{mean shift} of the reference filter $N$ with respect to filter $n$; the former is called \emph{leading} or \emph{lagging}, with respect to the latter, depending on the mean-shift being positive or negative. The statistic could be split into separate downturn and upturn sums in the case of asymmetry. A classic t-test can be used to infer statistical significance of a mean-lead or a -lag at zero-crossings, assuming the summands in \ref{leadlagstat} to be independently distributed. For illustration, fig.\ref{tau_vec} displays the cumulated shifts  at zero-crossings of the filters  in table \ref{perf_zcc_gap_trend_mean} (single long concatenation of all country-specific series).
<<label=init,echo=FALSE,results=hide>>=
file = paste("tau_vec.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(cumsum(tau_vec_long_mid),type="l",axes=F,xlab="Number of crossings",ylab="Cumulated shift",main=paste(" HP-trend vs. ",colnames(filter_mat)[4],sep=""),col=colo_SSA[2])
#mtext(paste("Relative lead/lag of ",colnames(mplot)[1], " over ",colnames(mplot)[i],sep=""),col=colo_SSA[i-1],line=-i)
axis(1,at=1:length(tau_vec_long_mid),labels=1:length(tau_vec_long_mid))
axis(2)
box()
plot(cumsum(tau_vec_long_fast),type="l",axes=F,xlab="Number of crossings",ylab="Cumulated shift",main=paste(colnames(filter_mat)[5]," vs. ",colnames(filter_mat)[4],sep=""),col=colo_SSA[3])
#mtext(paste("Relative lead/lag of ",colnames(mplot)[1], " over ",colnames(mplot)[i],sep=""),col=colo_SSA[i-1],line=-i)
axis(1,at=1:length(tau_vec_long_mid),labels=1:length(tau_vec_long_mid))
axis(2)
box()

invisible(dev.off())
@
<<label=z_box_plot_pure_mba_2.pdf,echo=FALSE,results=tex>>=
file = "tau_vec.pdf"
cat("\\begin{figure}[H]")
cat("\\begin{center}")
cat("\\includegraphics[height=3in, width=6in]{", file, "}\n",sep = "")
cat("\\caption{Cumulated Shift at zero-crossings computed on concatenated series: HP-trend (left panel) and SSA(7.66,18) (right panel) are both referenced against  SSA(12,18). An upward trend signifies a lead of the reference SSA(12,18).", sep = "")
cat("\\label{tau_vec}}", sep = "")
cat("\\end{center}")
cat("\\end{figure}")
@
The mean-shift or $\tau$-statistic corresponds to the slope of these curves and a positive slope signifies a lead of the reference design. The t-statistic tests the alternative (non-vanishing drift) against the null (vanishing drift): the large absolute t-value for the second SSA-filter in  table \ref{perf_zcc_gap_trend_mean} reflects the strong drift in the right panel of figure \ref{tau_vec}.  
Finally, note that the time-shift statistic considers differences of \emph{closest} crossings only, i.e. earlier or later 'noisy' sign-changes of the contender $y_{tn}$ are ignored. Therefore,  $\tau$ tends to be biased against the reference-filter $y_{tN}$ if the latter is smooth and leading, as is mostly the case of SSA-designs in our application. As an example, the $\tau$-statistic of HP-trend referenced against \Sexpr{colnames(filter_mat)[4]} in the top-right panel of fig.\ref{business_cycle_trend_covid} would vanish over the latest 'great-lockdown' crisis, because the closest crossings of HP-trend overlap with SSA. But the figure also indicates the presence of a couple of delayed 'noisy' sign-changes by HP, to the right of the reference SSA-crossings, which are ignored by our statistic. In practice, analysts would typically wait for a confirmation of a sign-change, up to a point where a relapse can be excluded with some confidence. Since these considerations are wholly ignored by $\tau$, the mean-shift can be considered as a conservative measure for the lead of SSA in our examples. 





%
\begin{thebibliography}{99}
%





\bibitem{} Anderson O.D. (1975) Moving Average Processes.  {\it Journal of the Royal Statistical Society. Series D (The Statistician)}. {\bf Vol. 24, No. 4}, 283-297


\bibitem{} Barnett J.T. (1996) Zero-crossing rates of some non-Gaussian processes with application to detection and estimation.  {\it Thesis report Ph.D.96-10, University of Maryland}.

\bibitem{} Brockwell P.J. and Davis R.A. (1993) Time Series: Theories and Methods (second edition).  {\it Springer Verlag}.




\bibitem{} Davies, N., Pate, M. B. and Frost, M. G. (1974). Maximum autocorrelations for moving average processes.  {\it Biometrika } {\bf 61}, 199-200.

\bibitem{} Granger, C.W.J.  (1966). The typical spectral shape of an economic variable.  {\it Econometrica } {\bf 34}, 150-161.



\bibitem{} Harvey, A. 1989. Forecasting, structural time series models and the Kalman filter.  {\it Cambridge: Cambridge University Press}.



\bibitem{} Hodrick, R. and Prescott, E. (1997) Postwar U.S. business
cycles: an empirical investigation.  {\it Journal of Money, Credit,
and Banking} {\bf 29}, 1--16.

\bibitem{} Kedem, B. (1986) Zero-crossings analysis.  {\it Research report AFOSR-TR-86-0413, Univ. of Maryland.}


\bibitem{} Kratz, M. (2006) Level crossings and other level functionals of stationary Gaussian processes.  {\it Probability surveys} {\bf Vol. 3}, 230-288.


\bibitem{} McElroy, T. (2006) Exact Formulas for the Hodrick-Prescott Filter.  {\it Research report series (Statistics 2006-9). U.S. Census Bureau }.



\bibitem{} McElroy, T. and Wildi , M. (2019) The trilemma between accuracy, timeliness and smoothness in real-time signal extraction.  {\it International Journal of Forecasting  } {\bf 35 (3)}, 1072-1084.



\bibitem{} McElroy, T. and Wildi , M. (2020) The multivariate linear prediction problem: model-based and direct filtering solutions.  {\it Econometrics and Statistics } {\bf 14}, 112-130.

\bibitem{} Morten, O. and Uhlig, H. (2002) On Adjusting the Hodrick-Prescott Filter for the Frequency of Observations. {\it The Review of Economics and Statistics} {\bf 84} (2), 371-376. 

\bibitem{} Osterrieder, J. (2017) The Statistics of Bitcoin and Cryptocurrencies.  {\it Advances in Economics, Business and Management Research (AEBMR)} {\bf Vol. 26}.



\bibitem{} Rice,S.O. (1944) Mathematical analysis of random noise.  {\it I. Bell. Syst. Tech. J } {\bf 23}, 282-332.





\end{thebibliography}



\end{document}








\end{document}


