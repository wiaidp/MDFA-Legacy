
\chapter{Multivariate Direct Filter Analysis for Non-stationary Processes}
\label{chap:int}

 We now extend the basic MDFA of Chapter \ref{chap:basic}  by considering
 the method's application to  non-stationary processes.  
 Section \ref{sec:constraint} introduces the idea of filter constraints
arising from time-varying means, a form of non-stationarity.
 This treatment is generalized in Section \ref{sec:non-stat}
  by the definition of non-stationary processes, and theory for the corresponding
   model-based filters is developed.  Finally, the MDFA criterion for
    non-stationary processes is discussed in Section \ref{sec:mdfa-nonstat}.
 

\section{Constrained MDFA}
\label{sec:constraint}

 Various constraints upon the concurrent filter can be envisioned, 
   and imposing such strictures results in  a constrained MDFA. 
   A chief case of interest arises when the 
    data process has a time-varying mean (which is a form of  non-stationarity);
  then it is necessary to impose additional filter constraints -- otherwise
   the filter error will not have mean zero.    To see why, 
   Write $\Lambda (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter,
   so that we see  from (\ref{eq:dfa-error})  
   that $\EE [ E_t ] = \Lambda (L) \, \EE [ X_t ]$; 
   by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  
  If $\EE [ X_t] = 0$ then this condition is always satisfied, but
   for most time series of interest the mean will be nonzero, and is typically
    time-varying.  For such cases additional constraints on $\Lambda (L)$ must be imposed,
    which implicitly amount to constraints on $\widehat{\Psi} (L)$.
    
\begin{Example}    {\bf Constant Mean.}  \rm
\label{exam:constant.mean}
  If $\EE [ X_t ] = \mu$, some nonzero constant,  then we require $\Lambda (1) = 0$.
  This is because the mean of the filter error is
  \[
   \Lambda (L) \, \EE [ X_t] = \Lambda(L) \, \mu = \sum_j \lambda (j) \, \mu =
   \Lambda (1) \, \mu,
  \]
  and this is zero only if $\Lambda (1) = 0$.  This is called a Level Constraint (LC).
\end{Example}  

\begin{Example}    {\bf Linear Mean.}  \rm
\label{exam:linear.mean}
  Suppose that $\EE [ X_t ] = \mu \, t$, where $\mu$ is a nonzero slope
 of a linear time trend.  Then it is required that  $\partial {\Lambda} (1) = 0$
  in addition to the LC,  which is seen as follows:
  \[
   \Lambda (L) \, \EE [ X_t] = \Lambda(L) \, \mu \, t =   \mu \, \sum_j \lambda (j) \, (t-j)
   = \mu \, \left(t \, \sum_j \lambda (j) - \sum_j j \, \lambda (j) \right)
    = \mu \, t \, \Lambda(1) - \mu \, \partial \Lambda (1).
  \]
  This mean of the filter error  is zero only if both $\Lambda(1)=0$ and
  $\partial \Lambda (1)=0$; the latter condition is called the
   Time-Shift Constraint (TSC).  
\end{Example}  

     Hence, for linear means we obtain
 three fundamental types of constraints: LC, TSC, and Level plus 
 Time-Shift Constraint (LTSC), which combines both LC and TSC.
  Using the fact that $\Lambda (L) = \Psi (L) - \widehat{\Psi} (L)$,
   these three constaints can be described as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Lambda (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Lambda} (1) = 0 \quad \mbox{or} \quad 
 \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Lambda (1) = 0,  \,  \partial {\Lambda} (1) = 0 \quad 
 \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) =
 \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.   More generally, we consider  linear constraints  formulated via
\begin{equation}
\label{eq:concurrent-constrain}
 P = R \, \Phi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\Phi$ is $n r \times n$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times n$ dimensional.


\begin{Illustration}  {\bf Level Constraint (LC).}   \rm
\label{ill:lc}
 Note that $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \Phi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} {\Psi (1)}^{\prime}
	\\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Time Shift Constraint (TSC).}   \rm
\label{ill:tsc}
   The constraint is $\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \,
 \widehat{\psi} (j) $.
 Hence  $ \Phi^{\prime}  = [ \widehat{\psi} (0), \widehat{\psi} (2),
 \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ {\partial {\Psi} (1)}^{\prime}
	\\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Level and Time Shift Constraint (LTSC).}  \rm
\label{ill:ltsc}
   Take the Time Shift Constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  
 -  \sum_{j=2}^{q-1} j  \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \Phi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] 
		\otimes 1_n \qquad
	Q = \left[ \begin{array}{c} {\Psi (1) - \partial {\Psi} (1)}^{\prime}  \\ 
	\partial {\Psi (1)}^{\prime} \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}

 
  More generally, we can envision an LPP involving $m$ linear constraints
 on each scalar filter in $\mathcal{G}$ corresponding to (\ref{eq:conc.filter}),
  taking the form  $   K = [ J \otimes 1_n ] \, P$, where $J$ is $m \times q$ dimensional
 ($m < q$), $K$ is $n m \times n$ dimensional, and $P$ is given by 
  (\ref{eq:conc.filter}).
 (The LC, TSC, and LTSC examples all have this form.) 
 In order to express this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition 
 (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $m \times m$ dimensional), 
 a rectangular upper triangular matrix $G$
 (which is $m \times q$ dimensional), and a permutation matrix 
 $\Pi$ (which is $q \times q$ dimensional).  
 Standard matrix software such as $\textsc{R}$ will provide the Q-R decomposition $J$,
 and should produce the rank of $J$ as  a by-product --
 if this is less than $m$, then there are redundancies in the 
 constraints that should first be eliminated. 
 

 Hence  proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ 
 such that $G_1$ has $m$ columns and $G_2$
 has $q-m$ columns.  This quantity $q-m$ corresponds to the number 
 of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, 
 and moreover it is invertible.   Therefore
 \[
  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, K  = 
  \left( \left[ 1_m , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes 1_n  \right) \, P,
\]
 and the action of $\Pi$ (together with the tensor product) amounts
 to a block-wise permutation of the elements of $P$.
  Let the output of this permutation be denoted
\[
   { \left[ { P^{\sharp} }^{\prime},  {  P^{\flat} }^{\prime}  \right] }^{\prime} =
 \left( \Pi \otimes I_N \right) \, P,
\]
 where $ {P}^{\sharp}$ is $n m \times n$ dimensional and 
 $ P^{\flat}$ is $n r \times n$ dimensional.  
 Then  by substitution we can solve for $P^{\sharp}$ in terms of $P^{\flat}$:
\[
  P^{\sharp} =  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, 
  K - \left[  G_1^{-1} \, G_2  \otimes 1_n   \right] \, P^{\flat}.
\]
 Therefore we recognize the free variables $\Phi = P^{\flat}$,
 and obtain $R$ and $Q$ in (\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ 1_{r} \end{array} \right] \otimes 1_n  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes 1_n  \right) \, K.
\end{align*}
   
 \begin{Exercise} {\bf QR Decomposition.} \rm
 \label{exer:qr.constraint}
  Consider an arbitrary set of constraints $J$ on $P$, such that
    $   K = [ J \otimes 1_n ] \, P$ for a given matrix $K$.  
    Encode the procedure that obtains $R$ and $Q$, and apply this
    to the cases of the LC, TSC, and LTSC scenarios with $n=1$, verifying the results
    given in Illustrations \ref{ill:lc}, \ref{ill:tsc}, and \ref{ill:ltsc}.
    Use one-step ahead forecasting as the target filter.
 \end{Exercise}
 
<<exercise_qr-constraint,echo=True>>=
  N <- 1
  q <- 10

  ## level constraint case
	constraint.mat <- matrix(rep(1,q),nrow=1)
	constraint.vec <- t(diag(N))
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
	
  ## time shift constraint case
	constraint.mat <- matrix(seq(0,q-1),nrow=1)
	constraint.vec <- -t(diag(N))
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
  
	## level and time shift constraint case
	constraint.mat <- rbind(rep(1,q),seq(0,q-1))
	constraint.vec <- rbind(t(diag(N)),-t(diag(N)))
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
@

 The pivoting in the Q-R decomposition  can change the form of $R$, because 
 the free variables that determine $\Phi$ depend on the these pivots;
 that is why the matrix $R$ for the time shift constraint looks different
 in Exercise  \ref{exer:qr.constraint}.
  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain}) 
   from given constraints, and
 an analytical solution to the resulting MDFA criterion 
 be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by the determinant of
 (\ref{eq:mdfa-criterion2}),  with respect to  $\mathcal{G}$  -- consisting 
 of all length $q$ concurrent filters 
 subject to  linear constraints of the form (\ref{eq:concurrent-constrain}) -- is
\begin{equation}
\label{eq:phi.soln-constained}
 \Phi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \, 
 \left( A - B \, Q \right).
\end{equation}
  Letting $H = 1_{nq} - R \,   { \left[ R^{\prime} \, B \, R \right] }^{-1} \,
  R^{\prime} \, B$, the minimal value is the determinant of
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (e^{-i \omega}) \, G (\omega) \,
{ \Psi (e^{i \omega}) }^{\prime} \rangle }_0 -
 A^{\prime} \, R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,  A
	+ Q^{\prime} \, B \, H \, Q -   A^{\prime} \, H \, Q - Q^{\prime} \, H^{\prime} \, A.
\end{equation}
\end{Proposition}

 
  
\paragraph{Proof of Proposition \ref{prop:mdfa.quadsoln-constrain}.}
 Substituting (\ref{eq:concurrent-constrain}) in (\ref{eq:mdfa-crit.linear}) yields
\begin{align*}
  D_{\Psi} (\vartheta, G) &  = \Phi^{\prime} \,  
  \left[ R^{\prime} \, B \, R \right] \,  \Phi 
  + \left[ Q^{\prime} \, B \, R - A^{\prime} \, R \right] \, \Phi + \Phi^{\prime} \,
   \left[ R^{\prime} \, B \, Q - R^{\prime} \, A \right]  \\
 & + Q^{\prime} \, B \, Q  - Q^{\prime} \, A - A^{\prime} \, Q  + 
{ \langle \Psi (e^{-i \omega}) \, G (\omega) \, { \Psi (e^{i \omega}) }^{\prime} \rangle }_0.
\end{align*}
  Now by applying the method of proof in Proposition \ref{prop:mdfa.quadsoln}, we obtain 
  the formula (\ref{eq:phi.soln-constained}) for $\Phi$.  Plugging back into
  $D_{\Psi} (\vartheta, G)$ yields the minimal value 
  (\ref{eq:opt.val.mdfa-constrained}).  $\quad \Box$

\vspace{.5cm}

For computation, we utilize the same approximations to $B$ and $b$ as discussed 
in  Chapter \ref{chap:basic},
 obtaining the constrained MDFA filter $\vartheta$ via (\ref{eq:phi.soln-constained})
 followed by (\ref{eq:concurrent-constrain}).

\begin{Exercise} {\bf  Constrained MDFA for VAR(1) with Linear Trend.} \rm
\label{exer:var1trend-mdfa}
This exercise applies the constrained MDFA in the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a VAR(1) process that exhibits a linear trend.
 Simulate a sample of size $T=5000$ of a
   bivariate VAR(1) process with linear trend  given by
   \begin{equation}
   \label{eq:wntrend-lin.trend}
    \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + t \, 
    \left[ \begin{array}{c} -.002 \\ .001 \end{array} \right],
   \end{equation}
    such that the demeaned process satisfies
\[
  X_t =  \left[ \begin{array}{cc}  1  & 1/2 \\    -1/5  &  3/10
    \end{array} \right] \, X_{t-1} + \epsilon_t,
\]
 with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.   Apply the   ideal low-pass filter
  (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  with LC, TSC, and LTSC constraints
  (Proposition \ref{prop:mdfa.quadsoln-constrain}),
  as well as unconstrained MDFA  (Proposition \ref{prop:mdfa.quadsoln}), to find the best
 concurrent filter, setting $q= 30$. 
  (Hint: compute the periodogram from OLS residuals obtained by regressing the simulation
   on a constant plus time.)
 Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}


<<exercise_var1trend-mdfa,echo=True>>=
# Simulate a VAR(1) of sample size 5000:
set.seed(1234)
T <- 5000
N <- 2
levels <- c(1,2)
slopes <- c(-2,1)/1000
phi.matrix <- rbind(c(1,.5),c(-.2,.3))
innovar.matrix <- diag(N)
true.psi <- var.par2pre(array(phi.matrix,c(2,2,1)))
gamma <- VARMAauto(array(phi.matrix,c(2,2,1)),NULL,innovar.matrix,10)
gamma.0 <- gamma[,,1]
x.init <- t(chol(gamma.0)) %*% rnorm(N)
x.next <- x.init
x.sim <- NULL
for(t in 1:T)
{
 	x.next <- phi.matrix %*% x.next + t(chol(innovar.matrix)) %*% rnorm(N)
 	x.sim <- cbind(x.sim,x.next)
}
x.sim <- ts(t(x.sim))
time.trend <- seq(1,T)
x.sim <- t(levels) %x% rep(1,T) + t(slopes) %x% seq(1,T) + x.sim
sim.ols <- lm(x.sim ~ time.trend)
x.resid <- sim.ols$residuals

# construct and apply low pass filter
mu <- pi/6
len <- 1000
lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
lp.filter <- c(rev(lp.filter),lp.filter[-1])
x.trend.ideal <- mvar.filter(x.sim,array(t(lp.filter) %x% diag(N),c(N,N,(2*len+1))))

# get MDFA concurrent filter
q <- 30
grid <- T
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))

# frf for ideal low-pass
frf.psi <- rep(0,grid)
frf.psi[abs(freq.ft) <= mu] <- 1
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,grid))
spec.hat <- mdfa.pergram(x.resid,1)	
lp.mdfa.uc <- mdfa.unconstrained(frf.psi,spec.hat,q)
lp.mdfa.lc <- mdfa.levelconstraint(frf.psi,spec.hat,q)
lp.mdfa.tsc <- mdfa.tsconstraint(frf.psi,spec.hat,q)
lp.mdfa.ltsc <- mdfa.ltsconstraint(frf.psi,spec.hat,q)

# case 1: apply the unconstrained MDFA concurrent filter 
x.trend.mdfa <- mvar.filter(x.sim,lp.mdfa.uc[[1]])[(len-q+2):(T-q+1-len),]

# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.uc[[2]])

# case 2: apply the lc MDFA concurrent filter 
x.trend.mdfa <- mvar.filter(x.sim,lp.mdfa.lc[[1]])[(len-q+2):(T-q+1-len),]
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.lc[[2]])

# case 3: apply the tsc MDFA concurrent filter 
x.trend.mdfa <- mvar.filter(x.sim,lp.mdfa.tsc[[1]])[(len-q+2):(T-q+1-len),]
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.tsc[[2]])

# case 4: apply the ltsc MDFA concurrent filter 
x.trend.mdfa <- mvar.filter(x.sim,lp.mdfa.ltsc[[1]])[(len-q+2):(T-q+1-len),]
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.ltsc[[2]])
@
  
In Exercise \ref{exer:var1trend-mdfa} the in-sample empirical MSE
 does not match the criterion value (\ref{eq:opt.val.mdfa}) in the
  unconstrained and TSC cases of the MDFA, because of the presence
  of the deterministic linear trend.  Also, the MSE of the 
 unconstrained MDFA can be higher than that of the LC MDFA, and this is
  because $q < \infty$.  As $q$ is taken larger, the unconstrained MDFA 
  incorporates filters that satisfy level and time shift constraints,
  and so the in-sample empirical MSE will better approximate the
  theoretical MSE (\ref{eq:opt.val.mdfa-constrained}) as $q \tends \infty$.  
  On the other hand,
  the LC MDFA can have much lower in-sample empirical MSE,  and will be
  a better approximation of the theoretical MSE -- which, however, is 
  higher than that of the unconstrained case.   This is an instance of
  the phenomenon of regularization, whereby improved performance is obtained --
  equivalent to taking a much larger space of filters -- by imposing a 
  constraint on the filter coefficients.
  
  



\section{Background on Non-stationary Vector Time Series }
\label{sec:non-stat}

We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.  
 This type of non-stationary process substantially broadens
  the possible types of applications over the stationary processes
   considered in Chapters \ref{chap:lpp} and \ref{chap:basic}.
  Also, as such processes typically can have a time-varying mean,
  they also necessitate the use of filter constraints such as those
   considered in Section \ref{sec:constraint}.
  
 We suppose that there exists a degree $d$ scalar polynomial $\Delta (L)$
  that reduces each component series of $\{ X_t \}$ to a stationary
   time series (which is allowed to have a non-zero constant mean $\mu$),
   and suppose this is the minimal degree polynomial that accomplishes
    this reduction.  For convenience, and without loss of generality,
    we suppose that $\delta (0) = 1$, or $\Delta (0) = 1$.
    We write $\partial X_t = \Delta (L) \, X_t$ for
  the stationary, differenced time series,
   where $\partial X_t = \mu + Z_t$ and $\{ Z_t \}$ has a spectral
    representation (\ref{eq:specRep}).  Then it is possible
   to give time-domain and frequency-domain representations of
    the original process $\{ X_t \}$ in terms of the stationary
  ``increments" $\{ \partial X_t \}$, together with deterministic functions
  of time that depend on ``initial values" of the process.
   These deterministic functions can be obtained from 
   the theory of Ordinary Difference Equations (ODE): 
  all solutions to   $\Delta (L) X_t = \partial X_t$
   must include a homogeneous solution, i.e., solutions to
    $\Delta (L) X_t = 0$,   which include all functions of 
    $t$ that are annihilated by $\Delta (L)$.  Below we develop
  a general method of solution, but We first provide a few
   illustrations through specific cases.
 
 \begin{Example} {\bf Representation for an $I(1)$ Process.}  \rm
 \label{exam:I1-rep}
    Letting $\Delta (L)= 1-L$, we obtain a once-integrated process,
  denoted as $I(1)$ for short.    
  Because  the constant function (which up to proportionality, is the function $1$)
  is annihilated by $1-L$, we expect the solution to take the form
   of a constant plus some function of the increments $\partial X_t$.
   Proceeding recursively, we obtain
\[
  X_t =    X_{t-1} + \partial X_t = X_{t-2} + \partial X_t + \partial X_{t-1} = \ldots
\]
    Let us suppose an initial time of $t=0$ for this process, so that the
      solution is expressed as 
\[
 X_t = X_0 + \sum_{j=1}^t \partial X_j
\]
    for $t \geq 1$ (and can be extended to $t=0$ by taking the sum to be empty in that case).
  Note that this involves a constant function of time,
  the term $X_0$.  Moreover, applying $X_j = \mu + Z_j$ and the spectral representation,
  we obtain
\begin{equation*}
 X_t = X_0 + t \, \mu +   \int_{-\pi}^{\pi} \sum_{j=1}^t e^{i \omega j}
   \, \mathcal{Z} (d\omega).  
\end{equation*}
  The summation inside the integral can be re-expressed when $\omega \neq 0$ as
  $(1 - e^{i \omega (t+1)})/(1-e^{i \omega})$.
\end{Example}   


\begin{Example} {\bf Representation for an $I(2)$ Process.} \rm
\label{exam:i2-rep}
  Now we set  $\Delta (L)= {(1-L)}^2$  for a twice-integrated process,
  denoted as $I(2)$ for short.    So first differences of $\{ X_t \}$ 
  have a representation as an $I(1)$ process, and the expression for
   $X_t$ will involve a linear function of time $t$, because $t$ is
  annihilated by $\Delta (L)$.  Applying the recursive technique of 
  Example \ref{exam:I1-rep} twice, we obtain
 \[
 X_t = (t+1) \, X_0 - t \, X_{-1}  + \sum_{j=1}^t (t+1-j) \, \partial X_j,
\]
 which holds for $t \geq 1$  (but can be extended to $t=0,-1$ by setting
  the summation to zero).  The linear function of time has slope
  $X_0 - X_{-1}$ and intercept $X_0$.
 Applying $X_j = \mu + Z_j$ and the spectral representation,
  we obtain
\begin{equation*}
 X_t =(t+1) \, X_0 - t \, X_{-1}  + \binom{t+1}{2} \, \mu
 +  \int_{-\pi}^{\pi} \sum_{j=1}^t (t+1-j) \, e^{i \omega j}
   \, \mathcal{Z} (d\omega).  
\end{equation*}
 It can be verified that $(1-L) X_t$ is an $I(1)$ process with level
 $X_0 - X_{-1}$.
\end{Example}

A general technique for obtaining the representation for non-stationary 
processes involves the     inverse of the polynomial of $\Delta(z)$, which is
 denoted by $\Xi (z)$:
\begin{equation}
\label{eq:xi-def}
 \Xi (L) = {\Delta (L) }^{-1} = \sum_{j \geq 0 }  \xi (j) \, L^j.
\end{equation}
 This $\xi (z)$ is a power series that converges on a disk inside the unit circle.
  Because $\Delta (z) \, \Xi (z) = 1$, one can recursively solve for the
   coefficients $\xi (j)$ in terms of past coefficients, using the $\delta (k)$.
  In particular, the $j$th coefficient of $\Delta (z) \, \Xi (z)$ is given by
  the convolution formula:
\begin{equation}
\label{eq:delta.xi-conv}
  \sum_{k \geq 0} \delta (k) \, \xi (j-k) = 1_{ \{ j=0 \} }.
\end{equation}
  In this formula, the equality is due to the fact that the $j$th coefficient of
   the constant function $1$ (viewed as a power series in $z$) is zero unless $j=0$,
   in which case it equals one.  On the left hand side of (\ref{eq:delta.xi-conv})
   the sum runs from $0$ to  $j \wedge d$.  From the assumption that $\delta (0) =1$,
 it is immediate that $\xi (0) = 1$ and 
  \[
  \xi (j) = - \sum_{k \geq 1} \delta (k) \, \xi (j-k)
  \]
for $j > 0$.  Next, for any $ 0 \leq h \leq d-1$ we define
\begin{equation}
\label{eq:init.cond-fcns}
  A_{t} (h) = \sum_{k=0}^h \delta (k) \, \xi (h+t-k).
\end{equation}
 Note that for $h \geq d$ the formula (\ref{eq:init.cond-fcns}) equals 
 (\ref{eq:delta.xi-conv}), and hence equals zero unless $h+t = 0$; but because
$h < d$ in the definition of $A_t (h)$, the function is non-zero.
  Also, when $ t \leq 0$ we have $\xi (h+t-k) = 0$ for $ k > h$, so that
\[
 A_t (h) = \sum_{k=0}^d \delta (k) \, \xi (h+t-k)
 = 1_{ \{ h+t = 0 \} } = 1_{ \{ t = -h \}}.
\]
 Using these definitions, we can state the following result.
 
 \begin{Theorem}
 \label{thm:nonstat-rep}
 The solution for $t \geq 1-d$ to $\Delta (L) X_t = \partial X_t$ is given by
\begin{equation}
 \label{eq:nonstatCausalRep}
 X_t  = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi (j) \, \partial X_{t-j},
\end{equation}
 where the coefficients $\xi (j)$ are defined recursively through (\ref{eq:delta.xi-conv}),
  and the time-varying functions $A_t (h)$ are defined via (\ref{eq:init.cond-fcns}).
  Moreover, the algebraic identity 
\begin{equation}
 \label{eq:Identity1}
  1 - \sum_{h=0}^{d-1} A_{t} (h) \, z^{t+h} = \sum_{k=0}^{t-1} \xi (k) \, z^k
  \, \Delta (z)
\end{equation}
holds, and hence the   spectral representation for
 $\{ X_t\}$ is
\begin{equation}
 \label{eq:nonstatRep-spec}
  X_t = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h}  +
   \sum_{k=0}^{t-1} \xi (k) \, \mu +
  \int_{-\pi}^{\pi}
   \frac{ e^{i \omega t} - \sum_{h=0}^{d-1} A_{t} (h) \,  e^{-i \omega h } 
    }{ \Delta (e^{-i \omega}) } \, \mathcal{Z}  (d\omega).
\end{equation}
\end{Theorem}  
  
\paragraph{Proof of Theorem  \ref{thm:nonstat-rep}.}  
  We begin by proving (\ref{eq:Identity1}), from which the other results follow.
 First write
 \[
  \Xi (z) = {[ \Xi (z) ]}_0^{t-1} + {[ \Xi (z)]}_t^{\infty}
 \]
  and multiply by $\Delta (z)$, yielding
\[
  {[ \Xi (z) ]}_0^{t-1} \, \Delta (z) = 1 -  {[ \Xi (z)]}_t^{\infty}  \, \Delta (z)
\]
via application of (\ref{eq:xi-def}).  Next, 
\begin{align*}
  {[ \Xi (z)]}_t^{\infty}  \, \Delta (z) & = 
     \sum_{\ell \geq t} \xi (\ell) \, z^{\ell}
     \, \sum_{k=0}^d \delta (k) \, z^k \\
  & =   \sum_{k, \ell \geq 0} \delta (k) \, \xi (\ell+t) \, z^{k+\ell+t}  \\
  & = z^t \, \sum_{h \geq 0}  \left( \sum_{\ell \geq 0} 
  \delta (h-\ell) \, \xi (\ell+t) \right) \, z^h \\
    & = z^t \, \sum_{h \geq 0}  \left( \sum_{k=0}^h 
  \delta (k) \, \xi (h+t-k) \right) \, z^h.
\end{align*}
 We see that the coefficient of $z^h$ is either $1_{ \{ h+t =0 \} }$ for $h \geq d$
  or equals $A_t (h)$ for $0 \leq h \leq d-1$.  Hence for $t \geq 1-d$ the calculation
  simplifies to
\[
 {[ \Xi (z)]}_t^{\infty}  \, \Delta (z) =  z^t \sum_{h=0}^{d-1} A_t (h) \, z^h,
\]
 from which  (\ref{eq:Identity1}) follows. 
 Next, multiply both sides of (\ref{eq:Identity1}) by $\Delta (z)$, replace $z$ by $L$,
  and apply the resulting power series to $\{ X_t \}$.  Using
  $\Delta (L) X_t = \partial X_t$, this yields
  \[
   \sum_{j=0}^{t-1} \xi (j) \, \partial X_{t-j} = 
    X_t - \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h},
\]
 from which (\ref{eq:nonstatCausalRep}) follows.  The spectral representation is
  obtained from (\ref{eq:nonstatCausalRep}) as follows:
\begin{align*}
 X_t  & = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi (j) \,  \left( \mu + 
  \int_{-\pi}^{\pi} e^{i \omega (t-j)}  \, \mathcal{Z} (d\omega) \right) \\
  & = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi (j) \, \mu  +
    \int_{-\pi}^{\pi} e^{i \omega t } \, \sum_{j=0}^{t-1} \xi (j) \,e^{-i \omega j}
    \, \mathcal{Z}  (d\omega),
\end{align*}
  from which (\ref{eq:nonstatRep-spec}) follows.  $\quad \Box$
 
\vspace{.5cm}
  
  Theorem  \ref{thm:nonstat-rep} shows how a non-stationary process
  can be represented in  terms of a predictable
  portion -- determined by the functions $A_{t} (h)$ and the
  variables $X_{1-d}, \ldots, X_{-1}, X_0$ -- and a
  non-predictable portion involving a time-varying filter of the $\{
  \partial X_t \}$ series. 
  The time-varying function $\sum_{k=0}^{t-1} \xi (k)$ can be computed
  by evaluating (\ref{eq:Identity1}) at $z=1$ and dividing by $\Delta (1)$
   so long as this is non-zero.  Otherwise, if $\Delta (1) = 0$ we can
   use L'Hopital's rule to obtain
  \[
   \sum_{k=0}^{t-1} \xi (k) = \sum_{k=0}^{t-1} \xi (k) \, z^k \vert_{z = 1}
   = \frac{ - \sum_{h=0}^{d-1} A_t (h) \, (t+h) }{ \dot{\Delta} (1)}.
  \]
     Each of the time-varying functions $A_t (h)$ is annihilated
 by $\Delta (L)$, i.e., $\Delta (L) A_{t} (h) = 0$ for $0 \leq
 h \leq d-1$.   As a consequence, we can rewrite each $A_{t} (h)$ as a linear
 combination of the basis functions of the null space of $\Delta
 (L)$, which are given by $\zeta^{-t}$ for non-repeated roots $\zeta$
  of $\Delta (z)$ (when the roots are repeated, we instead consider
  functions $t \, \zeta{-t}$, etc.).  
    Let the basis  functions be denoted
    $\phi_t (k) $ for $1 \leq k \leq d$; see Brockwell and Davis (1991)
     for additional details about difference equations.  Then we can
 write $A_{t} (h) = \sum_{k=1}^d \alpha_{hk} \phi_t (k)$ for each $0 \leq
 h \leq d-1$, for some coefficients $\alpha_{hk}$.  It follows that
\[
 \sum_{h=0}^{d-1} A_{t}(h) \, z^h = \sum_{k=1}^d \left(
 \sum_{h=0}^{d-1} \alpha_{hk} z^h \right)  \, \phi_t (k).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $z$, and will henceforth be denoted as $p^{(k)}
 (z)$.   Substituting the new formulation, we obtain
\begin{equation}
\label{eq:nonstat.rep-basis}
 X_t = \sum_{k=1}^d \phi_t (k) p^{(k)} (L) \, X_{0} + 
  \sum_{k=0}^{t-1} \xi (k) \, \mu  +  \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{k=1}^d \phi_t (k) \, 
 p^{(k)} ( e^{-i \omega  } )}{ \Delta (e^{-i \omega}) } \,  \mathcal{Z}  (d\omega),
\end{equation}
 where $p^{(k)} (L)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $L$.  
  This representation allows us to 
  understand the action of a filter on a non-stationary time series,
  as the following result demonstrates.
  
\begin{Proposition}
  \label{prop:filter-nonstat}
  The application of a filter $\Psi (L)$ to a non-stationary process $\{ X_t \}$
  with representation  (\ref{eq:nonstat.rep-basis}) has spectral representation
 \begin{align*}
 \Psi (B) X_t & =   \sum_{k=1}^d \Psi (L) \phi_t (k) \, p^{(k)} (L) \, X_{0}  +
   \sum_{k \geq 1}  \Psi (L)  \xi (t-k) \, \mu \\
   & +   \int_{-\pi}^{\pi}
   \frac{ e^{i \omega t} \, \Psi (e^{-i \omega}) 
   - \sum_{k=1}^d \Psi (L) \phi_t (k) \, p^{(k)} (e^{-i \omega}) 
    }{ \Delta (e^{-i \omega}) } \, \mathcal{Z}  (d\omega).
 \end{align*}
 \end{Proposition}
  
  
Depending on the action of $\Psi (L) $ on each $\phi_t (k)$, the order of
 integration for the output process can be less than that of $\{ X_t \}$.
 For instance, if $\Psi (\zeta) = 0$ for some root $\zeta$ of $\Delta (z)$,
  then at least one of the basis functions $\phi_t (k)$ is annihilated by
   $\Psi (L)$, which means we can rewrite the representation in 
   Proposition \ref{prop:filter-nonstat} in terms of $d-1$ instead of $d$ basis
   functions.  We now consider a class of filters known as model-based (MB) filters,
  since they arise as MSE optimal linear filters for a certain class of linear
  signal extraction problems.
  
  Suppose that the process $\{ X_t \}$ is viewed as the sum of two latent processes
  $\{ S_t \}$ and $\{ N_t \}$, labelled as signal and noise respectively.
   This nomenclature indicates merely that the signal is a process one desires
  to estimate, or extract, whereas the noise is to be expunged.  There is no necessity
  that trend non-stationarity must pertain to the signal -- in fact, for the problem
  of business cycle analysis, where one wishes to extract a stationary business cycle
  component, the noise will include trend as well as seasonal effects.
  In general, because
\begin{equation}
\label{eq:signal-noise-decomp}
 X_t = S_t + N_t
\end{equation}
 and $\Delta (L) X_t$ is stationary, it is necessary that $\Delta (L) S_t$ and 
  $\Delta (L) N_t$ are stationary, although these not need be the minimal degree
  differencing operators.  We assume there is a relatively prime factorization of
  $\Delta (z)$ into components $\Delta^S (z)$ and $\Delta^N (z)$, which are polynomials
  of degrees $d_S$ and $d_N$ that reduce signal and noise to stationarity:
\[
  \partial S_t = \Delta^S (L) S_t \qquad \partial N_t = \Delta^N (L) N_t.
\]
  These time series are all $n$-dimensional, but we assume that the same differencing
  operators are relevant for each component series.  It then follows that
\begin{equation}
\label{eq:diff-x.signoise}
 \partial X_t = \Delta^N (L) \, \partial S_t + \Delta^S (L) \, \partial N_t,
\end{equation}
 which allows us to relate the spectral density $f_{\partial X}$ of $\{ \partial X_t \}$
 to the spectral densities $f_{\partial S}$
  and $f_{\partial N}$ of $\{ \partial S_t \}$ and $\{ \partial N_t \}$:
 \begin{equation}
 \label{eq:sig-and-noise.sdf}
 f_{\partial X} (\omega) = {| \Delta^N (e^{-i \omega}) |}^2 \, f_{\partial S} (\omega)
  + {| \Delta^S (e^{-i \omega}) |}^2 \, f_{\partial N} (\omega).
 \end{equation}
  Each of these spectral densities
   is an $n \times n$-dimensional Hermitian function of $\omega$.
   Next, dividing (\ref{eq:sig-and-noise.sdf}) through by ${| \Delta (e^{-i\omega}) |}^2$,
  and defining the pseudo-spectral densities by
\begin{equation}
\label{eq:pseudo-sdf}
  f_X (\omega) = \frac{ f_{\partial X} (\omega)}{ {| \Delta (e^{-i\omega}) |}^2 }
  \quad   f_S (\omega) = \frac{ f_{\partial S} (\omega)}{ {| \Delta^S (e^{-i\omega}) |}^2 }
  \quad   f_N (\omega) = \frac{ f_{\partial N} (\omega)}{ {| \Delta^N (e^{-i\omega}) |}^2 },
\end{equation}
  we obtain the relation
\[
  f_X = f_S + f_N.
\]
 Next, the objective of MB signal extraction is to find a linear estimator 
   of $S_t$.  That is,  we seek to find a filter $\Psi (L)$ such that
  $\widehat{S}_{t} = \Psi (L) X_t$ has minimal mean squared estimation error of
   $S_{t}$.   This $\Psi (L)$ is called the Wiener-Kolmogorov (WK) signal extraction filter.
   The solution to this problem is not well-defined unless 
   additional assumptions about the signal and noise processes are made.  
   Extending the approach of Bell (1984) to the multivariate case,
   we assume that the initial values of $\{ X_t \}$ are uncorrelated with
    the differenced signal and noise processes; this is called Assumption A.
    
   {\bf Assumption A.}  The initial values   $X_0, \ldots, X_{1-d}$
   of the stochastic process  $\{ X_t \}$ (cf. Theorem  \ref{thm:nonstat-rep})
      are uncorrelated with $\{ \partial S_t \}$ and $\{ \partial N_t \}$.

\vspace{.25cm}
 
   
   The key result of McElroy and Trimbur (2015) is that the WK filter is
   obtained by the formula
  \begin{equation}
  \label{eq:wk.frf-gen}
    \Psi (e^{-i \omega}) =   f_{\partial S} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \, {| \Delta^N (e^{-i \omega}) |}^2.
  \end{equation}
  This presumes that $f_{\partial X} (\omega) $ is invertible; given this condition,
    one computes each filter coefficient by Fourier inversion of the frf.  
    
\begin{Theorem}
\label{thm:wk}
 Suppose that $\{ X_t \}$ is  a non-stationary stochastic process
 with representation (\ref{eq:nonstatCausalRep}), such that
 (\ref{eq:signal-noise-decomp}) holds and $f_{\partial X}$ is invertible.
 If the signal and noise differencing operators
 $\Delta^S (L)$ and $\Delta^N (L)$ are relatively prime, and Assumption A holds,
 then the WK filter has frf given by (\ref{eq:wk.frf-gen}).
\end{Theorem}
 
 
\paragraph{Proof of Theorem \ref{thm:wk}.} 
 Because of the factor $\Delta^N (e^{-i \omega})$  in the filter's frf,
  the corresponding filter $\Psi (L)$ can be expressed as
  $\Omega (L) \Delta^N (L)$, where $\Omega (e^{-i \omega})$ has bounded modulus
   for $\omega \in [-\pi, \pi]$.  Also, it follows from
   (\ref{eq:sig-and-noise.sdf}) that the noise filter's frf is
\[
 1 - \Psi (e^{-i \omega}) = f_{\partial N} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \, {| \Delta^S (e^{-i \omega}) |}^2.
\]
 Hence, $1 - \Psi (L) = \Phi (L) \Delta^S (L)$ for some $\Phi (e^{-i \omega})$
 with bounded modulus.  As a result, we can write the error process as
\[
 \varepsilon_t = S_t - \Psi (L) X_t = (1 - \Psi (L)) S_t - \Psi (L) N_t 
  = \Phi (L) \partial S_t - \Theta (L) \partial N_t.
\]
 This shows that the error process $\{ \varepsilon_t \}$ is covariance stationary
  with mean zero.   Optimality is proved if we can show that $\varepsilon_t$
  is uncorrelated with $X_{t-h}$ for all $h \in \ZZ$.  
 Using (\ref{eq:nonstatCausalRep}), it suffices to show that $\varepsilon_t$
  is uncorrelated with $\partial X_{t-h}$ for all $h \in \ZZ$, since by Assumption
  A the initial values of $\{ X_t \}$ are uncorrelated with $\{ \partial S_t \}$ and
   $\{ \partial N_t \}$.  Next, using (\ref{eq:diff-x.signoise}) we obtain
\begin{align*}
  \EE [ \varepsilon_t { \partial X_{t-h}}^{\prime} ]
  & = \frac{1}{2\pi} \int_{-\pi}^{\pi}  e^{i \omega h} \Phi (e^{-i \omega})
    f_{\partial S} (\omega) \Delta^N (e^{i \omega}) \, d\omega
    - \frac{1}{2\pi} \int_{-\pi}^{\pi}  e^{i \omega h} \Omega (e^{-i \omega})
    f_{\partial N} (\omega) \Delta^S (e^{i \omega}) \, d\omega  \\
  & = \frac{1}{2\pi} \int_{-\pi}^{\pi}  e^{i \omega h}
    f_{\partial N} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \,  \Delta^S (e^{i \omega})
      f_{\partial S} (\omega) \Delta^N (e^{i \omega}) \, d\omega \\
    &  - \frac{1}{2\pi} \int_{-\pi}^{\pi}  e^{i \omega h}
            f_{\partial S} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \,  \Delta^N (e^{i \omega})
      f_{\partial N} (\omega) \Delta^S (e^{i \omega}) \, d\omega  \\
  & =    \frac{1}{2\pi} \int_{-\pi}^{\pi}  e^{i \omega h} \left(
     f_{\partial N} (\omega) \, { f_{\partial X} (\omega) }^{-1} \,  
     f_{\partial S} (\omega)   
   -  f_{\partial S} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \,  
      f_{\partial N} (\omega) \right) \Delta (e^{i \omega}) \, d\omega.
\end{align*}
 Lastly, it can be shown that
\[ f_{\partial N} (\omega) \, { f_{\partial X} (\omega) }^{-1} \,  
     f_{\partial S} (\omega) = f_{\partial S} (\omega) \, 
    { f_{\partial X} (\omega) }^{-1} \,  
      f_{\partial N} (\omega),
\]
 which concludes the proof.  $\quad \Box$


\vspace{.5cm}

 We can use (\ref{eq:pseudo-sdf})   to obtain a simpler expression for the WK filter's frf:
\[
   \Psi (e^{-i \omega}) =    f_{ S} (\omega) \,     { f_{ X} (\omega) }^{-1}.
  \]
  It is interesting that we do not require that $f_{\partial S} (\omega)$ be 
  invertible -- this will be further explored in Chapter \ref{chap:coint}.
  We now discuss several examples of WK filters.

\begin{Example} {\bf Model-Based Random Walk Trend.} \rm
\label{exam:trend-i1}
  The Local Level Model (LLM) discussed in Harvey (1989) is capable
  of modeling a time series consisting
 of a  random walk trend $\{ S_t \}$ and a     white noise irregular
 $\{ N_t \}$, such  that $X_t = S_t + N_t$. 
  Hence $\{ S_t \}$ is $I(1)$, and $\partial S_t = (1-L) S_t$.
  Thus $\Delta^S (z) = 1- z$, but $\{ N_t \}$ is stationary so that
  $\Delta^N (z) = 1$.  
 Both the multivariate trend and  irregular are driven by independent 
 white noise processes, with respective covariance matrices
   $\Sigma_{S}$ and $\Sigma_{N}$,
  and it follows that the spectra for the differenced processes 
   are
  \[
    f_{\partial S} (\omega) = \Sigma_S \qquad f_{N} (\omega) = \Sigma_N.
\]
  Therefore  the frf for the optimal trend extraction filter is
\[ 
 \Psi (e^{-i \omega}) = \Sigma_{S} \, 
 { \left[ \Sigma_{S} + (2 - 2 \, \cos (\omega)) \, \Sigma_{N} \right] }^{-1},
\]
 which utilizes (\ref{eq:sig-and-noise.sdf}).
 \end{Example}

\begin{Exercise} {\bf LLM Model-Based Trend Filter.} \rm
\label{exer:trend-i1}
 For a bivariate LLM of Example \ref{exam:trend-i1} with parameters 
\[
 \Sigma_{S} = 10^{-4} \,\left[ \begin{array}{ll} 
   2.32  &  5.04  \\
   5.04  & 34.73   \end{array}  \right]
 \qquad  \Sigma_{N} = 10^{-5} \, \left[ \begin{array}{ll}
        110.44   &  7.17  \\
        7.17     & 128.57   \end{array} \right],
\]
 numerically compute and plot the trend extraction filter's frf.
\end{Exercise}

<<exercise_trend-i1,echo=True>>=
psi.sim <- c(2.17150287559847, -8.36795922528, -6.04133725367594, 
             0.0648981656699, -6.80849700177184, -6.66004335288479, 
             -0.00016098322952, 0.00051984185863)
psi.sim[7:8] <- c(0,0)
N <- 2
grid <- 1000
delta <- array(t(c(1,-1)) %x% diag(N),c(N,N,2))
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),10,Inf)
Sigma.mu <- mu.sim[[2]]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),10,Inf)
Sigma.irr <- irr.sim[[2]]
#print(Sigma.mu)
#print(Sigma.irr)

iden <- array(diag(N),c(N,N,1))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.irr <- mdfa.spectra(iden,iden,Sigma.irr,grid)
trend.frf <- mdfa.wkfrf(iden,delta,f.irr,f.mu)
@

<<echo=False>>=
# visualize
file <- paste("llm_frf.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
par(mar=c(2,2,2,2)+0.1,cex.lab=.8,mfrow=c(N,N))
for(i in 1:N)
{
  for(j in 1:N)
  {
    plot(ts(Re(trend.frf[i,j,]),frequency=grid/2,start=-1),ylim=c(0,1),ylab="",
         xlab="Cycles",yaxt="n",xaxt="n")
    axis(1,cex.axis=.5)
    axis(2,cex.axis=.5)
  }
}
invisible(dev.off())
@


\begin{figure}[htb!]
\begin{center}
\includegraphics[]{llm_frf.pdf}
\caption{Frequency response function for model-based trend
 extraction filter from the Local Level Model.}
\label{fig:llm-frf}
\end{center}
\end{figure} 


\begin{Example} {\bf Model-Based Integrated Random Walk Trend.} \rm
\label{exam:trend-i2}
  Example \ref{exam:trend-i1} can be generalized to the Smooth 
  Trend Model (STM) developed in Harvey (1989),
 where now the trend $\{ S_t \}$ is an integrated random walk, 
 i.e., ${(1-L)}^2 S_t$ is white noise of
 covariance matrix $\Sigma_{S}$.   Then the frf for the optimal 
 trend extraction filter -- which also coincides
 with the multivariate HP filter  -- is given by
\[ 
 \Psi (e^{-i \omega}) = e_j^{\prime} \, \Sigma_{S} \, 
 { \left[ \Sigma_{S} + {(2 - 2 \, \cos (\omega))}^2 \, \Sigma_{N} \right] }^{-1}.
\]
 The chief difference with the frf of the LLM is that the sinusoidal factor is now squared.  
\end{Example}

\begin{Exercise} {\bf STM Model-Based Trend Filter.} \rm
\label{exer:trend-i2}
 For a bivariate STM of Example \ref{exer:trend-i2} with parameters 
\[
 \Sigma_{S} = 10^{-5} \, \left[ \begin{array}{ll} 
   .66   &  1.25   \\
   1.25  &  2.92   \end{array}  \right]
 \qquad  \Sigma_{N} =  10^{-4} \, \left[ \begin{array}{ll}
        2.52  &  1.67    \\
        1.67 &  35.70   \end{array} \right],
\]
 numerically compute and plot the trend extraction filter's frf.
\end{Exercise}


<<exercise_trend-i2,echo=True>>=
psi.sim <- c(1.8905590615422, -11.9288577633298, -12.0809347541079, 
             0.660897814610799, -8.2863379601304, -5.66645335346871, 
             -1.34743227511595e-05, -1.41207967213544e-05)
psi.sim[7:8] <- c(0,0)
N <- 2
grid <- 1000
delta <- array(t(c(1,-2,1)) %x% diag(N),c(N,N,3))
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),10,Inf)
Sigma.mu <- mu.sim[[2]]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),10,Inf)
Sigma.irr <- irr.sim[[2]]
#print(Sigma.mu)
#print(Sigma.irr)

iden <- array(diag(N),c(N,N,1))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.irr <- mdfa.spectra(iden,iden,Sigma.irr,grid)
trend.frf <- mdfa.wkfrf(iden,delta,f.irr,f.mu)
@

<<echo=False>>=
# visualize
file <- paste("stm_frf.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
par(mar=c(2,2,2,2)+0.1,cex.lab=.8,mfrow=c(N,N))
for(i in 1:N)
{
  for(j in 1:N)
  {
    plot(ts(Re(trend.frf[i,j,]),frequency=grid/2,start=-1),ylim=c(0,1),ylab="",
         xlab="Cycles",yaxt="n",xaxt="n")
    axis(1,cex.axis=.5)
    axis(2,cex.axis=.5)
  }
}
invisible(dev.off())
@

 \begin{figure}[htb!]
\begin{center}
\includegraphics[]{stm_frf.pdf}
\caption{Frequency response function for model-based trend
 extraction filter from the Smooth Trend Model.}
\label{fig:stm-frf}
\end{center}
\end{figure} 


\begin{Example} {\bf Model-Based Seasonal Adjustment.} \rm
\label{exam:sa}
  Flexible structural models were discussed in McElroy (2017), 
  with atomic components for each distinct unit root
 (with any conjugate roots) in the differencing operator.  
 For monthly data  where $\Delta (L) = (1-L)(1-L^{12})$,
 we obtain an integrated random walk trend component $\{ C_t \}$ 
 (identical to the trend discussed in Example \ref{exam:trend-i2})
 and six atomic seasonal components that combine into a single 
 seasonal component $\{ P_t \}$ with differencing
 operator $U(L) = 1 + L + L^2 + \ldots + L^{11}$, along with the
 irregular $\{ I_t \}$, which is a white noise. 
  Six separate covariance matrices govern the dynamics of the seasonal
  component, allowing for different degrees of
 smoothness at each of the six seasonal frequencies. 
 In particular, we have $P_t = \sum_{\ell=1}^6 P^{(\ell)}_t$ and
\begin{align*}
  {(1-L)}^2 C_t & = \partial C_t \\
   (1 - 2 \cos (\pi/6) L + L^2) P^{(1)}_t & = \partial P^{(1)}_t \\
  (1 - 2 \cos (2\pi/6) L + L^2) P^{(2)}_t & = \partial P^{(2)}_t \\
  (1 - 2 \cos (3\pi/6) L + L^2) P^{(3)}_t & = \partial P^{(3)}_t \\
  (1 - 2 \cos (4\pi/6) L + L^2) P^{(4)}_t & = \partial P^{(4)}_t \\
  (1 - 2 \cos (5\pi/6) L + L^2) P^{(5)}_t & = \partial P^{(5)}_t \\
  (1 +L) P^{(6)}_t & = \partial P^{(6)}_t. 
 \end{align*}
 Each of the stationary processes is assumed to be an independent white noise,
 where the covariance matrices are denoted by $\Sigma_C$, $\Sigma_{1}$,
  $\Sigma_2$, $\Sigma_3$, $\Sigma_4$, $\Sigma_5$, $\Sigma_6$, and 
  $\Sigma_I$ respectively.   For seasonal adjustment we seek to suppress
  seasonality, so $S_t = C_t + I_t$ and $N_t = P_t$.  Thus $\Delta^S (z) = {(1-z)}^2$
  and $\Delta^N (z) = U(z)$, and the MB seasonal adjustment filter   has frf
\[
  \Psi (e^{-i \omega}) = e_j^{\prime} \, \left( \Sigma_{C} + {|1 - e^{-i \omega}|}^4
   \Sigma_I \right) \, 
    { f_{\partial X} (\omega) }^{-1} \, {| U (e^{-i \omega}) |}^2.
\]
\end{Example}


\begin{Exercise} {\bf Structural Model-Based Seasonal Adjustment.} \rm
\label{exer:sa}
 Consider a quadvariate structural model of Example \ref{exer:sa} with parameters 
\begin{align*}
 \Sigma_C & = 10^{-2} \, \left[ \begin{array}{llll} 
  9.54 & 4.71 & 1.70 &  3.26  \\
  4.71 & 2.74 & 1.01 & 1.96  \\
  1.70 & 1.01 & 0.49 & 0.81 \\
  3.26 & 1.96 & 0.81 &  1.70  \end{array} \right] \\
  \Sigma_1 & = 10^{-2} \, \left[ \begin{array}{llll} 
 7.97 & 6.98 & 1.77 & 3.99 \\
 6.98 & 7.47 & 2.01 & 4.57 \\
 1.77 & 2.01 & 0.80 & 1.50 \\
 3.99 & 4.57 & 1.50 & 3.85 \end{array} \right] \\
   \Sigma_2 & = 10^{-2} \, \left[ \begin{array}{llll} 
 1.76 & 0.17 & 0.35 & 1.49 \\
 0.17 & 0.94 & 0.48 & 0.72 \\
 0.35 & 0.48 & 1.17 & 1.58 \\
 1.49 & 0.72 & 1.58 & 4.49 \end{array} \right] \\
    \Sigma_3 & = 10^{-2} \, \left[ \begin{array}{llll} 
  4.71 &  4.35 & -1.87 &  1.15 \\
  4.35 &  4.89 & -2.04 &  1.38 \\
 -1.87 & -2.04 &  1.55 & -0.04 \\
  1.15 &  1.38 & -0.04 &  1.43 \end{array} \right] \\
    \Sigma_4 & = 10^{-2} \, \left[ \begin{array}{llll} 
 12.56 &  3.30 & -2.28  & 1.88 \\
  3.30 &  3.49 & -0.88 &  1.06 \\
 -2.28 & -0.88 &  0.78 & -0.45 \\
  1.88 &  1.06 & -0.45 &  0.88 \end{array} \right] \\
     \Sigma_5 & = 10^{-2} \, \left[ \begin{array}{llll} 
 1.07 & 1.51 & -0.01 & -0.24  \\
 1.51 & 3.04 &  0.01  & 0.79 \\
-0.01 & 0.01  & 0.09  & 0.05 \\
-0.24 & 0.79  & 0.05  & 1.69 \end{array} \right] \\
     \Sigma_6 & = 10^{-2} \, \left[ \begin{array}{llll} 
 11.79 & -1.17 &   1.38 &  1.86 \\
 -1.17 &   2.77 & -0.23 & 0.30 \\
  1.38 & -0.23 &   0.77 & 0.40 \\
  1.86  & 0.30  & 0.40 & 3.11 \end{array} \right] \\
      \Sigma_I & =  \left[ \begin{array}{llll} 
  6.53 & 0.81 & 0.19 & -0.57 \\
  0.81 & 1.26 & 0.23  & 0.49 \\
  0.19 & 0.23 & 0.30  & 0.15 \\
 -0.57 & 0.49 & 0.15  & 1.08 \end{array} \right].
\end{align*} 
 Numerically compute and plot the  filter frf for seasonal adjustment.
\end{Exercise}

<<exercise_sa,echo=True>>=
psi.sim <- c(0.493586093056948, 0.178487258592539, 0.341217399125708, 
             0.399177274154249, 0.848325304642642, 0.68306879252262, 
             -2.3494687111314, -5.47534663726587, -6.69385117951384, 
             -6.08364145983965, 0.875100150810273, 0.221971271148611, 
             0.500866759201029, 0.340625016984097, 0.791037805495801, 
             0.985440262768576, -2.52890913740106, -4.29524634814519, 
             -5.98519527750281, -4.88659954275053, 0.0957466327314851, 
             0.201313350626488, 0.849351809157598, 0.48420520104336, 
             0.62643997675928, 1.13945063379914, -4.04217214895869, 
             -4.68919816059416, -4.73313805629826, -4.0627015759002,
             0.923495751608401, -0.396067294450726, 0.244665046194039, 
             -0.36570474542918, 0.363995718736632, 0.758715172737758, 
             -3.05567431351817, -4.74337970092605, -4.96364133429136, 
             -5.06144086942249, 0.262963683605793, -0.181599400661918, 
             0.149795833258992, -0.105991649100357, 0.21503766242974, 
             -0.141649861043968, -2.07489346121933, -3.64302004053168, 
             -5.69277788172285, -5.3689470753418, 1.40718934367933,
             -0.0085452878747676, -0.219886337273936, 0.0283662345070971,
             1.23786259577472, 0.199834135215749, -4.53336362894347, 
             -4.70016052568401, -7.07530853221777, -6.03054443735399, 
             -0.0995506040524902, 0.116607848697947, 0.157899802233636, 
             -0.0363184981547607, 0.18385749297074, 0.329351477585333, 
             -2.1377604820296, -3.62882764786239, -5.11279846492415, 
             -3.62475631527416, 0.124305286145147, 0.0292507920421885, 
             -0.0873349194845382, 0.178977764316143, 0.484389128732254,
             0.265835976421986, 1.87566939226944, 0.1445002084775, 
             -1.34264222816582, -0.305367634014929, -0.00488431480035087, 
             -0.000945659564684563, -0.00106126820173145, -0.000413658838890233)
psi.sim[81:84] <- c(0,0,0,0)
N <- 4
grid <- 1000
mu.sim <- mdfa.wnsim(psi.sim[1:10],rep(1,N),10,Inf)
Sigma.mu <- mu.sim[[2]]
seas1.sim <- mdfa.wnsim(psi.sim[11:20],rep(1,N),10,Inf)
Sigma.seas1 <- seas1.sim[[2]]
seas2.sim <- mdfa.wnsim(psi.sim[21:30],rep(1,N),10,Inf)
Sigma.seas2 <- seas2.sim[[2]]
seas3.sim <- mdfa.wnsim(psi.sim[31:40],rep(1,N),10,Inf)
Sigma.seas3 <- seas3.sim[[2]]
seas4.sim <- mdfa.wnsim(psi.sim[41:50],rep(1,N),10,Inf)
Sigma.seas4 <- seas4.sim[[2]]
seas5.sim <- mdfa.wnsim(psi.sim[51:60],rep(1,N),10,Inf)
Sigma.seas5 <- seas5.sim[[2]]
seas6.sim <- mdfa.wnsim(psi.sim[61:70],rep(1,N),10,Inf)
Sigma.seas6 <- seas6.sim[[2]]
irr.sim <- mdfa.wnsim(psi.sim[71:80],rep(1,N),10,Inf)
Sigma.irr <- irr.sim[[2]]
#print(Sigma.mu)
#print(Sigma.seas1)
#print(Sigma.seas2)
#print(Sigma.seas3)
#print(Sigma.seas4)
#print(Sigma.seas5)
#print(Sigma.seas6)
#print(Sigma.irr)

iden <- array(diag(N),c(N,N,1))
dpoly.1 <- c(1,-2*cos(pi/6),1)
dpoly.2 <- c(1,-2*cos(2*pi/6),1)
dpoly.3 <- c(1,-2*cos(3*pi/6),1)
dpoly.4 <- c(1,-2*cos(4*pi/6),1)
dpoly.5 <- c(1,-2*cos(5*pi/6),1)
dpoly.6 <- c(1,1)
dpoly.but1 <- polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but2 <- polymult(dpoly.1,polymult(dpoly.3,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but3 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but4 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.5,dpoly.6))))
dpoly.but5 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,dpoly.6))))
dpoly.but6 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,dpoly.5))))
delta.c <- array(t(c(1,-2,1)) %x% diag(N),c(N,N,3))
delta.but1 <- array(t(dpoly.but1) %x% diag(N),c(N,N,10))
delta.but2 <- array(t(dpoly.but2) %x% diag(N),c(N,N,10))
delta.but3 <- array(t(dpoly.but3) %x% diag(N),c(N,N,10))
delta.but4 <- array(t(dpoly.but4) %x% diag(N),c(N,N,10))
delta.but5 <- array(t(dpoly.but5) %x% diag(N),c(N,N,10))
delta.but6 <- array(t(dpoly.but6) %x% diag(N),c(N,N,11))
delta.seas <- array(t(rep(1,12)) %x% diag(N),c(N,N,12))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.seas1 <- mdfa.spectra(iden,delta.but1,Sigma.seas1,grid)
f.seas2 <- mdfa.spectra(iden,delta.but2,Sigma.seas2,grid)
f.seas3 <- mdfa.spectra(iden,delta.but3,Sigma.seas3,grid)
f.seas4 <- mdfa.spectra(iden,delta.but4,Sigma.seas4,grid)
f.seas5 <- mdfa.spectra(iden,delta.but5,Sigma.seas5,grid)
f.seas6 <- mdfa.spectra(iden,delta.but6,Sigma.seas6,grid)
f.irr <- mdfa.spectra(iden,delta.c,Sigma.irr,grid)
f.signal <- f.mu + f.irr
f.noise <- f.seas1 + f.seas2 + f.seas3 + f.seas4 + f.seas5 + f.seas6
sa.frf <- mdfa.wkfrf(delta.seas,delta.c,f.noise,f.signal)
@

<<echo=False>>=
# visualize
file <- paste("sauc_frf.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
par(mar=c(2,2,2,2)+0.1,cex.lab=.8,mfrow=c(N,N))
for(i in 1:N)
{
  for(j in 1:N)
  {
    plot(ts(Re(sa.frf[i,j,]),frequency=grid/2,start=-1),ylim=c(-.5,1),ylab="",
          xlab="Cycles",yaxt="n",xaxt="n")
    abline(h=0,col=grey(.7))
    axis(1,cex.axis=.5)
    axis(2,cex.axis=.5)
  }
}
invisible(dev.off())
@

\begin{figure}[htb!]
\begin{center}
\includegraphics[]{sauc_frf.pdf}
\caption{Frequency response function for model-based seasonal adjustment
  filter from the Structural Model.}
\label{fig:sauc-frf}
\end{center}
\end{figure} 



\section{Error Criterion and Computation}
\label{sec:mdfa-nonstat}

We now consider LPPs for non-stationary processes. 
 The definition of target is the same as that given in Definition \ref{def:target2},
 and the LPP is still defined via Definition \ref{def:lpp2},
 but now the underlying process is non-stationary.  This changes
  slightly the solution, because forecasting must take the non-stationarity
  into account.
 

\begin{Proposition}
 \label{prop:GPP-nonstat}
 Suppose that $\{ X_t \}$ is  a non-stationary stochastic process
 with representation (\ref{eq:nonstatCausalRep}), such that
 $\{ \partial X_t \}$ is mean zero and weakly stationary 
 with  causal Wold decomposition expressed as $\partial X_t = \Theta (L) \, \epsilon_t$,
 where $\Theta (L)$ is invertible and
  $\{ \epsilon_t \}$ is white noise of covariance $\Sigma$.
  Then the solution
 to the LPP posed by a   target $Y_t = \Psi (L) \, X_t$ is given by
\begin{equation}
 \label{eq:GPPsoln-nonstat}
  \widehat{\Psi} (L) = \sum_{\ell = 0}^{\infty} \psi (\ell) \, L^{\ell}
    + \sum_{ \ell = -1}^{- \infty} \psi (\ell) \, 
\left(  \sum_{h=0 }^{d-1} A_{-\ell} (h) L^{h} + \sum_{k=1}^{-\ell} \xi (-\ell-k)
  { [  \Theta (L) ] }_{k}^{\infty} L^{-k} \Delta (L) { \Theta (L)}^{-1} 
 \right).
\end{equation}
 Moreover, the   MSE  corresponding to this solution is given by
\begin{equation} 
\label{eq:minimalMSE}
 \frac{1}{ 2 \pi} \int_{-\pi}^{\pi}   \sum_{\ell, k > 0 } \psi (-\ell) \,
  {[ \Theta  (e^{-i \omega}) / \Delta (e^{-i \omega}) ]}_0^{\ell-1}   \,  \Sigma \,
  { {[ \Theta  (e^{i \omega}) / \Delta (e^{i \omega}) ]}_0^{ k-1} }^{\prime}  \,
   {\psi (-k) }^{\prime} \,  e^{i \omega (\ell - k) }   \, d\omega.
\end{equation}
 \end{Proposition}
 
\paragraph{Proof of Proposition \ref{prop:GPP-nonstat}.} 
 We show that the filter error $\Lambda (z) = \Psi (z) - \widehat{\Psi} (z)$
  is divisible by $\Delta (z)$, so that we can write
   $\Lambda (z) = \widetilde{\Lambda} (z) \Delta (z)$, from which it follows
    that the error process must be stationary.
  We make use of an algebraic identity proved in McElroy and Findley (2010):
  for any $j \geq 0$
\begin{equation}
 \label{eq:diffop-algid}
  {[ \Theta (z)/ \Delta (z)]}_0^{j} = \sum_{k=0}^j \xi (k) z^k { [ \Theta (z) ] }_0^{j-k}.
\end{equation}
 Then using the definition of $\widehat{\Psi}(L)$ in (\ref{eq:GPPsoln-nonstat}),
\begin{align*}    
  \Lambda (z) & =  \sum_{\ell = -1}^{- \infty} \psi (\ell) 
   \left( z^{\ell} - \sum_{h=0}^{d-1} A_{-\ell} (h) z^h  - \sum_{k=1}^{-\ell} \xi (-\ell-k)
    {[ \Theta (z)]}_k^{\infty} z^{-k} \Delta (z) { \Theta (z)}^{-1}  \right) \\
    & = \sum_{\ell = -1}^{- \infty} \psi (\ell) 
   \left( \sum_{k=0}^{-\ell-1} \xi (k) z^{k+\ell} \Theta (z) -
     \sum_{k=0}^{-\ell-1} \xi (k)
    {[ \Theta (z)]}_{-\ell-k}^{\infty} z^{k+\ell} \Delta (z) { \Theta (z)}^{-1} \right) \\
  & = \sum_{\ell = -1}^{- \infty} \psi (\ell) 
   \left( \sum_{k=0}^{-\ell-1} \xi (k)  z^{k+\ell} 
    {[ \Theta (z)]}_0^{-\ell-k-1}   \right) \Delta (z) { \Theta (z)}^{-1} \\
   & = \sum_{\ell = -1}^{- \infty} \psi (\ell) z^{\ell}
    { [ \Theta (z) / \Delta (z)]}_0^{-\ell-1} 
    \Delta (z) { \Theta (z)}^{-1},
\end{align*}    
   where the second equality follows from (\ref{eq:Identity1}), 
    and the fourth equality  follows from (\ref{eq:diffop-algid}).
    Hence, we find that
\[
 \widetilde{\Lambda} (z) = \sum_{\ell = -1}^{- \infty} \psi (\ell) z^{\ell}
    { [ \Theta (z) / \Delta (z)]}_0^{-\ell-1}  { \Theta (z)}^{-1},
\]
 which is well-defined under the invertibility assumption.  
 The real-time error process is then
\[
  \Lambda (L) X_t = \widetilde{\Lambda} (L) \, \partial X_t
    = \sum_{\ell = -1}^{- \infty} \psi (\ell)  
    { [ \Theta (L) / \Delta (L)]}_0^{-\ell-1}\, \epsilon_{t-\ell},
\]
 which only involves future innovations.  Hence the error process is 
  uncorrelated with present and past values of the process $\{ X_t \}$,
   demonstrating optimality.  The MSE is computed 
 by taking the variance of the error process.  $\qquad \Box$

% \widehat{\Psi} (L) = \sum_{\ell = -\infty }^{\infty} \psi (\ell) \, L^{\ell} - 
% \sum_{\ell < 0 } \psi (\ell)
% \,  { [ \Theta (L) / \Delta (L) ]}_{0}^{ -\ell-1  } \, L^{\ell} \,
% {\Theta (L) }^{-1} \Delta(L).

 
\vspace{.5cm}

 Proposition  \ref{prop:GPP-nonstat} gives the optimal concurrent filter in 
  the non-stationary case, and the answer depends upon knowing the process'
   dynamics.  However, for MDFA we wish to proceed without such an assumed
   knowledge.   From the proof of Proposition  \ref{prop:GPP-nonstat}, it
  is clear that  the error process will  
 not be stationary unless we make certain assumptions
 about $\Lambda (L) = \Psi (L) - \widehat{\Psi} (L)$.    
 In order to remove the time-varying functions it is necessary that 
 we can factor $\Delta (L)$ from $\Lambda (L)$, i.e., we require the existence of
 $\widetilde{\Lambda } (L)$ such that
\begin{equation}
 \label{eq:delta.factor}
  \Lambda (L) = \widetilde{\Lambda } (L) \, \Delta (L),
\end{equation}
 as otherwise we cannot guarantee that $\{ E_t \}$ will be stationary. 
 (This condition (\ref{eq:delta.factor}) is indeed satisfied by the optimal 
 concurrent filter of Proposition \ref{prop:GPP-nonstat}, as is shown in the 
  proof.)  Moreover, if $\mu =0$ then (\ref{eq:delta.factor}) is also 
  sufficient to guarantee
 that the filter error be stationary, because
\[
  E_t = \widetilde{\Lambda} (L) \, \partial X_t
\]
 in such a case.   We next discuss a set of filter 
 constraints that guarantee (\ref{eq:delta.factor}), beginning with a result
 that discusses the factorization of filters.  
 We say a filter $\Psi (L)$ is absolutely convergent 
 if $\sum_{j \in \ZZ} \| \psi (j) \| < \infty$
 for a given matrix norm $\| \cdot \|$.

\begin{Proposition}
\label{prop:filter-decompose}
 Any linear filter $\Psi (L)$ can be expressed as
\[
  \Psi (L) = \Psi (\zeta) + (L - \zeta) \, \Psi^{\sharp} (L)
\]
 for any $\zeta \in \CC$  such that $| \zeta | = 1$,
  and an absolutely convergent filter $\Psi^{\sharp} (L)$,
  so long as  $\partial \Psi (L) $ is absolutely convergent.
 If in addition $ \partial \partial \Psi (L) =
 \sum_{ j \in \ZZ} j (j-1) \, \psi (j) \, L^j$
   is absolutely convergent, then there also exists an absolutely
   convergent filter $\Psi^{\flat} (L)$  such that
\[
 \Psi (L) = \Psi (\zeta) + \partial \Psi (\zeta) \, 
 (L- \zeta) \, \overline{\zeta} + {(L - \zeta)}^2 \, \Psi^{\flat} (L).
\]
\end{Proposition}


\paragraph{Proof of Proposition \ref{prop:filter-decompose}.}
 We claim that $\Psi^{\sharp} (L) = \sum_{j \in \ZZ} \psi^{\sharp} (j) \, L^j$ with
\[
 \psi^{\sharp} (j) = \begin{cases}  {\zeta}^{-(j+1)} \,  
 \sum_{k \geq j+1} \psi (k) \, \zeta^k  \quad j \geq 0 \\
					-\zeta^{-(j+1)} \, \sum_{k \geq -j} \psi (-k) \,  
					{\zeta}^{-k} \quad j \leq -1.	
		\end{cases}
\]
To show this, first observe that 
\[
  \Psi (L) - \Psi (\zeta) = \sum_{j \geq 1} \psi (j) \, 
  (L^j - \zeta^j) + \sum_{j \leq -1} \psi (j) \, (L^j - \zeta^j).
\]
 Beginning with the first term, so that $j \geq 1$, we write 
 $L^j - \zeta^j = \zeta^j \, (L/\zeta - 1) \, p_{j-1} (L/\zeta)$
 where $p_k (z) = \sum_{\ell=0}^k z^{\ell}$.   Next, by coefficient
 matching we can verify that
\begin{align*}
   \sum_{j \geq 1} \psi (j) \, (L^j - \zeta^j) & = (L/\zeta - 1) \,
   \sum_{j \geq 1} \psi (j) \, \zeta^j \, p_{j-1} (L/\zeta) \\
 & = (L/\zeta -1) \, \sum_{j \geq 0}  \sum_{k \geq j+1} \psi (k) \,
 \zeta^k \, {(L/\zeta)}^{j}
 = (L - \zeta) \,  \sum_{j \geq 0}  \psi^{\sharp} (j) \, L^j.
\end{align*}
 Next, take $j \leq -1$, and use the symbol $F = L^{-1}$:
\begin{align*}
  \sum_{j \leq -1} \psi (j) \, (L^j - \zeta^j) & = \sum_{j \geq 1}
  \psi (-j) \, (F^j - \zeta^{-j}) 
   =   (F \zeta - 1) \, \sum_{j \geq 1} \psi (-j) \, \zeta^{-j} \, p_{j-1} (F \zeta) \\
 & =  (F \zeta - 1) \, \sum_{j \geq 0} \sum_{k \geq j+1}  
  \psi (-k) \, \zeta^{-k} \, {(F \zeta)}^{j}  \\
 &  =  - F\,  (L - \zeta ) \, \sum_{j \geq 1}  \zeta^{j-1} 
 \, \sum_{k \geq j}  \psi (-k) \, \zeta^{-k} \,  F^{j-1}  \\
 &= (L - \zeta) \, \sum_{j \leq -1}  \psi^{\sharp} (j) \, L^j.
\end{align*}
 This establishes algebraically that $\Psi^{\sharp} (L)$ 
 with coefficients as defined above equals
 $(\Psi (L) - \Psi (\zeta))/(L- \zeta)$, whenever the Laurent
 series converges.  Based on the above calculations, we can write
\[
  \frac{ \Psi (L) - \Psi (\zeta) }{ L - \zeta} 
  = \sum_{j \geq 1} \left( \psi (j) \, \zeta^{j-1} \, p_{j-1}
  (L/\zeta) - \psi (-j) \, \zeta^{-j} \, F \, p_{j-1} (F \zeta) \right).
\]
 To check the absolute convergence, it suffices to set $L = 1$;  
 note that $| p_k  (\zeta) | \leq (k+1)$
 if $|\zeta| = 1$.  Thus  we obtain the bound
\[
 \left\|   \frac{ \Psi (L) - \Psi (\zeta) }{ L - \zeta}  \right\| 
 \leq \sum_{j \geq 1}  j \, \left( \| \psi (j) \|  +\| \psi (-j) \| \right),
\]
 which is finite by the assumption that $\partial \Psi (L)$ 
 is absolutely convergent.   Next, we claim that
$\Psi^{\flat} (L) = \sum_{j \in \ZZ} \psi^{\flat} (j) \, L^j$ with
\[
 \psi^{\flat} (j) = \begin{cases}  {\zeta}^{-(j+2)} \, 
 \sum_{k \geq j+2} (k-1-j) \, \psi (k) \, \zeta^k  \quad j \geq 0 \\
					\zeta^{-(j+2)} \, \sum_{k \geq -j} (k+j+1) \,
					\psi (-k) \,  {\zeta}^{-k} \quad j \leq -1.	
		\end{cases}
\]
To verify this, observe that
\begin{align*}
  \Psi (L) - \Psi (\zeta)  - \partial \Psi (\zeta) \, (L- \zeta) \, \zeta^{-1} 
 & = \sum_{j \geq 1} \psi (j) \, \left[ (L^j - \zeta^j)  
 - j \, \zeta^{j-1}   \, (L- \zeta) \right]  \\
& + \sum_{j \leq -1} \psi (j) \, \left[ (L^j - \zeta^j)  
- j \, \zeta^{j-1}   \, (L- \zeta) \right].
\end{align*}
 First assuming that $j \geq 1$, note that $p_{\ell-1} (z) - \ell$
 equals zero unless $\ell \geq 2$, 
 and otherwise   equals $\sum_{k=1}^{\ell-1} p_{k-1} (z) \, (z -1)$. Therefore 
\begin{align*}
 \sum_{j \geq 1} \psi (j) \, \left[ (L^j - \zeta^j)  
 - j \, \zeta^{j-1}  \, (L- \zeta) \right]    & 
 = (L- \zeta) \, \sum_{j \geq 1} \psi (j) \, 
 \zeta^{j-1} \, \left[   p_{j-1} (L/\zeta) 	  - j   \right]   \\
 & =  {(L - \zeta)}^2 \, \sum_{j \geq 2} \psi (j) \, 
 \zeta^{j-2} \, \sum_{k=1}^{j-1} p_{k-1} (L/\zeta)   \\
 & =  {(L - \zeta)}^2 \, \sum_{j \geq 0} \psi^{\flat} (j) \,  L^j 
\end{align*}
 by coefficient matching in the final step.  Similarly, letting $j \leq -1$ 
 and using $z p_{\ell-1} (z) - \ell = (z-1) \, 
 \sum_{k=1}^{\ell} p_{k-1} (z)$, we have
\begin{align*}
 \sum_{j \leq -1} \psi (j) \, \left[ (L^j - \zeta^j) 
 - j \, \zeta^{j-1}  \, (L- \zeta) \right]    & 
  =  (L- \zeta) \, \sum_{j \geq 1} \psi (-j) \, \zeta^{-j} \,
  \left[ - F \,  p_{j-1} (F \zeta)  +  j  \, \zeta^{-1} \right]   \\
 & =  {(L- \zeta)}^2 \, F \, \sum_{j \geq 1} \psi(-j) \, 
 \zeta^{-(j+1)} \, \sum_{k=1}^{j} p_{k-1} (F \zeta)  \\
 & = {(L- \zeta)}^2 \, F \, \zeta^{-1} \, \sum_{j \geq 0} 
 \sum_{k \geq j+1}  (k-j) \, \psi (-k) \, \zeta^{-k} \, {(F \zeta)}^j \\
 & = {(L-\zeta)}^2 \,  \sum_{j \leq -1}  \psi^{\flat} (j) \, L^j
\end{align*}
 by matching coefficients.  To establish convergence of the 
 Laurent series for $\Psi^{\flat} (L)$,  observe that
\[
 \frac{  \Psi (L) - \Psi (\zeta)  - \partial \Psi (\zeta) 
 \, (L- \zeta) \, \zeta^{-1}  }{ {(L- \zeta)}^2 } =
    \sum_{j \geq 2} \psi (j) \, \zeta^{j-2} \, \sum_{k=1}^{j-1} p_{k-1} (L/\zeta) +
     F \, \sum_{j \geq 1} \psi(-j) \, \zeta^{-(j+1)} \, \sum_{k=1}^{j} p_{k-1} (F \zeta).
\]
 Hence the matrix norm has   the bound (setting $L=1$ and taking $|\zeta| = 1$) of
\[
 \left\|   \frac{  \Psi (L) - \Psi (\zeta) 
 - \partial \Psi (\zeta) \, (L- \zeta) \, \zeta^{-1}  }{ {(L- \zeta)}^2 }  \right\|
  \leq  \sum_{j \geq 2} \| \psi (j)  \|  \, \binom{j}{2} + 
  \sum_{j \geq 1} \| \psi(-j)  \|  \, \binom{j+1}{2},
\]
 using $| \sum_{k=1}^j p_{k-1} (\zeta) | \leq \binom{j+1}{2}$.  
 Because $\partial \partial \Psi (L)$ is 
 absolutely convergent, the above norm is finite.  $\quad \Box$

\vspace{.5cm}

 Note that if $\Psi (\zeta) = 0$, it follows from 
  Proposition \ref{prop:filter-decompose} that $L-\zeta$ can be factored from
  $\Psi (L)$.  Similarly, ${(L- \zeta)}^2$ can be factored 
   from $\Psi (L)$ if $\Psi(\zeta) = \partial \Psi (\zeta) =0$.
  Next, we introduce the concept of $\omega$-dynamics, which correspond
  to basis functions $\phi_k (t) = e^{i \omega t}$.

\begin{Definition} \rm
\label{def:filter-noise}
 For $\omega \in [-\pi, \pi]$, a filter $\Psi (L)$ annihilates 
 $\omega$-dynamics of order $1$ if $\Psi (e^{-i \omega}) = 0$,
 and annihilates $\omega$-dynamics of order $2$ if in addition 
 $\partial \Psi (e^{-i \omega}) = 0$.
\end{Definition}


Hence, we have the following immediate corollary of 
Proposition \ref{prop:filter-decompose}.

\begin{Corollary}
 \label{cor:filter-noise}
  If a filter $\Psi (L)$ annihilates $\omega$-dynamics of order $1$ 
  and $\partial \Psi (L)$ is absolutely convergent, then
\[
  \Psi (L) = (L- e^{-i \omega}) \, \Psi^{\sharp} (L).
\]
 If a filter $\Psi (L)$ annihilate $\omega$-dynamics of order $2$,  
 and $\partial \partial \Psi (L)$ is absolutely convergent, then
\[
  \Psi (L) = {(L- e^{-i \omega}) }^2 \, \Psi^{\flat} (L).
\]
\end{Corollary}

 We can apply Corollary \ref{cor:filter-noise} to factor a 
 noise-differencing polynomial $\Delta^N (L)$ from $\Lambda (L)$:
 for each $\omega$ such that the target filter $\Psi (L)$ annihilates
 $\omega$-dynamics of order $d$, we impose the constraint
 that $\widehat{\Psi} (L)$ shall have the same property, and hence 
 ${(L- e^{-i \omega})}^d$ can be factored from both
 filters.   For instance, if noise frequencies are $\omega_{\ell}$
 with multiplicities $d_{\ell}$, then repeated application of
 Corollary \ref{cor:filter-noise} yields
\[
 \Psi (L) = \Delta^N (L) \, \Psi^{\natural} (L)
\]
 for some residual filter $\Psi^{\natural} (L)$, where 
 and $\Delta^N (L) = \prod_{\ell} (1 - e^{i \omega_{\ell}} \, L)$.
 By imposing the same linear constraints on $\widehat{\Psi} (L)$, 
 we likewise obtain $\widehat{\Psi} (L) = \Delta^N (L) \, \widehat{\Psi}^{\natural} (L)$,
 and hence
\begin{equation}
 \label{eq:delta-noise}
\Lambda (L) = \left(  {\Psi}^{\natural} (L) - 
\widehat{\Psi}^{\natural} (L) \right) \, \Delta^N (L).
\end{equation}
  So if $\Delta (L) = \Delta^N (L)$, then (\ref{eq:delta.factor}) 
  holds at once.  More generally, a given process' differencing polynomial
 may be factored into relatively prime polynomials $\Delta^N (z)$ and $\Delta^S (z)$, 
 which correspond to noise and signal dynamics
 respectively -- see Bell (1984) and McElroy (2008a). 
 Many  signal extraction filters $\Psi (L)$   have the property that they
 annihilate $\omega$-dynamics of the appropriate order, 
 such that $\Delta^N (L)$ can be factored.
 By imposing that 
\begin{equation}
\label{eq:non-stat.constraint.single}
 \widehat{\Psi} (e^{-i \omega}) = \Psi (e^{-i \omega})
 \end{equation}
 for  all simple roots $\zeta = e^{-i \omega}$ of $\Delta (z)$, we ensure that
  (\ref{eq:delta.factor}) holds. If there is a double root, then we also impose
 \begin{equation}
\label{eq:non-stat.constraint.double}
 \partial \widehat{\Psi} (e^{-i \omega}) = \partial \Psi (e^{-i \omega}).
 \end{equation}
   These conditions are sufficient, because 
  (\ref{eq:non-stat.constraint.single}) and  (\ref{eq:non-stat.constraint.double})
    imply $\Lambda (e^{-i \omega}) = 0$
  for all the roots of $\Delta (z)$, so that by repeated application of Corollary 
 \ref{cor:filter-noise} we can factor out $\Delta (z)$, labelling the
 remaining factor as $\widetilde{\Lambda } (L)$.  
   Note that if $\omega$ corresponds to noise, i.e., $\Delta^N (e^{-i \omega}) = 0$,
    then (\ref{eq:non-stat.constraint.single}) becomes 
  $ \widehat{\Psi} (e^{-i \omega}) = 0$, but if $\omega$ corresponds to signal
  then $\Psi (e^{-i \omega}) \neq 0$. 
  
  In fact, this characterization actually
  defines signal and noise.   Given a non-stationary process with differencing
  polynomial $\Delta (z)$ and a target filter $\Psi (L)$, we define $\Delta^N (z)$
  as consisting of those factors of $\Delta (z)$ such that
   $ \widehat{\Psi} (e^{-i \omega}) = 0$, and set $\Delta^S (z) = \Delta (z)/ \Delta^N (z)$.
  For either signal or noise $\omega$-dynamics, we know that 
  (\ref{eq:delta.factor}) holds if we impose  (\ref{eq:non-stat.constraint.single}) for all
   the single roots of $\Delta (z)$
   (and for double roots, also impose  (\ref{eq:non-stat.constraint.double})).
 In practice, we must determine the real and imaginary  parts of each such 
 constraint, and write the corresponding constraints on $\widehat{\Psi} (L)$ 
 in the form $K = [J \otimes 1_n] \, \vartheta$ for
  filters of form (\ref{eq:conc.filter}), applying the methodology 
  of this chapter's first section.   
In particular with $P$ defined via (\ref{eq:conc.filter}), we find that
 (\ref{eq:non-stat.constraint.single}) can be rendered as
 \[
  {\Psi (e^{-i \omega}) }^{\prime} = [1,e^{-i \omega}, e^{-i 2 \omega}, \ldots, 
   e^{-i (q-1) \omega} ] \otimes 1_n  \, P,
 \]
  which provides the definition of the matrix $J$ in terms of powers of $e^{-i \omega}$.
  Similarly, (\ref{eq:non-stat.constraint.double}) is rendered as
  \[
  {\partial \Psi (e^{-i \omega}) }^{\prime} = [0 ,1, 2 e^{-i \omega}, \ldots, 
   (q-1) e^{-i (q-2) \omega} ] \otimes 1_n  \, P.
 \] 
    With these constraints in play, 
   the formula (\ref{eq:dfa-mvar}) holds with $\Psi (z) - \widehat{\Psi} (z)$
   replaced by $\widetilde{\Lambda} (z)$
 and $F$ being the spectral density of $\{ \partial X_t \}$, i.e., 
 we define the nonstationary MDFA criterion
 function as $\det D_{\Psi } (\vartheta, G)$ for
\begin{equation}
\label{eq:mdfa-criterion-nonstat}
 D_{\Psi} (\vartheta, G) =  { \langle  \widetilde{\Lambda} (e^{-i \omega})   \, 
 G (\omega) \,  {\widetilde{\Lambda} (e^{i \omega}) }^{\prime}   \rangle }_0
 = { \langle  \left[ \Psi (e^{-i \omega}) - 
 \widehat{\Psi}_{\vartheta} (e^{-i \omega}) \right] \, 
 G (\omega) \, {|\Delta (e^{-i \omega}) |}^{-2} \,
  {  \left[ \Psi (e^{i \omega}) - 
  \widehat{\Psi}_{\vartheta} (e^{i \omega}) \right] }^{\prime} \rangle }_0.
\end{equation}
  The second expression in (\ref{eq:mdfa-criterion-nonstat}) 
  utilizes (\ref{eq:delta.factor}), and employs the understanding
 that poles in ${\Delta (z) }^{-1}$ are exactly canceled out by the 
 corresponding zeros in $\Psi (z) - \widehat{\Psi} (z)$.
  Moreover, the ratio $(\Psi (z) - \widehat{\Psi} (z))/\Delta (z) = 
  \widetilde{\Lambda} (z)$ is bounded in $\omega$ for $z = e^{-i \omega}$,
 as the previous discussion guarantees. 
  
 
 For computation, it is convenient to calculate the integrand given in the
  right-hand expression of (\ref{eq:mdfa-criterion-nonstat}), which involves
   the pseudo-spectrum $ G \, {|\Delta (z) |}^{-2}$.  Numerical
    evaluation of this integrand will yield $0/0$ at $\omega$-dynamics such
  that $\Delta (e^{-i \omega}) = 0$, and L'Hopital's rule could be used
  to resolve this quotient into an expression for
  $\widetilde{\Lambda} (z)   \,  G \, 
  {\widetilde{\Lambda} (z) }^* \vert_{z= e^{-i \omega}}$.
    However, as there are only a finite number of such resolvable singularities,
   and such points constitute a set of   Lebesgue measure zero, 
   calculation of $D_{\Psi} (\vartheta, G)$ can proceed by 
   integrating over $\omega \in [-\pi, \pi]$ such that a neighborhood containing
    each singularity is omitted.  This procedure can yield an expression arbitrarily
     close to $D_{\Psi} (\vartheta, G)$ (by taking the neighborhoods sufficiently small),
    while allowing for easy calculation, since $\Lambda (z)$ and 
    $ G \, {|\Delta (z) |}^{-2}$ are easily computed.
    
A drawback is that for any finite mesh that is specified, the error generated
by omitting a neighborhood of the singularity can be substantial.  
    Alternatively, an exact approach can be used as follows.  Consider a 
    unit root $\zeta$ of $\Delta (z)$, and for simplicity suppose this is
    the only root.  Then $\Delta (z) = 1 - z/\zeta = -\zeta^{-1} (z - \zeta)$,
    and by Proposition \ref{prop:filter-decompose}
  \begin{align*}
   & \frac{\Psi (z) - \widehat{\Psi}_{\vartheta} (z) }{ z- \zeta} \\
   & =  \frac{ \Psi (\zeta) + (z - \zeta) \Psi^{\sharp} (z)  
   - \widehat{\Psi}_{\vartheta} (\zeta) - (z - \zeta) 
    \widehat{\Psi}^{\sharp}_{\vartheta} (z)  }{z- \zeta} \\
  & = \Psi^{\sharp} (z) - \widehat{\Psi}^{\sharp}_{\vartheta} (z), 
    \end{align*}
  where the last equality uses   (\ref{eq:non-stat.constraint.single}).
     Therefore, (\ref{eq:mdfa-criterion-nonstat}) becomes
     (noting that  ${|\zeta|}^2 =1$)
\[
 D_{\Psi} (\vartheta, G) =  { \langle  \left[ \Psi^{\sharp} (e^{-i \omega}) - \widehat{\Psi}^{\sharp}_{\vartheta} (e^{-i \omega}) \right] \,  
 G (\omega) \,  {  \left[ \Psi^{\sharp} (e^{i \omega}) - 
  \widehat{\Psi}^{\sharp}_{\vartheta} (e^{i \omega}) \right] }^{\prime} \rangle }_0.
\]
 Noting that $  \widehat{\Psi}^{\sharp}_{\vartheta} (z)$ is unconstrained,
 we can optimize the criterion using basic MDFA, where the input target filter
 is $\Psi^{\sharp} (z) = ( \Psi (z) - \Psi (\zeta))/(z - \zeta)$, which 
 can be directly computed; the value at $z = \zeta$ can be obtained by
 l'Hopital's rule, viz. 
\[
  \Psi^{\sharp} (\zeta) = \partial \Psi (\zeta) = \sum_{j \in \ZZ} \psi (j)
  j \zeta^{j-1}.
\]
 It can be checked that we get the same expression by 
 using the explicit formulas for $\psi^{\sharp} (j)$ in the proof of
  Proposition \ref{prop:filter-decompose}.
 Once the optimal MDFA filter 
 $  \widehat{\Psi}^{\sharp}_{\vartheta} (z)$ has been found, 
  the constrained MDFA filter can be computed via
\[
 \widehat{\Psi}_{\vartheta} (z) = \Psi (\zeta) + (z- \zeta) 
 \, \widehat{\Psi}^{\sharp}_{\vartheta} (z),
\]
 which again uses  (\ref{eq:non-stat.constraint.single}).
 It is straightforward to see that
 \[
  \widehat{\psi} (j) =  \Psi (\zeta) \, 1_{ \{ j = 0 \}} + 
   \widehat{\psi}^{\sharp} (j-1) \, 1_{ \{ j \geq 1 \}}
   - \zeta \, \widehat{\psi}^{\sharp} (j).
\]
 This can also be expressed in the format of (\ref{eq:concurrent-constrain}),
 where
\[
	R  = \left[ \begin{array}{cccc} 
	- \zeta & 0 &  \ldots &  0  \\
	1 & -\zeta  &  0  & \ldots  \\
		0 & 1 & -\zeta & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 
		0 & \ldots & 1 & - \zeta \\
		0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (\zeta) \\ 0
	\\ 0 \\ \vdots \\ 0 \end{array} \right].
\] 
 For double roots or multiple distinct roots in $\Delta (z)$, 
 the same principle can be applied in an iterative fashion.
  Whereas the theoretical filter error MSE is given by $D_{\Psi, F}$, 
  with $F$ being the spectral density of $\{ \partial X_t \}$,
 for estimation we approximate the integral over Fourier frequencies, 
 and utilize the periodogram of the differenced data for $G$.
% We omit any contributions to the sum arising from Fourier frequencies
% that correspond to zeros of $\Delta (z)$, as such an omission
% only results in a loss of order $T^{-1}$, which is of the same order
% as the Riemann sum approximation over Fourier frequencies to the exact integral.


\begin{Exercise} {\bf  Ideal Low-Pass Filter for a Random Walk.} \rm
\label{exer:rwtrend-mdfa}
This exercise considers the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a random walk process.
 Simulate a sample of size $T=5000$ from a
  bivariate random walk process with 
  $\Sigma$ equal to the identity.  
      Apply the   ideal low-pass filter (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  for an $I(1)$ process  to find the best
 concurrent filter, setting $q= 30$. 
   Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}
  

<<exercise_rwtrend-mdfa,echo=True>>=
# Simulate a Gaussian RW of sample size 5000:
set.seed(1234)
T.sim <- 5000
burn <- 1000
N <- 2
dpoly <- c(1,-1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,2))
d <- length(dpoly) - 1
z.sim <- mdfa.wnsim(rep(0,3),rep(1,N),T.sim+burn,Inf)
Sigma <- z.sim[[2]]
x.sim <- mdfa.ucsim(delta,z.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
   
# construct and apply ideal low-pass filter
mu <- pi/6
len <- 1000
lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
lp.filter <- c(rev(lp.filter),lp.filter[-1])
x.trend.ideal <- mvar.filter(x.sim,array(t(lp.filter) %x% diag(N),c(N,N,(2*len+1))))

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
grid <- T.sim - d
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))

# old way
#frf.psi <- rep(0,grid)
#frf.psi[abs(freq.ft) <= mu] <- 1
#frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
#frf.psi <- array(frf.psi,c(N,N,grid))
#constraints.mdfa <- mdfa.getconstraints(frf.psi,0,NULL,0*diag(N),q)
#spec.hat <- mdfa.pergram(x.diff,dpoly)
#bw.mdfa <- mdfa.filter(frf.psi,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
#x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(len-q+2):(T-q+1-len),]
 
# frf for ideal low-pass
frf.psi <- rep(0,grid)
frf.psi[abs(freq.ft) <= mu] <- 1
frf.psi.sharp <- (frf.psi - frf.psi[m+1])/(exp(-1i*freq.ft)-1)
frf.psi.sharp[m+1] <- 0
frf.psi.sharp <- matrix(frf.psi.sharp,nrow=1) %x% diag(N) 	  
frf.psi.sharp <- array(frf.psi.sharp,c(N,N,grid))
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,grid))
spec.hat.sharp <- mdfa.pergram(x.diff,1)
bw.mdfa.sharp <- mdfa.unconstrained(frf.psi.sharp,spec.hat.sharp,q-1)
R.mat <- toeplitz(c(-1,1,rep(0,q-2)))
R.mat[upper.tri(R.mat)] <- 0
R.mat <- R.mat[,-q] %x% diag(N)
Q.mat <- c(1,rep(0,q-1)) %x% frf.psi[,,m+1]
bw.mdfa.filter <- array(t(R.mat %*% t(matrix(bw.mdfa.sharp[[1]],nrow=N)) + Q.mat),c(N,N,q))
x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa.filter)[(len-q+2):(T-q+1-len),]

# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(bw.mdfa.sharp[[2]])
@
 


\begin{Exercise} {\bf Ideal Band-Pass Filter for an $I(1)$ Process.} \rm
\label{exer:bandpass.i1-mdfa}
This exercise considers   an ideal band-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to an $I(1)$ process that exhibits cyclical dynamics.
Consider a non-stationary bivariate  process such that the first differences
 are a stationary VAR(1) with matrix
\[
    \left[ \begin{array}{ll} .942 & -.124 \\ .124 & .942 \end{array} \right].
\]
  Simulate a sample of size $T=5000$ from a
 such a process with innovation variance matrix
  $\Sigma$ equal to the identity.  
      Apply the   ideal band-pass filter (cf. Example \ref{exam:ideal-bp}) with 
  $\mu = \pi/60$ and $\eta = \pi/12$ 
    to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  for an $I(1)$ process  to find the best
 concurrent filter, setting $q= 10$. 
   Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal cycle.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the cycles for both time series.
 Hint: now trend is considered noise, and the stationary cycle is the signal.
\end{Exercise}


<<exercise_bandpass.i1-mdfa,echo=True>>=
# Simulate an integrated VAR(1) of sample size 5000:
set.seed(1234)
T.sim <- 5000
burn <- 1000
N <- 2
rho <- .95
theta <- pi/24
phi <- matrix(c(rho*cos(theta),rho*sin(theta),-rho*sin(theta),rho*cos(theta)),c(2,2))
phi.array <- array(cbind(diag(N),-1*phi),c(N,N,2))
dpoly <- c(1,-1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,2))
d <- length(dpoly) - 1
z.sim <- mdfa.wnsim(rep(0,3),rep(1,N),T.sim+burn,Inf)
Sigma <- z.sim[[2]]
var.sim <- mdfa.ucsim(phi.array,z.sim[[1]])
x.sim <- mdfa.ucsim(delta,var.sim)[(burn+1-d-2):(T.sim+burn-d-2),]
   
# construct and apply ideal band-pass filter
mu <- pi/60
eta <- pi/12
len <- 1000
bp.filter <- c(eta/pi,sin(seq(1,len)*eta)/(pi*seq(1,len))) - 
  c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
bp.filter <- c(rev(bp.filter),bp.filter[-1])
x.cycle.ideal <- filter(x.sim,bp.filter,method="convolution",sides=2)[(len+1):(T-len),]

# get MDFA concurrent filter
q <- 10
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
grid <- T.sim - d
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))

# old way
#frf.psi <- rep(0,grid)
#frf.psi[(abs(freq.ft) >= mu) & (abs(freq.ft) <= eta)] <- 1
#frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
#frf.psi <- array(frf.psi,c(N,N,grid))
#constraints.mdfa <- mdfa.getconstraints(frf.psi,NULL,0,0*diag(N),q)
#spec.hat <- mdfa.pergram(x.diff,dpoly)
#bp.mdfa <- mdfa.filter(frf.psi,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
#x.cycle.mdfa <- mvar.filter(x.sim,bp.mdfa[[1]])[(len-q+2):(T-q+1-len),]

# frf for ideal band-pass
frf.psi <- rep(0,grid)
frf.psi[(abs(freq.ft) >= mu) & (abs(freq.ft) <= eta)] <- 1
frf.psi.sharp <- (frf.psi - frf.psi[m+1])/(exp(-1i*freq.ft)-1)
frf.psi.sharp[m+1] <- 0
frf.psi.sharp <- matrix(frf.psi.sharp,nrow=1) %x% diag(N) 	  
frf.psi.sharp <- array(frf.psi.sharp,c(N,N,grid))
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,grid))
spec.hat.sharp <- mdfa.pergram(x.diff,1)
bp.mdfa.sharp <- mdfa.unconstrained(frf.psi.sharp,spec.hat.sharp,q-1)
R.mat <- toeplitz(c(-1,1,rep(0,q-2)))
R.mat[upper.tri(R.mat)] <- 0
R.mat <- R.mat[,-q] %x% diag(N)
Q.mat <- c(1,rep(0,q-1)) %x% frf.psi[,,m+1]
bp.mdfa.filter <- array(t(R.mat %*% t(matrix(bp.mdfa.sharp[[1]],nrow=N)) + Q.mat),c(N,N,q))
x.cycle.mdfa <- mvar.filter(x.sim,bp.mdfa.filter)[(len-q+2):(T-q+1-len),]

# compare in-sample performance
print(c(mean((x.cycle.ideal[,1] - x.cycle.mdfa[,1])^2),
	mean((x.cycle.ideal[,2] - x.cycle.mdfa[,2])^2)))

# compare to criterion value
diag(bp.mdfa.sharp[[2]])
@



\begin{Exercise} {\bf  Hodrick-Prescott Filter for an $I(2)$ Process.} \rm
\label{exer:hptrend-mdfa}
This exercise takes as target the HP low-pass filter of Example
 \ref{exam:hp-low} applied to a STM process (Example \ref{exam:trend-i2}).
 Simulate a sample of size $T=5000$ from an STM process with parameters
 given in Exercise \ref{exer:trend-i2}.   Apply the HP low-pass filter
 using formula (\ref{eq:hp.mvar-def}) with $ Q = (1/1600) \, 1_2 $
%\[
 % Q = 10^{-2} \, \left[ \begin{array}{ll}  2.46  & 0.23 \\
 %  4.55 & 0.61  \end{array} \right]
%\]
(truncate the filter to $1000$ coefficients on each side).
 Use the moving average filter  MDFA  for an $I(2)$ process  to find the best
 concurrent filter, setting $q= 30$.
   Apply this concurrent filter
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}


<<exercise_hptrend-mdfa,echo=True>>=
# Simulate a Gaussian STM  of sample size 5000:
set.seed(1234)
T.sim <- 5000
burn <- 1000
N <- 2
psi.sim <- c(1.8905590615422, -11.9288577633298, -12.0809347541079, 
             0.660897814610799, -8.2863379601304, -5.66645335346871, 
             -1.34743227511595e-05, -1.41207967213544e-05)
psi.sim[7:8] <- c(0,0)
len <- 1000
dpoly <- c(1,-2,1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,3))
d <- length(dpoly) - 1
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(delta,mu.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + irr.sim
#Q.snr <- Sigma.mu %*% solve(Sigma.irr)
Q.snr <- (1/1600) * diag(N)
 
grid <- T.sim - d
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
# frf for HP low-pass
frf.psi <- array(0,c(N,N,grid))
for(i in 1:grid)
{
  frf.psi[,,i] <- Q.snr %*% solve(Q.snr + (2 - 2*cos(freq.ft[i]))^2*diag(N))
}
# construct and apply low-pass HP filter
hp.filter <- mdfa.coeff(frf.psi,-len,len)
x.trend.hp <- mvar.filter(x.sim,hp.filter)

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]

# frf calculations
frf.psi.sharp <- array(0,c(N,N,grid))
for(i in 1:grid)
{
  frf.psi.sharp[,,i] <- -1*(1-exp(1i*freq.ft[i]))^2*
    solve(Q.snr + (2 - 2*cos(freq.ft[i]))^2*diag(N))
}  
spec.hat.sharp <- mdfa.pergram(x.diff,1)
hp.mdfa.sharp <- mdfa.unconstrained(frf.psi.sharp,spec.hat.sharp,q-2)
R.mat <- toeplitz(c(rev(dpoly),rep(0,q-3)))
R.mat[upper.tri(R.mat)] <- 0
R.mat <- R.mat[,-c(q-1,q)] %x% diag(N)
Q.mat <- c(1,rep(0,q-1)) %x% frf.psi[,,m+1]
hp.mdfa.filter <- array(t(R.mat %*% t(matrix(hp.mdfa.sharp[[1]],nrow=N)) + Q.mat),c(N,N,q))
x.trend.mdfa <- mvar.filter(x.sim,hp.mdfa.filter)[(len-q+2):(T-q+1-len),]

# compare in-sample performance
print(c(mean((x.trend.hp[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.hp[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(hp.mdfa.sharp[[2]])
@
 
In Exercise \ref{exer:hptrend-mdfa} we are able to find a remainder polynomial
 $\Psi^{\star} (z)$ such that
\begin{equation}
\label{eq:psi-to-sharp-and-star}
 \Psi (z) = \Delta (z) {\delta (d)}^{-1} \Psi^{\sharp} (z) + \Psi^{\star} (z),
 \end{equation}
 where  $\delta (d) $ is the 
 leading coefficient of $\Delta (z)$; in this case, $\Psi^{\star} (z) = 1_n$
 and 
 \[
 \Psi^{\sharp}(z) = - {(1-z^{-1})}^2 \, {\left( Q + {|1 - z|}^4 \right)}^{-1}.
 \]
 More generally, we seek a degree $d-1$ polynomial $\Psi^{\star} (z)$ such that
\[
 \Psi^{\sharp} (z) = \frac{ \delta (d) \, ( \Psi (z) - \Psi^{\star} (z) )
 }{ \Delta (z)}
\]
is bounded for all $z$ on the unit circle, as this will facilitate calculation
of the MDFA criterion.  Suppose the differencing operator has $r$
distinct roots $\zeta_1, \ldots, \zeta_r$
with multiplicities $m_1, \ldots, m_r$, i.e.,
$\Delta (z) = \prod_{\ell=1}^r {(1 - z/\zeta_{\ell})}^{m_{\ell}}$.
 Then the leading coefficient is $\delta (d)  =
  \prod_{\ell=1}^r {(-\zeta_{\ell})}^{m_{\ell}}$, which is real with
  unit magnitude.  The desired $\Psi^{\star} (z)$ must satisfy
  (\ref{eq:psi-to-sharp-and-star}), and it follows from 
  straightforward calculus that $\partial^k \Psi (\zeta_j) =
   \partial^k \Psi^{\star} (\zeta_j)$ for $0 \leq k  < m_j$ and
   $1 \leq j \leq r$.  These necessary constraints are also
   sufficient to define $\Psi^{\star} (z)$, whose coefficients
  are obtained by solving the linear system
\[
 \left[ \begin{array}{c} \Psi (\zeta_1) \\ \partial \Psi (\zeta_1) \\
  \vdots \\ \partial^{m_1-1} \Psi (\zeta_1) \\
  \Psi (\zeta_2) \\ \partial \Psi (\zeta_2) \\
  \vdots \\ \partial^{m_2-1} \Psi (\zeta_2) \\
  \vdots \\ \Psi (\zeta_r) \\ \partial \Psi (\zeta_r) \\
  \vdots \\ \partial^{m_r-1} \Psi (\zeta_r) \end{array} \right]
 = \left[ \begin{array}{ccccc}
  1 & \zeta_1 & \ldots & \ldots & \zeta_1^{d-1} \\
  0 &  1 & 2 \zeta_1 & \ldots & (d-1) \zeta_1^{d-2} \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  0 & \ldots & \ldots & \ldots &  \binom \zeta_1^{d-m_1} \\

  

\right] 
 
\] 

  HERE  finish pages 1-3 of notes
   
 

 



\section{Replication and Efficiency Gain Over Model-Based Filters}

In this final section of the chapter, we proceed through several examples
 in some detail.  We consider the main examples from the second section of the
  chapter, where the parameters are based upon models fitted to real series.
  First we show that MDFA can replicate the optimal model-based concurrent filters
  when the model is correctly specified, and secondly that it out-performs
  when the model is incorrectly specified.
  Finally, we show the actual performance of MDFA on each of the real series.
   
  
In order to understand the topic of model-based replication, we proceed
to determine the optimal concurrent filter for the LPP determined by a model-based
target.  This filter is similar to the Wiener-Kolmogorov (WK) filter, and is sometimes
called the Wiener-Hopf (WH) filter because it is related to the Wiener-Hopf 
 factorization problem.
When the model is correctly specified, the best concurrent filter 
 approximation to the symmetric model-based filter (i.e., the WK filter) given by
 (\ref{eq:wk.frf-gen}) is the causal filter that extracts the signal $S_t$ from
  present and past data $X_t, X_{t-1}, \ldots$.  Alternatively, the formula
   for the LPP solution can be used.  This optimal concurrent filter can be
   compactly expressed in terms of the spectral factorizations of the 
      pseudo-spectral densities $f_S$ and $f_X$.
    Spectral factorization is defined as follows: 
    let $\Theta (z)$ be a matrix power series such that
 \begin{equation}
 \label{eq:spectral-factor}
  f_{\partial X} (\omega) = \Theta (e^{-i \omega}) \, \Sigma \, 
    {\Theta (e^{i \omega})}^{\prime}.
 \end{equation}
 This decomposition is known as a spectral factorization, and can be computed
 with known algorithms, as discussed in McElroy (2018).
  Corresponding to this decomposition is the Wold representation for $\{ \partial X_t \}$,
   i.e., $\Delta (L) X_t = \Theta (L) \epsilon_t$,  
   where $\{ \epsilon_t \}$ is a multivariate 
   white noise sequence with covariance matrix $\Sigma$. 
  Let $\gamma_{\partial S} (z)$ denote the autocovariance generating function
  of $\{ \partial S_t \}$, so that 
   $\gamma_{\partial S} (e^{-i \omega}) = f_{\partial S} (\omega)$.  
    Then by adapting the results of Bell and Martin (2004) to the multivariate case, 
    when $\Theta (z)$ is invertible 
    the optimal concurrent filter  expressed as a power series is 
\begin{equation}
\label{eq:wiener-hopf}
  \widehat{\Psi} (z) =  
    { \left[  \gamma_{\partial S} (z) \,  \Delta^N (z^{-1}) \, {\Delta^S (z)}^{-1}
     { \Theta (z^{-1})}^{-1 \prime} \right] }_0^{\infty} \, \Sigma^{-1} \,
     { \Theta (z) }^{-1} \Delta (z). 
\end{equation}
  It is clear that (\ref{eq:wiener-hopf}) is a causal filter,
  as it only involves non-negative powers of $z$.
 Recall that our notion of optimality is the minimization of the determinant
 of the mean square error matrix.

 
%\begin{equation}
% \label{eq:opt.wk}
% \Psi (z) = \Theta^s (z) \, \Sigma_s \, { \Theta^s (z^{-1}) }^{\prime}  \, 
%    { \Theta^x (z^{-1}) }^{- \prime} \, \Sigma_x^{-1} \, { \Theta^x (z) }^{-1}.
%\end{equation}

%\begin{equation}
% \label{eq:opt.conc.sigex}
%  \widehat{\Psi} (z) =   { [  \Theta^s (z) \, \Sigma_s \, { \Theta^s (z^{-1}) }^{\prime}  \, 
%    { \Theta^x (z^{-1}) }^{- \prime}  ]}_0^{\infty}  \, \Sigma_x^{-1} \, 
%  { \Theta^x (z) }^{-1}.
%\end{equation}
 
 
 
\begin{Theorem}
\label{thm:asym.sigex}
 Suppose that $\{ X_t \}$ is  a non-stationary stochastic process
 with representation (\ref{eq:nonstatCausalRep}), such that
 (\ref{eq:signal-noise-decomp}) and (\ref{eq:spectral-factor}) hold,
  and $\Theta (z)$ is invertible.
 If the signal and noise differencing operators
 $\Delta^S (L)$ and $\Delta^N (L)$ are relatively prime, and Assumption A holds,
 then  the  optimal linear estimator $\widehat{S}_t$   of $S_t$ given data 
  $\{ X_{t-j}, j \geq 0 \}$ is   given by
 applying the filter $\widehat{\Psi} $  to $\{  X_t \}$, 
 with $\widehat{\Psi} (z)$  given by (\ref{eq:wiener-hopf}). 
 Moreover, this concurrent filter is identical with
 the LPP solution of Proposition \ref{prop:GPP2}, where the target 
 $\Psi (z)$ is the WK filter with frf (\ref{eq:wk.frf-gen}).
 Therefore (\ref{eq:wiener-hopf}) can be re-expressed as
\begin{equation}
\label{eq:optimal-conc-comp}
 \sum_{\ell = -\infty}^{\infty}  \psi  (\ell)  \,  z^{\ell}
 \, \left( 1 -  { [  \Phi (z) ]}_0^{-\ell-1} \, { \Phi (z) }^{-1} \right),
\end{equation}
 where   $\Phi(z) = \Theta (z) / \Delta (z)$, and ${ [  \Phi (z) ]}_0^{-\ell-1} = 0$
 when $\ell \geq 0$.
\end{Theorem}
 

\paragraph{Proof of Theorem \ref{thm:asym.sigex}.}
  The filter error is $\varepsilon_t = S_t - \widehat{S}_t$, and it suffices to 
  show that this is uncorrelated with $X_{t-h}$ for each $h \geq 0$.
  The filter error can be expressed as
\[
 \varepsilon_t =  ( S_t  - \Psi (L) X_t) +  (\Psi (L) - \widehat{\Psi} (L)) X_t.
\]
  The first term on the right hand side is the WK filter error, which
   has already been shown to be uncorrelated with $\{ X_t \}$ by
   Theorem \ref{thm:wk}.  For the second term, the difference in the two filters' frfs is
\[
  -{ \left[  \gamma_{\partial S} (z) \,  \Delta^N (z^{-1}) \, {\Delta^S (z)}^{-1}
     { \Theta (z^{-1})}^{-1 \prime} \right] }_{-\infty}^{-1} \, \Sigma^{-1} \,
     { \Theta (z) }^{-1} \Delta (z).
\]
   This  frf corresponds to a filter of the form $\Phi (L)  { \Theta (L) }^{-1} \Delta (L)$,
   where $\Phi (e^{-i \omega})$ is a function of bounded modulus.
   Furthermore,  the Laurent series expansion of $\Phi (z)$ only involves negative powers 
   of $z$, which means that the filter $\Phi (L)$ only involves negative powers of $L$.
   Such a filter is purely anti-causal, meaning that the output of such a filter only
   depends on future values.  
   Therefore, its application to $\{ X_t \}$ yields
\[
 (\Psi (L) - \widehat{\Psi} (L)) X_t = \Phi (L)  \epsilon_t,
\]
  and $\Phi (L) \epsilon_t$ is a linear combination of
  $\epsilon_{t+1}, \epsilon_{t+2}, \ldots$.
  Hence, this filter error is uncorrelated with $X_{t-h}$ for all $h \geq 0$, 
  and the first assertion of the Theorem is proved.
    It can be shown that the concurrent signal filter, when added
  to the noise filter, yields unity:
\begin{align*}
  \widehat{\Psi} (z) & = 
   { \left[  \gamma_{\partial S} (z) \,  \Delta^N (z) \Delta^N (z^{-1}) 
   \, {\Delta (z)}^{-1} 
     { \Theta (z^{-1})}^{-1 \prime} \right] }_0^{\infty} \, \Sigma^{-1} \,
     { \Theta (z) }^{-1} \Delta (z)   \\
    & =  { \left[  \left(  \gamma_{\partial X} (z) -
    \gamma_{\partial N} (z) \,  \Delta^S (z) \Delta^S (z^{-1}) \right)
   \, {\Delta (z)}^{-1} 
     { \Theta (z^{-1})}^{-1 \prime} \right] }_0^{\infty} \, \Sigma^{-1} \,
     { \Theta (z) }^{-1} \Delta (z)   \\
& = { \left[  \Theta (z) \, \Sigma \, {\Delta (z)}^{-1} \right] }_0^{\infty}
 \, \Sigma^{-1} \, { \Theta (z) }^{-1} \Delta (z) -
    { \left[ \gamma_{\partial N} (z) \,  \Delta^S (z^{-1}) \, {\Delta^N (z)}^{-1} 
     { \Theta (z^{-1})}^{-1 \prime} \right] }_0^{\infty} \, \Sigma^{-1} \,
     { \Theta (z) }^{-1} \Delta (z).
\end{align*}
  The second term is the concurrent noise filter.  In the first term,
  the expressions within the brackets are all power series, and hence
  the bracket can be discarded, i.e.,
\[
 { \left[  \Theta (z) \, \Sigma \, {\Delta (z)}^{-1} \right] }_0^{\infty}
 \, \Sigma^{-1} \, { \Theta (z) }^{-1} \Delta (z)
 =  \Theta (z) \, \Sigma \, {\Delta (z)}^{-1} 
 \, \Sigma^{-1} \, { \Theta (z) }^{-1} \Delta (z) = 1_n.
\]
To prove the second assertion, note that 
the LPP solution given by (\ref{eq:GPPsoln-nonstat}) can be written
\[
   \sum_{\ell = -\infty}^{\infty}  \psi  (\ell) \, z^{\ell} -  
 \sum_{\ell =-1}^{-\infty}  \psi  (\ell)  \,    
 { [  \Phi (z) ]}_0^{-\ell-1}     \, z^{\ell} \, { \Phi (z) }^{-1}.
\]
  The first summand is $\Psi (z)$.    Noting that
\[
    { [  \Phi (z) ]}_0^{-\ell-1}  =  \sum_{k=0}^{-\ell-1}  \phi (k) \, z^k =
   \sum_{k \geq 0}  \phi (-\ell-1-k) \, z^{-\ell-1-k},
\]
for the second summand we have
\begin{align*}
&  \sum_{\ell = -1}^{-\infty}  \psi  (\ell)   \, { [  \Phi (z) ]}_0^{-\ell-1} 
\, z^{\ell}   \\
& = \sum_{\ell = -1}^{-\infty}   \psi (\ell)   \,   \sum_{k = 0}^{\infty} 
  \phi^{x}  (-\ell-1-k) \, z^{-1-k} \\
 & =  \sum_{k = 0}^{\infty}    z^{-1-k} \, \sum_{\ell = -1}^{-\infty} \psi (\ell) \, 
   \phi  (-\ell-1-k)   \\
  & =  \sum_{k = 0}^{\infty}    z^{-1-k} \, \sum_{\ell = -1}^{-\infty} 
  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  \Psi (e^{-i \omega}) \, e^{i \omega \ell} \, d\omega 
\,  \phi  (-\ell-1-k).
\end{align*}
Substituting the expression for the frf of the WK filter yields
\begin{align*}
 & =  \sum_{k = 0}^{\infty}    z^{-1-k} \,  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  
 f_{ \partial S}(\omega)   \,  { f_{\partial X} (\omega)}^{-1} \, 
  {| \Delta^N (e^{-i \omega}) |}^2 \, 
   \sum_{\ell = -1}^{-\infty}  \phi (-\ell-1-k)   \, e^{i \omega \ell} \, d\omega   \\
 & =  \sum_{k = 0}^{\infty}   z^{-1-k} \,  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  
f_{ \partial S}(\omega)   \,  { f_{\partial X} (\omega)}^{-1} \, 
  {| \Delta^N (e^{-i \omega}) |}^2 
\,  \sum_{h = 0}^{\infty}   \phi (h-k)   \, e^{-i \omega (h+1)} \, d\omega  \\
 & =   \sum_{k = 0}^{\infty}   z^{-1-k} \,  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  
f_{ \partial S}(\omega)   \,  {| \Delta^N (e^{-i \omega}) |}^2 \, 
     {  \Theta (e^{i \omega}) }^{- \prime} \, \Sigma^{-1} \,
    {  \Theta (e^{-i \omega}) }^{-1}
\,   \Phi (e^{-i \omega}) \, e^{-i \omega (k+1) }  \, d\omega  \\
 & =   \sum_{k = 0}^{\infty}   z^{-1-k} \,  \frac{1}{2 \pi} \int_{-\pi}^{\pi}  
f_{ \partial S}(\omega) \,  { \Theta (e^{i \omega}) }^{- \prime} \, \Sigma^{-1} \,
  \Delta^N (e^{i \omega})/ \Delta^S (e^{-i \omega})
\,  e^{-i \omega (k+1) }  \, d\omega  \\
 & =   \sum_{k = 0}^{\infty}   z^{-1-k} \,  { \langle    f_{ \partial S}(\omega) \, 
    {  \Theta (e^{i \omega}) }^{-1 \prime} 
    \Delta^N (e^{i \omega})/ \Delta^S (e^{-i \omega})  \rangle  }_{-(k+1) } 
     \, \Sigma^{-1}  \\
 & =   { [  \gamma_{\partial S} (z)  \, 
    {  \Theta (z^{-1}) }^{-1 \prime} 
    \Delta^N (z^{-1})/ \Delta^S (z) ]}_{-\infty}^{-1}  \, \Sigma^{-1}.
\end{align*}
 Hence, the LPP solution is
\begin{align*}
  & \Psi (z) - { [  \gamma_{\partial S} (z)  \, 
    {  \Theta (z^{-1}) }^{-1 \prime} \Delta^N (z^{-1})/ \Delta^S (z)
    ]}_{-\infty}^{-1}  \, \Sigma^{-1} \,     { \Phi (z) }^{-1}  \\
  & =  \Psi (z) -  \gamma_{\partial S} (z)  \, 
    {  \Theta (z^{-1}) }^{-1 \prime}  \, \Delta^N (z^{-1})/ \Delta^S (z) \,
     \Sigma^{-1} \,
    { \Theta (z) }^{-1} \Delta (z)  \\
 &    +  { [  \gamma_{\partial S} (z)  \, 
    {  \Theta (z^{-1}) }^{-1 \prime}  \Delta^N (z^{-1})/ \Delta^S (z)
     ]}_{0}^{\infty}  \, \Sigma^{-1} \,
    { \Theta (z) }^{-1} \Delta (z),
\end{align*}
  which equals $\widehat{\Psi} (z)$.
  This verifies that the LPP solution indeed equals the optimal concurrent
  filter when $\Psi $ is the WK filter.
$\quad \Box$

\vspace{.5cm}

 It is   clear how to compute (\ref{eq:wiener-hopf}), using the formula
 (\ref{eq:optimal-conc-comp}):
 the coefficients $\phi (k)$ of $\Phi (z)$ are given by
\[
 \phi (k) = \sum_{j=0}^k \theta (k-j) \, \xi (j)
\]
 for $k \geq 0$  (and $\phi (k) = 0 $ if $k < 0$), and 
 ${ [  \Phi (z) ]}_0^{\ell-1} = \sum_{k=0}^{\ell-1} \phi (k) z^k$ for $\ell \geq 1$.
  Noting that
\begin{align*}
  \sum_{j=0}^d \phi (k-j) \, \delta (j) & =
   \sum_{j=0}^d { \langle z^{j-k} \Theta (z) \Xi (z) \rangle}_0 \, \delta (j) \\
  & =  { \langle  z^{-k} \Theta (z) \Xi (z)  \, \sum_{j=0}^d \delta (j) z^j \rangle} \\
 &   = { \langle \Theta (z) \rangle}_k = \theta (k),
\end{align*}
 we have the recursion
\[
  \phi (k) = - \sum_{j=1}^d \phi (k-j) \, \delta (j) \, { \delta (0)}^{-1} 
    + \theta (k) \, { \delta (0)}^{-1}.
\]
 Furthermore, it follows from the proof of Theorem \ref{thm:asym.sigex} that the functions
\[
  z^{\ell}  \, \left( 1_n -  { [  \Phi (z) ]}_0^{-\ell-1} \, { \Phi (z) }^{-1} \right)
\]
are causal for all $\ell \in \ZZ$.  Then we can re-express the optimal concurrent filter as
\[
 \widehat{\Psi} (z) = \psi (0) + \sum_{\ell=1}^{\infty} 
 \left\{ \psi (\ell) z^{\ell} + \psi (-\ell) z^{-\ell} 
 \left( 1_n -  { [  \Phi (z) ]}_0^{-\ell-1} \, { \Phi (z) }^{-1} \right) \right\},
\]
 which is most convenient for computation.

\subsection{Industrial Petroleum: the LLM Specification}


<<echo=False>>=
# visualize
load("Sweave/Data/petrol.RData")
period <- 12
petrol <- ts(petrol[,c(1,2)],start=c(1973,1),frequency=period,
             names=c("Consumption","Imports")) 
file <- paste("petrol_data.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
plot(petrol,xlab="Year")
invisible(dev.off())
@
  
\begin{figure}[htb!]
\begin{center}
\includegraphics[]{petrol_data.pdf}
\caption{Plot of monthly  Industrial Petroleum Consumption and OPEC Oil Imports, 
  January 1973 through December 2016 (seasonally adjusted). }
\label{fig:petroldata}
\end{center}
\end{figure}  

\begin{Example} {\bf Petrol MB Replication.} \rm
\label{exam:petrol-mb}
We now  study the  MB trend filter arising from a LLM  
(cf. Example \ref{exam:trend-i1}) fitted
 to a   bivariate petroleum series:
  Industrial Petroleum Consumption and OPEC Oil Imports, Jan 1973 through December 2016,
 both seasonally adjusted  (displayed in Figure \ref{fig:petroldata}).
   The MLEs for the model parameters are  given by
\[
 \Sigma_{S} = \left[ \begin{array}{ll} 
   2.32 \cdot 10^{-4} &  5.04 \cdot 10^{-4} \\
   5.04 \cdot 10^{-4}  & 34.73 \cdot 10^{-4}  \end{array}  \right]
 \qquad  \Sigma_{N} = \left[ \begin{array}{ll}
        110.44 \cdot 10^{-5} &  7.17 \cdot 10^{-5}  \\
        7.17 \cdot 10^{-5} & 128.57 \cdot 10^{-5}   \end{array} \right].
\]
  Because the trend variance for the second component is $15$ times 
  larger than that of the first component,
 the corresponding trend filter  does less smoothing. 
 From this fitted model,  we can construct both the WK filter -- recall Exercise
  \ref{exer:trend-i1}, and the plot of the WK filter's frf in Figure \ref{fig:llm-frf} --
  and the optimal concurrent (or WH) filter via (\ref{eq:wiener-hopf}).
\end{Example}

\begin{Exercise} {\bf Petrol MB Replication.}  \rm
\label{exer:petrol-mb1}
 The purpose of this exercise is to demonstrate that MDFA can replicate
 a MB concurrent filter, and even have superior performance when the 
 model is mis-specified. 
 Simulate a sample of size $T=5000$ from an LLM process with parameters
 given in Example \ref{exam:petrol-mb}.   Apply the WK trend filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent trend filter.  
 Use the moving average filter  MDFA  for an $I(1)$ process  to find the best
 concurrent filter, setting $q= 30$.
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}


<<exercise_llmpetrol-mdfa-null,echo=True>>=
# Simulate a Gaussian LLM  of sample size 5000:
set.seed(1234)
T.sim <- 5000
burn <- 1000
N <- 2
psi.sim <- c(2.17150287559847, -8.36795922528, -6.04133725367594, 
             0.0648981656699, -6.80849700177184, -6.66004335288479, 
             -0.00016098322952, 0.00051984185863)
psi.sim[7:8] <- c(0,0)
len <- 1000
dpoly <- c(1,-1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,2))
d <- length(dpoly) - 1
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(delta,mu.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + irr.sim

# construct and apply MB WK and WH filters
grid <- T.sim - d
iden <- array(diag(N),c(N,N,1))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.irr <- mdfa.spectra(iden,iden,Sigma.irr,grid)
trend.wkfrf <- mdfa.wkfrf(iden,delta,f.irr,f.mu)
trend.wkfilter <- mdfa.coeff(trend.wkfrf,-len,len)
x.trend.ideal <- mvar.filter(x.sim,trend.wkfilter)
trend.whfrf <- mdfa.whfrf(iden,delta,f.irr,f.mu,len)
trend.whfilter <- mdfa.coeff(trend.whfrf,-len,len)
x.trend.conc <- mvar.filter(x.sim,trend.whfilter)

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,0,NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(len-q+2):(T.sim-q+1-len),]

# compare in-sample performance
perf_null <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_null) <- c("MDFA","MB")
rownames(perf_null) <- c("Series 1","Series 2")
# compare to criterion value
print(perf_null)
diag(bw.mdfa[[2]])
@


\begin{Exercise} {\bf Petrol MB Out-Performance.}  \rm
\label{exer:petrol-mb2}
 Now we alter the specification of the LLM for the Petrol series,
  to illustrate that MDFA can out-perform the MB concurrent filter.  
  We do this by substantially increasing the
 variability in the irregular, producing a noisier simulation -- now the MB concurrent will do too little smoothing.  Consider the same MLE for the trend covariance
  matrix as in Exercise \ref{exam:petrol-mb}, but now set the irregular
 covariance matrix to be
\[
   \qquad  \Sigma_{N} = \left[ \begin{array}{ll}
       18.32 \cdot 10^{-3}  &  1.19 \cdot 10^{-3}   \\
       1.19 \cdot 10^{-3}  &  18.39 \cdot 10^{-3}  \end{array} \right].
\]
 Simulate a sample of size $T=5000$ from an LLM process with these parameters,
 but apply  the WK trend filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent trend filter  corresponding to the parameters
  given in Example \ref{exam:petrol-mb}.  
 Use the moving average filter  MDFA  for an $I(1)$ process  to find the best
 concurrent filter, setting $q= 30$.
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}

<<exercise_llmpetrol-mdfa-alt,echo=True>>=
# Simulate a Gaussian LLM  of sample size 5000:
set.seed(1234)
T.sim <- 5000
burn <- 1000
N <- 2
psi.sim <- c(2.17150287559847, -8.36795922528, -6.04133725367594, 
             0.0648981656699, -6.80849700177184, -6.66004335288479, 
             -0.00016098322952, 0.00051984185863)
psi.sim[5:6] <- c(-4,-4)
psi.sim[7:8] <- c(0,0)
len <- 1000
dpoly <- c(1,-1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,2))
d <- length(dpoly) - 1
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(delta,mu.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + irr.sim

# construct and apply MB WK and WH filters (from previous exercise)
grid <- T.sim - d
x.trend.ideal <- mvar.filter(x.sim,trend.wkfilter)
x.trend.conc <- mvar.filter(x.sim,trend.whfilter)

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,0,NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(len-q+2):(T.sim-q+1-len),]

# compare in-sample performance
perf_alt <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_alt) <- c("MDFA","MB")
rownames(perf_alt) <- c("Series 1","Series 2")
# compare to criterion value
print(perf_alt)
diag(bw.mdfa[[2]])
@




\begin{Exercise} {\bf Petrol MB Empirical Performance.}  \rm
\label{exer:petrol-mb3}
 We now examine the performance of MDFA   on the Petroleum data itself,
 recognizing that the LLM might be mis-specified.  
 Use the target filter resulting from the MLEs of the fitted LLM,
  from Example \ref{exam:petrol-mb}, truncating five years of observations at either end
   of the series.  
 Apply the  MDFA  for an $I(1)$ process  to find the best
 concurrent filter, setting $q= 30$.
  Apply both concurrent filters (MB and MDFA)
 to the data, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}
    
    
<<exercise_llmpetrol-mdfa-data,echo=True>>=
T <- dim(petrol)[1]
N <- dim(petrol)[2]
# truncation of ideal filter
trunc <- 5

# construct and apply MB WK and WH filters (from previous exercise)
grid <- T - d
x.trend.ideal <- mvar.filter(petrol,trend.wkfilter[,,(len-trunc*period):(len+trunc*period)])
x.trend.conc <- mvar.filter(petrol,trend.whfilter[,,(len-trunc*period):(len+trunc*period)])

# get MDFA concurrent filter
q <- 30
x.diff <- filter(petrol,dpoly,method="convolution",sides=1)[(d+1):T,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
trend.wkfrf <- mdfa.frf(trend.wkfilter,len,grid)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,0,NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(petrol,bw.mdfa[[1]])[(1+trunc*period-q):(T-q-trunc*period),]

# compare in-sample performance
perf_emp <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_emp) <- c("MDFA","MB")
rownames(perf_emp) <- colnames(petrol)
# compare to criterion value
print(perf_emp)
diag(bw.mdfa[[2]])
@
    
 The results of these Exercises   indicate that MDFA can both replicate model-based real-time
  filters (Exercise \ref{exer:petrol-mb1}) and out-perform them when
   the model is mis-specified (Exercises \ref{exer:petrol-mb2} and 
 \ref{exer:petrol-mb3}). Table \ref{tab:petrol.mat} summarizes the performances,
  where ``LLM null process" refers to correct specification (Exercise \ref{exer:petrol-mb1}),
  ``LLM alternative process" refers to mis-specification of the irregular variability
   (Exercise \ref{exer:petrol-mb2}), and ``LLM empirical process" refers
   the analysis of the actual Petrol series (Exercise \ref{exer:petrol-mb3}).
   The replication achieved by MDFA is not exact, and this is due to statistical
    error as well as the truncation to $q = 30$ that is used.  In contrast,
   the out-performance is modest in the case of LLM alternative process,
   and is substantial in the case of the Petrol data.
 
 \begin{table}[]
\centering
\caption{Empirical MSE for real-time trend estimators (MB WH filter versus
 MDFA filter) applied to bivariate LLM null process,  LLM alternative process,
 and LLM empirical process,  with target trend
 given by the null LLM MB trend.  }
\label{tab:petrol.mat}
\begin{tabular}{cllllll}
\hline
& \multicolumn{2}{c}{Null LLM} &\multicolumn{2}{c}{Alternative LLM} &
  \multicolumn{2}{c}{Empirical LLM} \\
\hline
  Series 	 &  MDFA    &  MB  &  MDFA    &  MB    &  MDFA    &  MB   \\
  Consumption &   \Sexpr{round(perf_null[1,1],digits=6)} &
  \Sexpr{round(perf_null[1,2],digits=6)} & 
    \Sexpr{round(perf_alt[1,1],digits=6)}  &  
    \Sexpr{round(perf_alt[1,2],digits=6)}    &  
    \Sexpr{round(perf_emp[1,1],digits=6)} &   
    \Sexpr{round(perf_emp[1,2],digits=6)} \\
  Imports	 &   \Sexpr{round(perf_null[2,1],digits=6)} &
  \Sexpr{round(perf_null[2,2],digits=6)} & 
    \Sexpr{round(perf_alt[2,1],digits=6)}  &  
    \Sexpr{round(perf_alt[2,2],digits=6)}    &  
    \Sexpr{round(perf_emp[2,1],digits=6)} & 
    \Sexpr{round(perf_emp[2,2],digits=6)} \\
\hline      
\end{tabular}
\end{table}
 
\subsection{Non-Defense Capitalization: the STM Specification}

 
<<echo=False>>=
# visualize
load("Sweave/Data/ndc.RData")
period <- 12
ndc <- ts(ndc[-1,c(1,2)],start=c(1992,2),frequency=period,
             names=c("Shipments","NewOrders")) 
file <- paste("ndc_data.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
plot(ndc,xlab="Year")
invisible(dev.off())
@
  
\begin{figure}[htb!]
\begin{center}
\includegraphics[]{ndc_data.pdf}
\caption{Plot of monthly  Shipments and New Orders of Non-Defense Capital Goods, 
  February 1992 through May 2020 (seasonally adjusted). }
\label{fig:ndcdata}
\end{center}
\end{figure}  



\begin{Example} {\bf NDC MB Replication.} \rm
\label{exam:ndc-mb}
Next, we  study the  MB trend filter arising from a STM  
(cf. Example \ref{exam:trend-i2}) fitted
 to a   bivariate   series of  monthly Shipments and New Orders from 
 the Non-Defense Capital Goods category of the Manufacturing, Shipments,
and Inventories survey (referred to as NDC, for short). 
The data for New Orders is not available at January
1992, since this series starts at February 1992;
 therefore we have trimmed this first observation.
 The data has already been seasonally adjusted, and is 
  displayed in  Figure \ref{fig:ndcdata}).
We consider MLEs for the model parameters (based on previous analysis)  given by
\[
 \Sigma_{S} = 10^{-5} \, \left[ \begin{array}{ll} 
   .66   &  1.25   \\
   1.25  &  2.92   \end{array}  \right]
 \qquad  \Sigma_{N} =  10^{-4} \, \left[ \begin{array}{ll}
        2.52  &  1.67    \\
        1.67 &  35.70   \end{array} \right],
\]
   From this fitted model,  we can construct both the WK filter 
   and the WH filter, as we did in Example \ref{exam:petrol-mb}.
\end{Example}
 
 
\begin{Exercise} {\bf NDC MB Replication.}  \rm
\label{exer:ndc-mb1}
 We again examine the replication capabilities of MDFA, 
 as in Exercise \ref{exer:petrol-mb1},
 but now considering a STM.  
 Simulate a sample of size $T=5000$ from a STM process with parameters
 given in Example \ref{exam:ndc-mb}.   Apply the WK trend filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent trend filter.  
 Use the moving average filter  MDFA  for an $I(2)$ process  to find the best
 concurrent filter, setting $q= 30$.
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}


<<exercise_stmndc-mdfa-null,echo=True>>=
# Simulate a Gaussian STM  of sample size 5000:
set.seed(777)
T.sim <- 5000
burn <- 1000
N <- 2
psi.sim <- c(1.8905590615422, -11.9288577633298, -12.0809347541079, 
             0.660897814610799, -8.2863379601304, -5.66645335346871, 
             -1.34743227511595e-05, -1.41207967213544e-05)
psi.sim[7:8] <- c(0,0)
len <- 1000
dpoly <- c(1,-2,1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,3))
d <- length(dpoly) - 1
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(delta,mu.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + irr.sim

# construct and apply MB WK and WH filters
grid <- T.sim - d
iden <- array(diag(N),c(N,N,1))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.irr <- mdfa.spectra(iden,iden,Sigma.irr,grid)
trend.wkfrf <- mdfa.wkfrf(iden,delta,f.irr,f.mu)
trend.wkfilter <- mdfa.coeff(trend.wkfrf,-len,len)
x.trend.ideal <- mvar.filter(x.sim,trend.wkfilter)
trend.whfrf <- mdfa.whfrf(iden,delta,f.irr,f.mu,len)
trend.whfilter <- mdfa.coeff(trend.whfrf,-len,len)
x.trend.conc <- mvar.filter(x.sim,trend.whfilter)

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,c(0,0),NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(2+len-q):(T.sim+1-q-len),]

# compare in-sample performance
perf_null <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_null) <- c("MDFA","MB")
rownames(perf_null) <- c("Series 1","Series 2")
# compare to criterion value
print(perf_null)
diag(bw.mdfa[[2]])
@


\begin{Exercise} {\bf NDC MB Out-Performance.}  \rm
\label{exer:ndc-mb2}
 Now we alter the specification of the STM for the NDC series,
  to illustrate that MDFA can out-perform the MB concurrent filter.  
  We do this by substantially increasing the
 variability in the simulation's trend, so that the MB WH filter
 will do too much smoothing.  Consider the same irregular covariance
  matrix as in Exercise \ref{exam:ndc-mb}, but now set the trend
 covariance matrix to be
\[
   \qquad  \Sigma_{S} = \left[ \begin{array}{ll}
       3.35 \cdot 10^{-4}  &  6.34 \cdot 10^{-4}   \\
       6.34 \cdot 10^{-4}  &  15.34 \cdot 10^{-4}  \end{array} \right].
\]
 Simulate a sample of size $T=5000$ from an STM process with these parameters,
 but apply  the WK trend filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent trend filter  corresponding to the parameters
  given in Example \ref{exam:ndc-mb}.  
 Use the moving average filter  MDFA  for an $I(2)$ process  to find the best
 concurrent filter, setting $q= 30$.
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}

<<exercise_stmndc-mdfa-alt,echo=True>>=
# Simulate a Gaussian STM  of sample size 5000:
set.seed(777)
T.sim <- 5000
burn <- 1000
N <- 2
psi.sim <- c(1.8905590615422, -11.9288577633298, -12.0809347541079, 
             0.660897814610799, -8.2863379601304, -5.66645335346871, 
             -1.34743227511595e-05, -1.41207967213544e-05)
psi.sim[2:3] <- c(-8,-8)
psi.sim[7:8] <- c(0,0)
len <- 1000
dpoly <- c(1,-2,1)
delta <- array(t(dpoly) %x% diag(N),c(N,N,3))
d <- length(dpoly) - 1
mu.sim <- mdfa.wnsim(psi.sim[1:3],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(delta,mu.sim[[1]])[(burn+1-d):(T.sim+burn-d),]
irr.sim <- mdfa.wnsim(psi.sim[4:6],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + irr.sim

# construct and apply MB WK and WH filters (from previous exercise)
grid <- T.sim - d
x.trend.ideal <- mvar.filter(x.sim,trend.wkfilter)
x.trend.conc <- mvar.filter(x.sim,trend.whfilter)

# get MDFA concurrent filter
q <- 30
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,c(0,0),NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(len-q+2):(T.sim-q+1-len),]

# compare in-sample performance
perf_alt <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_alt) <- c("MDFA","MB")
rownames(perf_alt) <- c("Series 1","Series 2")
# compare to criterion value
print(perf_alt)
diag(bw.mdfa[[2]])
@



\begin{Exercise} {\bf NDC MB Empirical Performance.}  \rm
\label{exer:ndc-mb3}
 We now examine the performance of MDFA   on the NDC data itself,
 recognizing that the STM is probably mis-specified.  
 Use the target filter resulting from the  fitted STM
  from Example \ref{exam:ndc-mb}, truncating eight years of observations at either end
   of the series.  
 Apply the  MDFA  for an $I(2)$ process  to find the best
 concurrent filter, setting $q= 30$.
  Apply both concurrent filters (MB and MDFA)
 to the data, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the trends for both time series.
\end{Exercise}
     

<<exercise_stmndc-mdfa-data,echo=True>>=
T <- dim(ndc)[1]
N <- dim(ndc)[2]
# truncation of ideal filter
trunc <- 8

# construct and apply MB WK and WH filters (from previous exercise)
grid <- T - d
x.trend.ideal <- mvar.filter(ndc,trend.wkfilter[,,(len-trunc*period):(len+trunc*period)])
x.trend.conc <- mvar.filter(ndc,trend.whfilter[,,(len-trunc*period):(len+trunc*period)])

# get MDFA concurrent filter
q <- 30
x.diff <- filter(ndc,dpoly,method="convolution",sides=1)[(d+1):T,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
trend.wkfrf <- mdfa.frf(trend.wkfilter,len,grid)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(trend.wkfrf,c(0,0),NULL,0*diag(N),q)
bw.mdfa <- mdfa.filter(trend.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.trend.mdfa <- mvar.filter(ndc,bw.mdfa[[1]])[(1+trunc*period-q):(T-q-trunc*period),]

# compare in-sample performance
perf_emp <- cbind(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)),
  c(mean((x.trend.ideal[,1] - x.trend.conc[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.conc[,2])^2)))
colnames(perf_emp) <- c("MDFA","MB")
rownames(perf_emp) <- colnames(ndc)
# compare to criterion value
print(perf_emp)
diag(bw.mdfa[[2]])
@
    
    
 The results of these Exercises   reinforces the conclusions of the LLM study.
 Namely, MDFA can replicate STM filters and also out-perform when the model is 
 mis-specified.  Table \ref{tab:ndc.mat} summarizes the performances,
  where ``STM null process" refers to correct specification (Exercise \ref{exer:ndc-mb1}),
  ``STM alternative process" refers to mis-specification of the trend variability
   (Exercise \ref{exer:ndc-mb2}), and ``STM empirical process" refers
   the analysis of the actual NDC series (Exercise \ref{exer:ndc-mb3}).
   The replication achieved by MDFA is not exact, and this is due to statistical
    error as well as the truncation to $q = 30$ that is used.  In contrast,
   the out-performance is modest in the case of STM alternative process,
   and is substantial in the case of the NDC data.
 
 \begin{table}[]
\centering
\caption{Empirical MSE for real-time trend estimators (MB WH filter versus
 MDFA filter) applied to bivariate STM null process,  STM alternative process,
 and STM empirical process,  with target trend
 given by the null STM MB trend.  }
\label{tab:ndc.mat}
\begin{tabular}{cllllll}
\hline
& \multicolumn{2}{c}{Null STM} &\multicolumn{2}{c}{Alternative STM} &
  \multicolumn{2}{c}{Empirical STM} \\
\hline
  Series 	 &  MDFA    &  MB  &  MDFA    &  MB    &  MDFA    &  MB   \\
  Consumption &   \Sexpr{round(perf_null[1,1],digits=6)} &
  \Sexpr{round(perf_null[1,2],digits=6)} & 
    \Sexpr{round(perf_alt[1,1],digits=6)}  &  
    \Sexpr{round(perf_alt[1,2],digits=6)}    &  
    \Sexpr{round(perf_emp[1,1],digits=6)} & 
    \Sexpr{round(perf_emp[1,2],digits=6)} \\
  Imports	 &   \Sexpr{round(perf_null[2,1],digits=6)} & 
  \Sexpr{round(perf_null[2,2],digits=6)} & 
    \Sexpr{round(perf_alt[2,1],digits=6)}  &
    \Sexpr{round(perf_alt[2,2],digits=6)}    &  
    \Sexpr{round(perf_emp[2,1],digits=6)} & 
    \Sexpr{round(perf_emp[2,2],digits=6)} \\
\hline      
\end{tabular}
\end{table}
 

\subsection{Housing Starts: the Atomic Seasonal Specification}


 
<<echo=False>>=
# visualize
load("Sweave/Data/starts.RData")
period <- 12
starts <- ts(starts,start=c(1964,1),frequency=period,
             names=c("South","West","NE","MW")) 
file <- paste("starts_data.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", 
    width = 6, height = 4)
plot(starts,xlab="Year")
invisible(dev.off())
@
  
\begin{figure}[htb!]
\begin{center}
\includegraphics[]{starts_data.pdf}
\caption{Plot of monthly  Housing Units Started (Single Family Units),
  January 1964 through December 2012. }
\label{fig:starts_data}
\end{center}
\end{figure}  

\begin{Example} {\bf Starts MB Replication.} \rm
\label{exam:starts-mb}
Finally, we  study the  MB seasonal adjustment filter arising from a
 structural model with atomic seasonal specification
(cf. Example \ref{exam:sa}),  fitted
 to a   quadvariate   series of  
 New Residential Construction (1964-2012), Housing Units Started, Single Family
Units.   The four series are from the Survey of Construction of the U.S. Census Bureau, available at 
\url{http://www.census.gov/construction/nrc/how_the_data_are_collected/soc.html}.
  The data (referred to as Starts for short)  correspond to the four regions of South, West, NorthEast, and MidWest, and is 
  displayed in  Figure \ref{fig:starts_data}).
  We consider  model parameters  given in Exercise \ref{exer:sa}.
     From this fitted model,  we can construct both the WK filter 
   and the WH filter; recall that the WK frf is given in Figure \ref{fig:sauc-frf}.
\end{Example}
 
 
\begin{Exercise} {\bf Starts MB Replication.}  \rm
\label{exer:starts-mb1}
 We yet again examine the replication capabilities of MDFA, 
  this time considering the case of seasonal noise.
 Simulate a sample of size $T=5000$ from the atomic seasonal
  process  with parameters
 given in Example \ref{exam:starts-mb}.   Apply the WK seasonal adjustment filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent seasonal adjustment filter.  
 Use the moving average filter  MDFA  for a process with signal
 differencing polynomial $\Delta^S (z) = {(1-z)}^2 1_4$ and noise 
  differencing polynomial $\Delta^N (z) = U(z) 1_4$.  Find the best
 concurrent filter, setting $q= 120$.
 (A higher $q$ is needed to capture the more nuanced features of this filter.)
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the seasonal adjustments
 for all four time series.
\end{Exercise}
 
<<exercise_atomstarts-mdfa-null,echo=True>>=
# Simulate a Gaussian atomic seasonal model  of sample size 5000:
set.seed(555)
T.sim <- 5000
burn <- 1000
N <- 4
psi.sim <- c(0.493586093056948, 0.178487258592539, 0.341217399125708, 
             0.399177274154249, 0.848325304642642, 0.68306879252262, 
             -2.3494687111314, -5.47534663726587, -6.69385117951384, 
             -6.08364145983965, 0.875100150810273, 0.221971271148611, 
             0.500866759201029, 0.340625016984097, 0.791037805495801, 
             0.985440262768576, -2.52890913740106, -4.29524634814519, 
             -5.98519527750281, -4.88659954275053, 0.0957466327314851, 
             0.201313350626488, 0.849351809157598, 0.48420520104336, 
             0.62643997675928, 1.13945063379914, -4.04217214895869, 
             -4.68919816059416, -4.73313805629826, -4.0627015759002,
             0.923495751608401, -0.396067294450726, 0.244665046194039, 
             -0.36570474542918, 0.363995718736632, 0.758715172737758, 
             -3.05567431351817, -4.74337970092605, -4.96364133429136, 
             -5.06144086942249, 0.262963683605793, -0.181599400661918, 
             0.149795833258992, -0.105991649100357, 0.21503766242974, 
             -0.141649861043968, -2.07489346121933, -3.64302004053168, 
             -5.69277788172285, -5.3689470753418, 1.40718934367933,
             -0.0085452878747676, -0.219886337273936, 0.0283662345070971,
             1.23786259577472, 0.199834135215749, -4.53336362894347, 
             -4.70016052568401, -7.07530853221777, -6.03054443735399, 
             -0.0995506040524902, 0.116607848697947, 0.157899802233636, 
             -0.0363184981547607, 0.18385749297074, 0.329351477585333, 
             -2.1377604820296, -3.62882764786239, -5.11279846492415, 
             -3.62475631527416, 0.124305286145147, 0.0292507920421885, 
             -0.0873349194845382, 0.178977764316143, 0.484389128732254,
             0.265835976421986, 1.87566939226944, 0.1445002084775, 
             -1.34264222816582, -0.305367634014929, -0.00488431480035087, 
             -0.000945659564684563, -0.00106126820173145, -0.000413658838890233)
psi.sim[81:84] <- c(0,0,0,0)
len <- 1000

# set up differencing polynomials
dpoly <- c(1,-1,rep(0,10),-1,1)
d <- length(dpoly) - 1
dpoly.0 <- c(1,-2,1)
dpoly.1 <- c(1,-2*cos(pi/6),1)
dpoly.2 <- c(1,-2*cos(2*pi/6),1)
dpoly.3 <- c(1,-2*cos(3*pi/6),1)
dpoly.4 <- c(1,-2*cos(4*pi/6),1)
dpoly.5 <- c(1,-2*cos(5*pi/6),1)
dpoly.6 <- c(1,1)
dpoly.but1 <- polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but2 <- polymult(dpoly.1,polymult(dpoly.3,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but3 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.4,polymult(dpoly.5,dpoly.6))))
dpoly.but4 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.5,dpoly.6))))
dpoly.but5 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,dpoly.6))))
dpoly.but6 <- polymult(dpoly.1,polymult(dpoly.2,polymult(dpoly.3,polymult(dpoly.4,dpoly.5))))
delta.c <- array(t(dpoly.0) %x% diag(N),c(N,N,3))
delta.but1 <- array(t(dpoly.but1) %x% diag(N),c(N,N,10))
delta.but2 <- array(t(dpoly.but2) %x% diag(N),c(N,N,10))
delta.but3 <- array(t(dpoly.but3) %x% diag(N),c(N,N,10))
delta.but4 <- array(t(dpoly.but4) %x% diag(N),c(N,N,10))
delta.but5 <- array(t(dpoly.but5) %x% diag(N),c(N,N,10))
delta.but6 <- array(t(dpoly.but6) %x% diag(N),c(N,N,11))
delta.seas <- array(t(rep(1,12)) %x% diag(N),c(N,N,12))

# get simulation
mu.sim <- mdfa.wnsim(psi.sim[1:10],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(array(t(dpoly.0) %x% diag(N),c(N,N,3)),
                     mu.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas1.sim <- mdfa.wnsim(psi.sim[11:20],rep(1,N),T.sim+burn,Inf)
Sigma.seas1 <- seas1.sim[[2]]
seas1.sim <- mdfa.ucsim(array(t(dpoly.1) %x% diag(N),c(N,N,3)),
                     seas1.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas2.sim <- mdfa.wnsim(psi.sim[21:30],rep(1,N),T.sim+burn,Inf)
Sigma.seas2 <- seas2.sim[[2]]
seas2.sim <- mdfa.ucsim(array(t(dpoly.2) %x% diag(N),c(N,N,3)),
                     seas2.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas3.sim <- mdfa.wnsim(psi.sim[31:40],rep(1,N),T.sim+burn,Inf)
Sigma.seas3 <- seas3.sim[[2]]
seas3.sim <- mdfa.ucsim(array(t(dpoly.3) %x% diag(N),c(N,N,3)),
                     seas3.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas4.sim <- mdfa.wnsim(psi.sim[41:50],rep(1,N),T.sim+burn,Inf)
Sigma.seas4 <- seas4.sim[[2]]
seas4.sim <- mdfa.ucsim(array(t(dpoly.4) %x% diag(N),c(N,N,3)),
                     seas4.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas5.sim <- mdfa.wnsim(psi.sim[51:60],rep(1,N),T.sim+burn,Inf)
Sigma.seas5 <- seas5.sim[[2]]
seas5.sim <- mdfa.ucsim(array(t(dpoly.5) %x% diag(N),c(N,N,3)),
                     seas5.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas6.sim <- mdfa.wnsim(psi.sim[61:70],rep(1,N),T.sim+burn,Inf)
Sigma.seas6 <- seas6.sim[[2]]
seas6.sim <- mdfa.ucsim(array(t(dpoly.6) %x% diag(N),c(N,N,2)),
                     seas6.sim[[1]])[(burn+1-1):(T.sim+burn-1),]
irr.sim <- mdfa.wnsim(psi.sim[71:80],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + seas1.sim + seas2.sim + seas3.sim +
  seas4.sim + seas5.sim + seas6.sim + irr.sim

# construct and apply MB WK and WH filters
grid <- T.sim - d
iden <- array(diag(N),c(N,N,1))
f.mu <- mdfa.spectra(iden,iden,Sigma.mu,grid)
f.seas1 <- mdfa.spectra(iden,delta.but1,Sigma.seas1,grid)
f.seas2 <- mdfa.spectra(iden,delta.but2,Sigma.seas2,grid)
f.seas3 <- mdfa.spectra(iden,delta.but3,Sigma.seas3,grid)
f.seas4 <- mdfa.spectra(iden,delta.but4,Sigma.seas4,grid)
f.seas5 <- mdfa.spectra(iden,delta.but5,Sigma.seas5,grid)
f.seas6 <- mdfa.spectra(iden,delta.but6,Sigma.seas6,grid)
f.irr <- mdfa.spectra(iden,delta.c,Sigma.irr,grid)
f.signal <- f.mu + f.irr
f.noise <- f.seas1 + f.seas2 + f.seas3 + f.seas4 + f.seas5 + f.seas6
sa.wkfrf <- mdfa.wkfrf(delta.seas,delta.c,f.noise,f.signal)
sa.wkfilter <- mdfa.coeff(sa.wkfrf,-len,len)
x.sa.ideal <- mvar.filter(x.sim,sa.wkfilter)
sa.whfrf <- mdfa.whfrf(delta.seas,delta.c,f.noise,f.signal,len)
sa.whfilter <- mdfa.coeff(sa.whfrf,-len,len)
x.sa.conc <- mvar.filter(x.sim,sa.whfilter)

# get MDFA concurrent filter
q <- 120
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(sa.wkfrf,c(0,0),seq(1,6)/6,0*diag(N),q)
bw.mdfa <- mdfa.filter(sa.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.sa.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(2+len-q):(T.sim+1-q-len),]

# compare in-sample performance
perf_null <- cbind(c(mean((x.sa.ideal[,1] - x.sa.mdfa[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.mdfa[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.mdfa[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.mdfa[,4])^2)),
  c(mean((x.sa.ideal[,1] - x.sa.conc[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.conc[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.conc[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.conc[,4])^2)))
colnames(perf_null) <- c("MDFA","MB")
rownames(perf_null) <- c("Series 1","Series 2","Series 3","Series 4")
# compare to criterion value
print(perf_null)
diag(bw.mdfa[[2]])
@


\begin{Exercise} {\bf Starts MB Out-Performance.}  \rm
\label{exer:starts-mb2}
 Now we alter the specification of Exercise \ref{exer:starts-mb1}
 by eliminating the fifth atomic seasonal. Hence the WH filter 
 will attempt to remove seasonality at this frequency when it is
  not required.
 Simulate a sample of size $T=5000$ from the atomic seasonal
  process  with parameters
 given in Example \ref{exam:starts-mb},
  except that  $\Sigma_5 = 0 \, 1_4$.
 Apply the WK seasonal adjustment filter
(truncate the filter to $1000$ coefficients on each side) and the 
 optimal concurrent seasonal adjustment filter
  from Exercise \ref{exer:starts-mb1}.
 Use the moving average filter  MDFA  for a process with signal
 differencing polynomial $\Delta^S (z) = {(1-z)}^2 1_4$ and noise 
  differencing polynomial $\Delta^N (z) = U(z) 1_4$.  Find the best
 concurrent filter, setting $q= 120$.
    Apply both concurrent filters (MB and MDFA)
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the seasonal adjustments
 for all four time series.
\end{Exercise} 

<<exercise_atomstarts-mdfa-alt,echo=True>>=
# Simulate a Gaussian atomic seasonal model  of sample size 5000:
set.seed(555)
T.sim <- 5000
burn <- 1000
N <- 4
psi.sim <- c(0.493586093056948, 0.178487258592539, 0.341217399125708, 
             0.399177274154249, 0.848325304642642, 0.68306879252262, 
             -2.3494687111314, -5.47534663726587, -6.69385117951384, 
             -6.08364145983965, 0.875100150810273, 0.221971271148611, 
             0.500866759201029, 0.340625016984097, 0.791037805495801, 
             0.985440262768576, -2.52890913740106, -4.29524634814519, 
             -5.98519527750281, -4.88659954275053, 0.0957466327314851, 
             0.201313350626488, 0.849351809157598, 0.48420520104336, 
             0.62643997675928, 1.13945063379914, -4.04217214895869, 
             -4.68919816059416, -4.73313805629826, -4.0627015759002,
             0.923495751608401, -0.396067294450726, 0.244665046194039, 
             -0.36570474542918, 0.363995718736632, 0.758715172737758, 
             -3.05567431351817, -4.74337970092605, -4.96364133429136, 
             -5.06144086942249, 0.262963683605793, -0.181599400661918, 
             0.149795833258992, -0.105991649100357, 0.21503766242974, 
             -0.141649861043968, -2.07489346121933, -3.64302004053168, 
             -5.69277788172285, -5.3689470753418, 1.40718934367933,
             -0.0085452878747676, -0.219886337273936, 0.0283662345070971,
             1.23786259577472, 0.199834135215749, -4.53336362894347, 
             -4.70016052568401, -7.07530853221777, -6.03054443735399, 
             -0.0995506040524902, 0.116607848697947, 0.157899802233636, 
             -0.0363184981547607, 0.18385749297074, 0.329351477585333, 
             -2.1377604820296, -3.62882764786239, -5.11279846492415, 
             -3.62475631527416, 0.124305286145147, 0.0292507920421885, 
             -0.0873349194845382, 0.178977764316143, 0.484389128732254,
             0.265835976421986, 1.87566939226944, 0.1445002084775, 
             -1.34264222816582, -0.305367634014929, -0.00488431480035087, 
             -0.000945659564684563, -0.00106126820173145, -0.000413658838890233)
psi.sim[81:84] <- c(0,0,0,0)
len <- 1000

# get simulation
mu.sim <- mdfa.wnsim(psi.sim[1:10],rep(1,N),T.sim+burn,Inf)
Sigma.mu <- mu.sim[[2]]
mu.sim <- mdfa.ucsim(array(t(dpoly.0) %x% diag(N),c(N,N,3)),
                     mu.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas1.sim <- mdfa.wnsim(psi.sim[11:20],rep(1,N),T.sim+burn,Inf)
Sigma.seas1 <- seas1.sim[[2]]
seas1.sim <- mdfa.ucsim(array(t(dpoly.1) %x% diag(N),c(N,N,3)),
                     seas1.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas2.sim <- mdfa.wnsim(psi.sim[21:30],rep(1,N),T.sim+burn,Inf)
Sigma.seas2 <- seas2.sim[[2]]
seas2.sim <- mdfa.ucsim(array(t(dpoly.2) %x% diag(N),c(N,N,3)),
                     seas2.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas3.sim <- mdfa.wnsim(psi.sim[31:40],rep(1,N),T.sim+burn,Inf)
Sigma.seas3 <- seas3.sim[[2]]
seas3.sim <- mdfa.ucsim(array(t(dpoly.3) %x% diag(N),c(N,N,3)),
                     seas3.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas4.sim <- mdfa.wnsim(psi.sim[41:50],rep(1,N),T.sim+burn,Inf)
Sigma.seas4 <- seas4.sim[[2]]
seas4.sim <- mdfa.ucsim(array(t(dpoly.4) %x% diag(N),c(N,N,3)),
                     seas4.sim[[1]])[(burn+1-2):(T.sim+burn-2),]
seas6.sim <- mdfa.wnsim(psi.sim[61:70],rep(1,N),T.sim+burn,Inf)
Sigma.seas6 <- seas6.sim[[2]]
seas6.sim <- mdfa.ucsim(array(t(dpoly.6) %x% diag(N),c(N,N,2)),
                     seas6.sim[[1]])[(burn+1-1):(T.sim+burn-1),]
irr.sim <- mdfa.wnsim(psi.sim[71:80],rep(1,N),T.sim,Inf)
Sigma.irr <- irr.sim[[2]]
irr.sim <- irr.sim[[1]] 
x.sim <- mu.sim + seas1.sim + seas2.sim + seas3.sim +
  seas4.sim + seas6.sim + irr.sim

# construct and apply MB WK and WH filters
grid <- T.sim - d
x.sa.ideal <- mvar.filter(x.sim,sa.wkfilter)
x.sa.conc <- mvar.filter(x.sim,sa.whfilter)

# get MDFA concurrent filter
q <- 120
x.diff <- filter(x.sim,dpoly,method="convolution",sides=1)[(d+1):T.sim,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(sa.wkfrf,c(0,0),seq(1,6)/6,0*diag(N),q)
bw.mdfa <- mdfa.filter(sa.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.sa.mdfa <- mvar.filter(x.sim,bw.mdfa[[1]])[(2+len-q):(T.sim+1-q-len),]

# compare in-sample performance
perf_alt <- cbind(c(mean((x.sa.ideal[,1] - x.sa.mdfa[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.mdfa[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.mdfa[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.mdfa[,4])^2)),
  c(mean((x.sa.ideal[,1] - x.sa.conc[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.conc[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.conc[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.conc[,4])^2)))
colnames(perf_alt) <- c("MDFA","MB")
rownames(perf_alt) <- c("Series 1","Series 2","Series 3","Series 4")
# compare to criterion value
print(perf_alt)
diag(bw.mdfa[[2]])
@

 

\begin{Exercise} {\bf Starts MB Empirical Performance.}  \rm
\label{exer:starts-mb3}
 We now examine the performance of MDFA   on the Starts data itself,
 recognizing that the given structural model is probably mis-specified.  
 Use the target filter resulting from the  fitted structural model
  from Example \ref{exam:starts-mb}, truncating ten years of observations at either end
   of the series.  
 Use the moving average filter  MDFA  for a process with signal
 differencing polynomial $\Delta^S (z) = {(1-z)}^2 1_4$ and noise 
  differencing polynomial $\Delta^N (z) = U(z) 1_4$.  Find the best
 concurrent filter, setting $q= 120$.
  Apply both concurrent filters (MB and MDFA)
 to the data, and compare the relevant portions to the ideal seasonal adjustment.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa-constrained}).   Target the seasonal adjustments for all
  four series.
\end{Exercise}
     
  


<<exercise_starts-mdfa-data,echo=True>>=
T <- dim(starts)[1]
N <- dim(starts)[2]
# truncation of ideal filter
trunc <- 10

# construct and apply MB WK and WH filters (from previous exercise)
grid <- T - d
x.sa.ideal <- mvar.filter(starts,sa.wkfilter[,,(len-trunc*period):(len+trunc*period)])
x.sa.conc <- mvar.filter(starts,sa.whfilter[,,(len-trunc*period):(len+trunc*period)])

# get MDFA concurrent filter
q <- 120
x.diff <- filter(starts,dpoly,method="convolution",sides=1)[(d+1):T,]
spec.hat <- mdfa.pergram(x.diff,dpoly)
m <- floor(grid/2)
sa.wkfrf <- mdfa.frf(sa.wkfilter,len,grid)
# The Fourier frequencies
freq.ft <- 2*pi*grid^{-1}*(seq(1,grid) - (m+1))
constraints.mdfa <- mdfa.getconstraints(sa.wkfrf,c(0,0),seq(1,6)/6,0*diag(N),q)
bw.mdfa <- mdfa.filter(sa.wkfrf,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])
x.sa.mdfa <- mvar.filter(starts,bw.mdfa[[1]])[(1+trunc*period-q):(T-q-trunc*period),]

# compare in-sample performance
perf_emp <- cbind(c(mean((x.sa.ideal[,1] - x.sa.mdfa[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.mdfa[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.mdfa[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.mdfa[,4])^2)),
  c(mean((x.sa.ideal[,1] - x.sa.conc[,1])^2),
	mean((x.sa.ideal[,2] - x.sa.conc[,2])^2),
	mean((x.sa.ideal[,3] - x.sa.conc[,3])^2),
	mean((x.sa.ideal[,4] - x.sa.conc[,4])^2)))
colnames(perf_emp) <- c("MDFA","MB")
rownames(perf_emp) <- colnames(starts)
# compare to criterion value
print(perf_emp)
diag(bw.mdfa[[2]])
@
    
    
 The results of these Exercises   reinforce the conclusions of the LLM and STM studies.
 Namely, MDFA can replicate MB filters and also out-perform when the model is 
 mis-specified.  Table \ref{tab:starts.mat} summarizes the performances,
  where ``Structual null process" refers to correct
  specification (Exercise \ref{exer:starts-mb1}),
  ``Structural alternative process" refers to mis-specification of the fifth
   atomic seasonal component 
   (Exercise \ref{exer:starts-mb2}), and ``Structural empirical process" refers
   the analysis of the actual Starts series (Exercise \ref{exer:starts-mb3}).
   The replication achieved by MDFA is not exact, and this is due to statistical
    error as well as the truncation to $q = 120$ that is used.  In contrast,
   the out-performance is modest in the case of the Structural alternative process,
   and is substantial in the case of the Starts data.
  
 \begin{table}[]
\centering
\caption{Empirical MSE for real-time seasonal adjustment estimators (MB WH filter versus
 MDFA filter) applied to quadvariate Structural null process,  Structural
 alternative process,
 and Structural empirical process,  with target seasonal adjustment
 given by the null Structural MB seasonal adjustment.  }
\label{tab:starts.mat}
\begin{tabular}{cllllll}
\hline
& \multicolumn{2}{c}{Null STM} &\multicolumn{2}{c}{Alternative STM} &
  \multicolumn{2}{c}{Empirical STM} \\
\hline
  Series 	 &  MDFA    &  MB  &  MDFA    &  MB    &  MDFA    &  MB   \\
  South &   \Sexpr{round(perf_null[1,1],digits=6)} &
  \Sexpr{round(perf_null[1,2],digits=6)} & 
    \Sexpr{round(perf_alt[1,1],digits=6)}  &
    \Sexpr{round(perf_alt[1,2],digits=6)}    &  
    \Sexpr{round(perf_emp[1,1],digits=6)} &
    \Sexpr{round(perf_emp[1,2],digits=6)} \\
  West	 &   \Sexpr{round(perf_null[2,1],digits=6)} &
  \Sexpr{round(perf_null[2,2],digits=6)} & 
    \Sexpr{round(perf_alt[2,1],digits=6)}  &  
    \Sexpr{round(perf_alt[2,2],digits=6)}    &  
    \Sexpr{round(perf_emp[2,1],digits=6)} & 
    \Sexpr{round(perf_emp[2,2],digits=6)} \\
  NE &   \Sexpr{round(perf_null[3,1],digits=6)} &
  \Sexpr{round(perf_null[3,2],digits=6)} & 
    \Sexpr{round(perf_alt[3,1],digits=6)}  &  
    \Sexpr{round(perf_alt[3,2],digits=6)}    &  
    \Sexpr{round(perf_emp[3,1],digits=6)} &
    \Sexpr{round(perf_emp[3,2],digits=6)} \\
  MW	 &   \Sexpr{round(perf_null[4,1],digits=6)} &
  \Sexpr{round(perf_null[4,2],digits=6)} & 
    \Sexpr{round(perf_alt[4,1],digits=6)}  &  
    \Sexpr{round(perf_alt[4,2],digits=6)}    &  
    \Sexpr{round(perf_emp[4,1],digits=6)} &
    \Sexpr{round(perf_emp[4,2],digits=6)} \\
\hline      
\end{tabular}
\end{table}
 
  